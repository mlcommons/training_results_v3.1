diff --git a/csrc/cpu/aten/MLPerfFreqEmbeddingBag.cpp b/csrc/cpu/aten/MLPerfFreqEmbeddingBag.cpp
new file mode 100644
index 000000000..808d7f9aa
--- /dev/null
+++ b/csrc/cpu/aten/MLPerfFreqEmbeddingBag.cpp
@@ -0,0 +1,197 @@
+#include <torch/all.h>
+#include "MLPerfFreqEmbeddingBag.h"
+#include <torch/csrc/autograd/function.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+DEFINE_DISPATCH(mlperf_freqembbag_spsfwd_local_kernel_stub);
+DEFINE_DISPATCH(mlperf_freqembbag_spsfwd_merge_kernel_stub);
+DEFINE_DISPATCH(mlperf_freqembbag_denfwd_kernel_stub);
+
+DEFINE_DISPATCH(mlperf_freqembbag_spsbwd_local_kernel_stub);
+DEFINE_DISPATCH(mlperf_freqembbag_spsbwd_adagrad_kernel_stub);
+DEFINE_DISPATCH(mlperf_freqembbag_denbwd_local_kernel_stub);
+// DEFINE_DISPATCH(mlperf_freqembbag_spsbwd_adagrad_kernel_stub);
+DEFINE_DISPATCH(mlperf_freqembbag_denbwd_adagrad_kernel_stub);
+
+std::tuple<std::vector<at::Tensor>, std::vector<at::Tensor>, std::vector<at::Tensor>> mlperf_freqembbag_spsfwd_local_cpu(
+    int64_t rank,
+    int64_t size,
+    int64_t gbatch,
+    const at::Tensor &input_splits, // need to be fix syk
+    const at::Tensor &multihot,
+    const std::vector<at::Tensor> &indices,
+    const at::Tensor &idx_offset,
+    const at::Tensor &lookup,
+    const at::Tensor &sparse_weight) {
+    // return std::vector<Tensor> sparse tensor
+    RECORD_FUNCTION("ipex::mlperf_freqembbag_spsfwd_local", c10::ArrayRef<c10::IValue>({}));
+    return mlperf_freqembbag_spsfwd_local_kernel_stub(
+        kCPU, rank, size, gbatch, input_splits, multihot,
+        indices, idx_offset, lookup,sparse_weight);
+}
+
+void mlperf_freqembbag_spsfwd_merge_cpu(
+    int64_t size, int64_t num_emb,
+    const at::Tensor&output_splits,
+    const std::vector<at::Tensor> &sps_idx,
+    const std::vector<at::Tensor> &sps_val,
+    const std::vector<at::Tensor> &sps_ofs,
+    const at::Tensor &result) {
+    // return None
+    RECORD_FUNCTION("ipex::mlperf_freqembbag_spsfwd_merge", c10::ArrayRef<c10::IValue>({}));
+    mlperf_freqembbag_spsfwd_merge_kernel_stub(kCPU, size, num_emb, output_splits, sps_idx, sps_val, sps_ofs, result);
+    return;
+}
+
+void mlperf_freqembbag_denfwd_cpu(
+    int64_t rank,
+    int64_t lbatch,
+    const at::Tensor &multihot,
+    const std::vector<at::Tensor> &indices,
+    const at::Tensor &idx_offset,
+    const at::Tensor &lookup,
+    const at::Tensor &dense_weight,
+    const at::Tensor &result) {
+    // return Tensor result
+    RECORD_FUNCTION("ipex::mlperf_freqembbag_denfwd_local", c10::ArrayRef<c10::IValue>({}));
+    mlperf_freqembbag_denfwd_kernel_stub(
+        kCPU, rank, lbatch, multihot,
+        indices, idx_offset, lookup, dense_weight, result);
+    return;
+}
+
+std::tuple<std::vector<at::Tensor>, std::vector<at::Tensor>, std::vector<at::Tensor>> mlperf_freqembbag_spsbwd_local_cpu(
+    const int64_t rank,
+    const int64_t size,
+    const int64_t lbatch,
+    const at::Tensor &input_splits,
+    const at::Tensor &multihot,
+    const std::vector<at::Tensor> &indices,
+    const at::Tensor &idx_offset,
+    const at::Tensor &lookup,
+    const at::Tensor &grad) {
+    // return vector<at::Tensor> send_buffer
+    RECORD_FUNCTION("ipex::mlperf_freqembbag_spsbwd_local", c10::ArrayRef<c10::IValue>({}));
+    return mlperf_freqembbag_spsbwd_local_kernel_stub(
+        kCPU, rank, size, lbatch, input_splits, multihot, indices, idx_offset, lookup, grad);
+}
+
+void mlperf_freqembbag_spsbwd_adagrad_cpu(
+    int64_t size,
+    double lr,
+    double eps,
+    const at::Tensor &output_splits,
+    const std::vector<at::Tensor> &sps_idx,
+    const std::vector<at::Tensor> &sps_val,
+    const std::vector<at::Tensor> &sps_ofs,
+    const at::Tensor &sps_wgt,
+    const at::Tensor &sps_hes) {
+    // return Tensor sparse grad
+    RECORD_FUNCTION("ipex::mlperf_freqembbag_spsbwd_adagrad", c10::ArrayRef<c10::IValue>({}));
+    mlperf_freqembbag_spsbwd_adagrad_kernel_stub(kCPU, size, lr, eps, output_splits, sps_idx, sps_val, sps_ofs, sps_wgt, sps_hes);
+    return;
+}
+
+void mlperf_freqembbag_denbwd_local_cpu(
+    int64_t rank,
+    int64_t size,
+    int64_t lbatch,
+    const at::Tensor &multihot,
+    const std::vector<at::Tensor> &indices,
+    const at::Tensor &lookup,
+    const at::Tensor &idx_offset,
+    const at::Tensor &grad,
+    const at::Tensor &ds_grad) {
+    // return Tensor dense_grad
+    RECORD_FUNCTION("ipex::mlperf_freqembbag_denbwd_local", c10::ArrayRef<c10::IValue>({}));
+    mlperf_freqembbag_denbwd_local_kernel_stub(
+        kCPU, rank, size, lbatch, multihot, indices, lookup, idx_offset, grad, ds_grad);
+    return;
+}
+
+void mlperf_freqembbag_denbwd_adagrad_cpu(
+    const double lr,
+    const double eps,
+    const at::Tensor &dense_grad,
+    const at::Tensor &dense_weight,
+    const at::Tensor &dense_hessian) {
+    // return None
+    RECORD_FUNCTION("ipex::mlperf_freqembbag_denbwd_adagrad", c10::ArrayRef<c10::IValue>({}));
+    mlperf_freqembbag_denbwd_adagrad_kernel_stub(
+        kCPU, lr, eps, dense_grad, dense_weight, dense_hessian);
+    return;
+}
+
+// void mlperf_freqembbag_spsbwd_adagrad_cpu(
+//     const int size,
+//     const float lr,
+//     const float eps,
+//     const at::Tensor &sparse_grad,
+//     at::Tensor &sparse_weight,
+//     at::Tensor &sparse_hessian) {
+//     // return None
+//     RECORD_FUNCTION("ipex::mlperf_freqembbag_spsbwd_adagrad", c10::ArrayRef<c10::IValue>({}));
+//     mlperf_freqembbag_spsbwd_adagrad_kernel_stub(
+//         size, lr, eps, sparse_grad, sparse_weight, sparse_hessian);
+//     return;
+// }
+
+}
+}
+
+namespace {
+TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
+    // forward
+    // sparse forward local
+    m.def(
+        "mlperf_freqembbag_spsfwd_local(int rank, int size, int batch, Tensor input_splits, Tensor multihot, Tensor[] indices, Tensor idx_offset, Tensor lookup, Tensor sparse_weight) -> (Tensor[], Tensor[], Tensor[])");
+    m.impl(
+        "mlperf_freqembbag_spsfwd_local",
+        c10::DispatchKey::CPU, torch_ipex::cpu::mlperf_freqembbag_spsfwd_local_cpu);
+    // sparse forward merge
+    m.def(
+        "mlperf_freqembbag_spsfwd_merge(int size, int num_emb, Tensor output_splits, Tensor []sps_idx, Tensor []sps_val, Tensor []sps_ofs, Tensor result) -> ()");
+    m.impl("mlperf_freqembbag_spsfwd_merge",
+           c10::DispatchKey::CPU, torch_ipex::cpu::mlperf_freqembbag_spsfwd_merge_cpu);
+
+    // dense forward
+    m.def(
+        "mlperf_freqembbag_denfwd(int rank, int size, Tensor multihot, Tensor []indices, Tensor idx_offset, Tensor lookup, Tensor dense_weight, Tensor result) -> ()");
+    m.impl(
+        "mlperf_freqembbag_denfwd",
+        c10::DispatchKey::CPU, torch_ipex::cpu::mlperf_freqembbag_denfwd_cpu);
+
+    // backward
+    // sparse backward local
+    m.def(
+        "mlperf_freqembbag_spsbwd_local(int rank, int size, int lbatch, Tensor input_splits, Tensor multihot, Tensor []indices, Tensor idx_offset, Tensor lookup, Tensor grad) -> (Tensor[], Tensor[], Tensor[])");
+    m.impl("mlperf_freqembbag_spsbwd_local",
+           c10::DispatchKey::CPU, torch_ipex::cpu::mlperf_freqembbag_spsbwd_local_cpu);
+
+    // sparse backward merge
+    m.def(
+        "mlperf_freqembbag_spsbwd_adagrad(int size, float lr, float eps, Tensor output_splits, Tensor []sps_idx, Tensor []sps_val, Tensor []sps_ofs, Tensor sps_wgt, Tensor sps_hes) -> ()");
+    m.impl("mlperf_freqembbag_spsbwd_adagrad",
+           c10::DispatchKey::CPU, torch_ipex::cpu::mlperf_freqembbag_spsbwd_adagrad_cpu);
+
+    // // adagrad update sparse
+    // m.def("mlperf_freqembbag_spsbwd_adagrad(int size, float lr, float eps, Tensor sparse_grad, Tensor sparse_weight, Tensor sparse_hessian) -> None");
+    // m.impl("mlperf_freqembbag_spsbwd_adagrad",
+    //        c10::DispatchKey::CPU, torch_ipex::cpu::);
+
+    // // dense backward local
+    m.def(
+        "mlperf_freqembbag_denbwd_local(int rank, int size, int lbatch, Tensor multihot, Tensor []indices, Tensor lookup, Tensor idx_offset, Tensor grad, Tensor ds_grad) -> ()");
+    m.impl("mlperf_freqembbag_denbwd_local",
+           c10::DispatchKey::CPU, torch_ipex::cpu::mlperf_freqembbag_denbwd_local_cpu);
+
+    // // adagrad update dense
+    m.def(
+        "mlperf_freqembbag_denbwd_adagrad(float lr, float eps, Tensor dense_grad, Tensor dense_weight, Tensor dense_hessian) -> ()");
+    m.impl("mlperf_freqembbag_denbwd_adagrad",
+           c10::DispatchKey::CPU, torch_ipex::cpu::mlperf_freqembbag_denbwd_adagrad_cpu);
+
+}
+}
diff --git a/csrc/cpu/aten/MLPerfFreqEmbeddingBag.h b/csrc/cpu/aten/MLPerfFreqEmbeddingBag.h
new file mode 100644
index 000000000..cd4f7b737
--- /dev/null
+++ b/csrc/cpu/aten/MLPerfFreqEmbeddingBag.h
@@ -0,0 +1,85 @@
+#pragma once
+
+#include <ATen/ATen.h>
+#include <dyndisp/DispatchStub.h>
+#include <vector>
+#include <omp.h>
+#include <iostream>
+#include <chrono>
+#include <immintrin.h>
+
+namespace torch_ipex{
+namespace cpu {
+
+// forward
+using mlperf_freqembbag_spsfwd_local_fn = std::tuple<std::vector<at::Tensor>, std::vector<at::Tensor>, std::vector<at::Tensor>> (*)(
+    int64_t rank, int64_t size, int64_t gbatch,
+    const at::Tensor &input_splits, // need to be fix syk for torchccl bug
+    const at::Tensor &multihot,
+    const std::vector<at::Tensor> &indices,
+    const at::Tensor &idx_offset,
+    const at::Tensor &lookup,
+    const at::Tensor &sparse_weight);
+
+using mlperf_freqembbag_spsfwd_merge_fn = void (*)(
+    int64_t size, int64_t num_emb,
+    const at::Tensor &output_splits,
+    const std::vector<at::Tensor> &sps_idx,
+    const std::vector<at::Tensor> &sps_val,
+    const std::vector<at::Tensor> &sps_ofs,
+    const at::Tensor &result);
+
+using mlperf_freqembbag_denfwd_fn = void (*) (
+    int64_t rank, int64_t lbatch,
+    const at::Tensor &multihot,
+    const std::vector<at::Tensor> &indices,
+    const at::Tensor &idx_offset,
+    const at::Tensor &lookup,
+    const at::Tensor &dense_weight,
+    const at::Tensor &result);
+DECLARE_DISPATCH(mlperf_freqembbag_spsfwd_local_fn, mlperf_freqembbag_spsfwd_local_kernel_stub);
+DECLARE_DISPATCH(mlperf_freqembbag_spsfwd_merge_fn, mlperf_freqembbag_spsfwd_merge_kernel_stub);
+DECLARE_DISPATCH(mlperf_freqembbag_denfwd_fn, mlperf_freqembbag_denfwd_kernel_stub);
+
+// backward
+using mlperf_freqembbag_spsbwd_local_fn = std::tuple<std::vector<at::Tensor>, std::vector<at::Tensor>, std::vector<at::Tensor>> (*)(
+    int64_t rank, int64_t size, int64_t lbatch,
+    const at::Tensor &input_splits, // need to be fix syk for torchccl bug
+    const at::Tensor &multihot,
+    const std::vector<at::Tensor> &indices,
+    const at::Tensor &idx_offset,
+    const at::Tensor &lookup,
+    const at::Tensor &grad);
+
+using mlperf_freqembbag_spsbwd_adagrad_fn = void (*) (
+    int64_t size, double lr, double eps,
+    const at::Tensor &output_splits,
+    const std::vector<at::Tensor> &sps_idx,
+    const std::vector<at::Tensor> &sps_val,
+    const std::vector<at::Tensor> &sps_ofs,
+    const at::Tensor &sps_wgt,
+    const at::Tensor &sps_hes);
+
+using mlperf_freqembbag_denbwd_local_fn = void (*) (
+    int64_t rank,
+    int64_t size,
+    int64_t lbatch,
+    const at::Tensor &multihot,
+    const std::vector<at::Tensor> &indices,
+    const at::Tensor &lookup,
+    const at::Tensor &idx_offset,
+    const at::Tensor &grad,
+    const at::Tensor &ds_grad);
+
+using mlperf_freqembbag_denbwd_adagrad_fn = void (*) (
+    double lr, double eps,
+    const at::Tensor &dense_grad,
+    const at::Tensor &dense_weight,
+    const at::Tensor &dense_hessian);
+DECLARE_DISPATCH(mlperf_freqembbag_spsbwd_local_fn, mlperf_freqembbag_spsbwd_local_kernel_stub);
+DECLARE_DISPATCH(mlperf_freqembbag_spsbwd_adagrad_fn, mlperf_freqembbag_spsbwd_adagrad_kernel_stub);
+DECLARE_DISPATCH(mlperf_freqembbag_denbwd_local_fn, mlperf_freqembbag_denbwd_local_kernel_stub);
+DECLARE_DISPATCH(mlperf_freqembbag_denbwd_adagrad_fn, mlperf_freqembbag_denbwd_adagrad_kernel_stub);
+
+}
+}
diff --git a/csrc/cpu/aten/MLPerfInteraction.cpp b/csrc/cpu/aten/MLPerfInteraction.cpp
new file mode 100644
index 000000000..ba47d6115
--- /dev/null
+++ b/csrc/cpu/aten/MLPerfInteraction.cpp
@@ -0,0 +1,75 @@
+#include <torch/all.h>
+#include "MLPerfInteraction.h"
+#include <torch/csrc/autograd/function.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+DEFINE_DISPATCH(mlperf_interaction_forward_kernel_stub);
+DEFINE_DISPATCH(mlperf_interaction_backward_kernel_stub);
+
+at::Tensor mlperf_interaction_forward_cpu(
+    const at::Tensor& x0,
+    const at::Tensor& xlw,
+    const at::Tensor& xl) {
+  RECORD_FUNCTION("ipex::mlperf_interaction_forward", c10::ArrayRef<c10::IValue>({}));
+
+  return mlperf_interaction_forward_kernel_stub(
+    kCPU, x0, xlw, xl);
+}
+
+std::tuple<at::Tensor, at::Tensor> mlperf_interaction_backward_cpu(
+    const at::Tensor& x0,
+    const at::Tensor& xlw,
+    const at::Tensor& xl,
+    const at::Tensor& diffresult) {
+  RECORD_FUNCTION("ipex::mlperf_interaction_backward", c10::ArrayRef<c10::IValue>({}));
+  return mlperf_interaction_backward_kernel_stub(
+      kCPU, x0, xlw, xl, diffresult);
+}
+
+} // namespace cpu
+} // namespace torch_ipex
+
+namespace torch_ipex {
+namespace autocast {
+
+at::Tensor mlperf_interaction_forward(
+    const at::Tensor& x0,
+    const at::Tensor& xlw,
+    const at::Tensor& xl) {
+  c10::impl::ExcludeDispatchKeyGuard no_autocastCPU(DispatchKey::AutocastCPU);
+  static auto op =
+      torch::Dispatcher::singleton()
+          .findSchemaOrThrow("torch_ipex::mlperf_interaction_forward", "")
+          .typed<decltype(mlperf_interaction_forward)>();
+  return op.call(x0, xlw, xl);
+}
+
+} // namespace autocast
+} // namespace torch_ipex
+
+
+namespace {
+
+TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
+  m.def(
+      "mlperf_interaction_forward(Tensor x0, Tensor xlw, Tensor xl) -> Tensor");
+  m.impl(
+      "mlperf_interaction_forward",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::mlperf_interaction_forward_cpu);
+
+  m.def(
+      "mlperf_interaction_backward(Tensor x0, Tensor xlw, Tensor xl, Tensor diffresult) -> (Tensor, Tensor)");
+  m.impl(
+      "mlperf_interaction_backward",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::mlperf_interaction_backward_cpu);
+
+  m.impl(
+      "mlperf_interaction_forward",
+      c10::DispatchKey::AutocastCPU,
+      torch_ipex::autocast::mlperf_interaction_forward);
+}
+}
diff --git a/csrc/cpu/aten/MLPerfInteraction.h b/csrc/cpu/aten/MLPerfInteraction.h
new file mode 100644
index 000000000..18b92a32c
--- /dev/null
+++ b/csrc/cpu/aten/MLPerfInteraction.h
@@ -0,0 +1,37 @@
+#pragma once
+
+#include <ATen/ATen.h>
+#include <dyndisp/DispatchStub.h>
+#include <vector>
+#include <omp.h>
+#include <iostream>
+#include <chrono>
+#include <immintrin.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+
+at::Tensor mlperf_interaction_kernel_impl(
+    const at::Tensor& x0,
+    const at::Tensor& xlw,
+    const at::Tensor& xl);
+}
+
+using mlperf_interaction_kernel_fn = at::Tensor (*)(
+    const at::Tensor& x0,
+    const at::Tensor& xlw,
+    const at::Tensor& xl);
+
+using mlperf_interaction_backward_kernel_fn = std::tuple<at::Tensor, at::Tensor> (*)(
+    const at::Tensor& x0,
+    const at::Tensor& xlw,
+    const at::Tensor& xl,
+    const at::Tensor& diffresult);
+
+DECLARE_DISPATCH(mlperf_interaction_kernel_fn, mlperf_interaction_forward_kernel_stub);
+DECLARE_DISPATCH(mlperf_interaction_backward_kernel_fn, mlperf_interaction_backward_kernel_stub);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/MLPerfMergedEmbeddingBag.cpp b/csrc/cpu/aten/MLPerfMergedEmbeddingBag.cpp
new file mode 100644
index 000000000..aa6fd0cbe
--- /dev/null
+++ b/csrc/cpu/aten/MLPerfMergedEmbeddingBag.cpp
@@ -0,0 +1,79 @@
+#include <torch/all.h>
+#include "MLPerfMergedEmbeddingBag.h"
+#include <torch/csrc/autograd/function.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+DEFINE_DISPATCH(mlperf_merged_embeddingbag_forward_kernel_stub);
+DEFINE_DISPATCH(mlperf_merged_embeddingbag_backward_kernel_stub);
+
+at::Tensor mlperf_merged_embeddingbag_forward_cpu(
+    const std::vector<at::Tensor>& indices,
+    const std::vector<at::Tensor>& offsets,
+    const std::vector<at::Tensor>& weights,
+    const std::vector<int64_t>& multihot_size) {
+  RECORD_FUNCTION("ipex::mlperf_merged_embeddingbag_forward", c10::ArrayRef<c10::IValue>({}));
+
+  return mlperf_merged_embeddingbag_forward_kernel_stub(
+    kCPU, indices, offsets, weights, multihot_size);
+}
+
+void mlperf_merged_embeddingbag_backward_cpu(
+    const std::vector<at::Tensor>& indices,
+    const std::vector<at::Tensor>& offsets,
+    const std::vector<at::Tensor>& weights,
+    const std::vector<at::Tensor>& hessian,
+    const at::Tensor& diffresult,
+    const std::vector<int64_t>& multihot_size,
+    const std::vector<int64_t>& embedding_size,
+    double lr,
+    double eps) {
+  RECORD_FUNCTION("ipex::mlperf_merged_embeddingbag_backward", c10::ArrayRef<c10::IValue>({}));
+
+  mlperf_merged_embeddingbag_backward_kernel_stub(
+    kCPU, indices, offsets, weights, hessian, diffresult, multihot_size, embedding_size, lr, eps);
+  return;
+}
+
+} // namespace cpu
+} // namespace torch_ipex
+
+namespace torch_ipex {
+namespace autocast {
+
+at::Tensor mlperf_merged_embeddingbag_forward(
+    const std::vector<at::Tensor>& indices,
+    const std::vector<at::Tensor>& offsets,
+    const std::vector<at::Tensor>& weights,
+    const std::vector<int64_t>& multihot_size) {
+  c10::impl::ExcludeDispatchKeyGuard no_autocastCPU(DispatchKey::AutocastCPU);
+  static auto op =
+      torch::Dispatcher::singleton()
+          .findSchemaOrThrow("torch_ipex::mlperf_merged_embeddingbag_forward", "")
+          .typed<decltype(mlperf_merged_embeddingbag_forward)>();
+  return op.call(indices, offsets, weights, multihot_size);
+}
+
+} // namespace autocast
+} // namespace torch_ipex
+
+
+namespace {
+
+TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
+  m.def(
+      "mlperf_merged_embeddingbag_forward(Tensor []indices, Tensor []offsets, Tensor []weights, int[] multihot_size) -> Tensor");
+  m.impl(
+      "mlperf_merged_embeddingbag_forward",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::mlperf_merged_embeddingbag_forward_cpu);
+
+  m.def(
+      "mlperf_merged_embeddingbag_backward(Tensor []indices, Tensor []offsets, Tensor []weights, Tensor []hessian, Tensor diffresult, int[] multihot_size, int[] embedding_size, float lr, float eps) -> ()");
+  m.impl(
+      "mlperf_merged_embeddingbag_backward",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::mlperf_merged_embeddingbag_backward_cpu);
+}
+}
diff --git a/csrc/cpu/aten/MLPerfMergedEmbeddingBag.h b/csrc/cpu/aten/MLPerfMergedEmbeddingBag.h
new file mode 100644
index 000000000..2a0544fc3
--- /dev/null
+++ b/csrc/cpu/aten/MLPerfMergedEmbeddingBag.h
@@ -0,0 +1,44 @@
+#pragma once
+
+#include <ATen/ATen.h>
+#include <dyndisp/DispatchStub.h>
+#include <vector>
+#include <omp.h>
+#include <iostream>
+#include <chrono>
+#include <immintrin.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+
+at::Tensor mlperf_merged_embeddingbag_kernel_impl(
+    const std::vector<at::Tensor>& indices,
+    const std::vector<at::Tensor>& offsets,
+    const std::vector<at::Tensor>& weights,
+    const std::vector<int64_t>& multihot_size);
+}
+
+using mlperf_merged_embeddingbag_kernel_fn = at::Tensor (*)(
+    const std::vector<at::Tensor>& indices,
+    const std::vector<at::Tensor>& offsets,
+    const std::vector<at::Tensor>& weights,
+    const std::vector<int64_t>& multihot_size);
+
+using mlperf_merged_embeddingbag_backward_kernel_fn = void (*)(
+    const std::vector<at::Tensor>& indices,
+    const std::vector<at::Tensor>& offsets,
+    const std::vector<at::Tensor>& weights,
+    const std::vector<at::Tensor>& hessian,
+    const at::Tensor& diffresult,
+    const std::vector<int64_t>& multihot_size,
+    const std::vector<int64_t>& embedding_size,
+    double lr,
+    double eps);
+
+DECLARE_DISPATCH(mlperf_merged_embeddingbag_kernel_fn, mlperf_merged_embeddingbag_forward_kernel_stub);
+DECLARE_DISPATCH(mlperf_merged_embeddingbag_backward_kernel_fn, mlperf_merged_embeddingbag_backward_kernel_stub);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/MLPerfFreqEmbeddingBagKrnl.cpp b/csrc/cpu/aten/kernels/MLPerfFreqEmbeddingBagKrnl.cpp
new file mode 100644
index 000000000..17eca2e52
--- /dev/null
+++ b/csrc/cpu/aten/kernels/MLPerfFreqEmbeddingBagKrnl.cpp
@@ -0,0 +1,1690 @@
+#include <aten/MLPerfFreqEmbeddingBag.h>
+#include <torch/csrc/autograd/function.h>
+#include <torch/all.h>
+#include <ATen/Tensor.h>
+#include "robin_hood.h"
+#include "vec/vec.h"
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+#include <vector>
+#include <tuple>
+using std::vector;
+using std::tuple;
+
+typedef at::BFloat16 BF16; // only test
+
+#if (defined CPU_CAPABILITY_AVX512_BF16)
+inline __m512
+_mm512_cvtpbh_ps(__m256i x) {
+    return (__m512)_mm512_slli_epi32(_mm512_cvtepu16_epi32(x), 0x10);
+}
+#endif
+
+using EMBIDX = int64_t;
+using EMBWGT = float;
+using EMBHES = BF16;
+using EMBGRD = BF16;
+using EMBRES = BF16;
+using EMBACC = BF16;
+
+#if (defined CPU_CAPABILITY_AVX512_BF16)
+template<typename T, typename V, int64_t emb_dim>
+inline
+void copy_add(const T *src, V *dst);
+
+template<>
+inline
+void copy_add<float, float, 128>(const float *src, float *dst) {
+    __m512 a0, a1, a2, a3, a4, a5, a6, a7;
+    __m512 b0, b1, b2, b3, b4, b5, b6, b7;
+    a0 = _mm512_load_ps(src);
+    a1 = _mm512_load_ps(src + 16);
+    a2 = _mm512_load_ps(src + 32);
+    a3 = _mm512_load_ps(src + 48);
+    a4 = _mm512_load_ps(src + 64);
+    a5 = _mm512_load_ps(src + 80);
+    a6 = _mm512_load_ps(src + 96);
+    a7 = _mm512_load_ps(src + 112);
+    b0 = _mm512_load_ps(dst);
+    b1 = _mm512_load_ps(dst + 16);
+    b2 = _mm512_load_ps(dst + 32);
+    b3 = _mm512_load_ps(dst + 48);
+    b4 = _mm512_load_ps(dst + 64);
+    b5 = _mm512_load_ps(dst + 80);
+    b6 = _mm512_load_ps(dst + 96);
+    b7 = _mm512_load_ps(dst + 112);
+    b0 = _mm512_add_ps(a0, b0);
+    b1 = _mm512_add_ps(a1, b1);
+    b2 = _mm512_add_ps(a2, b2);
+    b3 = _mm512_add_ps(a3, b3);
+    b4 = _mm512_add_ps(a4, b4);
+    b5 = _mm512_add_ps(a5, b5);
+    b6 = _mm512_add_ps(a6, b6);
+    b7 = _mm512_add_ps(a7, b7);
+    _mm512_store_ps(dst, b0);
+    _mm512_store_ps(dst + 16, b1);
+    _mm512_store_ps(dst + 32, b2);
+    _mm512_store_ps(dst + 48, b3);
+    _mm512_store_ps(dst + 64, b4);
+    _mm512_store_ps(dst + 80, b5);
+    _mm512_store_ps(dst + 96, b6);
+    _mm512_store_ps(dst + 112, b7);
+}
+#endif
+
+/*
+ * W datatype for weight
+ * H datatype for hessian
+ */
+template<typename W, typename H>
+inline
+void adagradupdatesparse(W *weight, H *hessian,
+                         __m512 lr, __m512 eps,
+                         __m512 g0, __m512 g1, __m512 g2, __m512 g3,
+                         __m512 g4, __m512 g5, __m512 g6, __m512 g7);
+
+template<>
+inline
+void adagradupdatesparse<float, float>(float *weight, float *hessian,
+                                       __m512 lr, __m512 eps,
+                                       __m512 g0, __m512 g1, __m512 g2, __m512 g3,
+                                       __m512 g4, __m512 g5, __m512 g6, __m512 g7) {
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    // hessian += grad**2
+    // weight += grad * lr / (sqrt(hessian) + eps)
+    // lr < 0
+    __m512 h0, h1, h2, h3, h4, h5, h6, h7;
+    __m512 w0, w1, w2, w3, w4, w5, w6, w7;
+    h0 = _mm512_load_ps(hessian);
+    h1 = _mm512_load_ps(hessian + 16);
+    h2 = _mm512_load_ps(hessian + 32);
+    h3 = _mm512_load_ps(hessian + 48);
+    h4 = _mm512_load_ps(hessian + 64);
+    h5 = _mm512_load_ps(hessian + 80);
+    h6 = _mm512_load_ps(hessian + 96);
+    h7 = _mm512_load_ps(hessian + 112);
+
+    h0 = _mm512_fmadd_ps(g0, g0, h0);
+    h1 = _mm512_fmadd_ps(g1, g1, h1);
+    h2 = _mm512_fmadd_ps(g2, g2, h2);
+    h3 = _mm512_fmadd_ps(g3, g3, h3);
+    h4 = _mm512_fmadd_ps(g4, g4, h4);
+    h5 = _mm512_fmadd_ps(g5, g5, h5);
+    h6 = _mm512_fmadd_ps(g6, g6, h6);
+    h7 = _mm512_fmadd_ps(g7, g7, h7);
+
+    _mm512_store_ps(hessian, h0);
+    _mm512_store_ps(hessian + 16, h1);
+    _mm512_store_ps(hessian + 32, h2);
+    _mm512_store_ps(hessian + 48, h3);
+    _mm512_store_ps(hessian + 64, h4);
+    _mm512_store_ps(hessian + 80, h5);
+    _mm512_store_ps(hessian + 96, h6);
+    _mm512_store_ps(hessian + 112, h7);
+
+    w0 = _mm512_load_ps(weight);
+    w1 = _mm512_load_ps(weight + 16);
+    w2 = _mm512_load_ps(weight + 32);
+    w3 = _mm512_load_ps(weight + 48);
+    w4 = _mm512_load_ps(weight + 64);
+    w5 = _mm512_load_ps(weight + 80);
+    w6 = _mm512_load_ps(weight + 96);
+    w7 = _mm512_load_ps(weight + 112);
+    // accurate
+    if (true) {
+    h0 = _mm512_sqrt_ps(h0);
+    h1 = _mm512_sqrt_ps(h1);
+    h2 = _mm512_sqrt_ps(h2);
+    h3 = _mm512_sqrt_ps(h3);
+    h4 = _mm512_sqrt_ps(h4);
+    h5 = _mm512_sqrt_ps(h5);
+    h6 = _mm512_sqrt_ps(h6);
+    h7 = _mm512_sqrt_ps(h7);
+
+    h0 = _mm512_add_ps(h0, eps);
+    h1 = _mm512_add_ps(h1, eps);
+    h2 = _mm512_add_ps(h2, eps);
+    h3 = _mm512_add_ps(h3, eps);
+    h4 = _mm512_add_ps(h4, eps);
+    h5 = _mm512_add_ps(h5, eps);
+    h6 = _mm512_add_ps(h6, eps);
+    h7 = _mm512_add_ps(h7, eps);
+
+    h0 = _mm512_div_ps(lr, h0);
+    h1 = _mm512_div_ps(lr, h1);
+    h2 = _mm512_div_ps(lr, h2);
+    h3 = _mm512_div_ps(lr, h3);
+    h4 = _mm512_div_ps(lr, h4);
+    h5 = _mm512_div_ps(lr, h5);
+    h6 = _mm512_div_ps(lr, h6);
+    h7 = _mm512_div_ps(lr, h7);
+    } else {
+    // // fast
+    h0 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h0));
+    h1 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h1));
+    h2 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h2));
+    h3 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h3));
+    h4 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h4));
+    h5 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h5));
+    h6 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h6));
+    h7 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h7));
+    }
+    h0 = _mm512_mul_ps(h0, g0);
+    h1 = _mm512_mul_ps(h1, g1);
+    h2 = _mm512_mul_ps(h2, g2);
+    h3 = _mm512_mul_ps(h3, g3);
+    h4 = _mm512_mul_ps(h4, g4);
+    h5 = _mm512_mul_ps(h5, g5);
+    h6 = _mm512_mul_ps(h6, g6);
+    h7 = _mm512_mul_ps(h7, g7);
+
+    w0 = _mm512_add_ps(w0, h0);
+    w1 = _mm512_add_ps(w1, h1);
+    w2 = _mm512_add_ps(w2, h2);
+    w3 = _mm512_add_ps(w3, h3);
+    w4 = _mm512_add_ps(w4, h4);
+    w5 = _mm512_add_ps(w5, h5);
+    w6 = _mm512_add_ps(w6, h6);
+    w7 = _mm512_add_ps(w7, h7);
+
+    _mm512_store_ps(weight, w0);
+    _mm512_store_ps(weight + 16, w1);
+    _mm512_store_ps(weight + 32, w2);
+    _mm512_store_ps(weight + 48, w3);
+    _mm512_store_ps(weight + 64, w4);
+    _mm512_store_ps(weight + 80, w5);
+    _mm512_store_ps(weight + 96, w6);
+    _mm512_store_ps(weight + 112, w7);
+    #endif
+}
+
+template<>
+inline
+void adagradupdatesparse<float, BF16>(float *weight, BF16 *hessian,
+                                      __m512 lr, __m512 eps,
+                                      __m512 g0, __m512 g1, __m512 g2, __m512 g3,
+                                      __m512 g4, __m512 g5, __m512 g6, __m512 g7) {
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    // hessian += grad**2
+    // weight += grad * lr / (sqrt(hessian) + eps)
+    // lr < 0
+    __m512i b0, b1, b2, b3;
+    __m512 h0, h1, h2, h3, h4, h5, h6, h7;
+    __m512 w0, w1, w2, w3, w4, w5, w6, w7;
+    b0 = _mm512_load_si512(hessian);
+    b1 = _mm512_load_si512(hessian + 32);
+    b2 = _mm512_load_si512(hessian + 64);
+    b3 = _mm512_load_si512(hessian + 96);
+
+    h0 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b0, 0));
+    h1 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b0, 1));
+    h2 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b1, 0));
+    h3 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b1, 1));
+    h4 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b2, 0));
+    h5 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b2, 1));
+    h6 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b3, 0));
+    h7 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b3, 1));
+
+    h0 = _mm512_fmadd_ps(g0, g0, h0);
+    h1 = _mm512_fmadd_ps(g1, g1, h1);
+    h2 = _mm512_fmadd_ps(g2, g2, h2);
+    h3 = _mm512_fmadd_ps(g3, g3, h3);
+    h4 = _mm512_fmadd_ps(g4, g4, h4);
+    h5 = _mm512_fmadd_ps(g5, g5, h5);
+    h6 = _mm512_fmadd_ps(g6, g6, h6);
+    h7 = _mm512_fmadd_ps(g7, g7, h7);
+
+    _mm512_store_si512(hessian, (__m512i)_mm512_cvtne2ps_pbh(h1, h0));
+    _mm512_store_si512(hessian + 32, (__m512i)_mm512_cvtne2ps_pbh(h3, h2));
+    _mm512_store_si512(hessian + 64, (__m512i)_mm512_cvtne2ps_pbh(h5, h4));
+    _mm512_store_si512(hessian + 96, (__m512i)_mm512_cvtne2ps_pbh(h7, h6));
+
+    w0 = _mm512_load_ps(weight);
+    w1 = _mm512_load_ps(weight + 16);
+    w2 = _mm512_load_ps(weight + 32);
+    w3 = _mm512_load_ps(weight + 48);
+    w4 = _mm512_load_ps(weight + 64);
+    w5 = _mm512_load_ps(weight + 80);
+    w6 = _mm512_load_ps(weight + 96);
+    w7 = _mm512_load_ps(weight + 112);
+    // accurate
+    if (true) {
+    h0 = _mm512_sqrt_ps(h0);
+    h1 = _mm512_sqrt_ps(h1);
+    h2 = _mm512_sqrt_ps(h2);
+    h3 = _mm512_sqrt_ps(h3);
+    h4 = _mm512_sqrt_ps(h4);
+    h5 = _mm512_sqrt_ps(h5);
+    h6 = _mm512_sqrt_ps(h6);
+    h7 = _mm512_sqrt_ps(h7);
+
+    h0 = _mm512_add_ps(h0, eps);
+    h1 = _mm512_add_ps(h1, eps);
+    h2 = _mm512_add_ps(h2, eps);
+    h3 = _mm512_add_ps(h3, eps);
+    h4 = _mm512_add_ps(h4, eps);
+    h5 = _mm512_add_ps(h5, eps);
+    h6 = _mm512_add_ps(h6, eps);
+    h7 = _mm512_add_ps(h7, eps);
+
+    h0 = _mm512_div_ps(lr, h0);
+    h1 = _mm512_div_ps(lr, h1);
+    h2 = _mm512_div_ps(lr, h2);
+    h3 = _mm512_div_ps(lr, h3);
+    h4 = _mm512_div_ps(lr, h4);
+    h5 = _mm512_div_ps(lr, h5);
+    h6 = _mm512_div_ps(lr, h6);
+    h7 = _mm512_div_ps(lr, h7);
+    } else {
+    // // fast
+    h0 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h0));
+    h1 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h1));
+    h2 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h2));
+    h3 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h3));
+    h4 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h4));
+    h5 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h5));
+    h6 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h6));
+    h7 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h7));
+    }
+    h0 = _mm512_mul_ps(h0, g0);
+    h1 = _mm512_mul_ps(h1, g1);
+    h2 = _mm512_mul_ps(h2, g2);
+    h3 = _mm512_mul_ps(h3, g3);
+    h4 = _mm512_mul_ps(h4, g4);
+    h5 = _mm512_mul_ps(h5, g5);
+    h6 = _mm512_mul_ps(h6, g6);
+    h7 = _mm512_mul_ps(h7, g7);
+
+    w0 = _mm512_add_ps(w0, h0);
+    w1 = _mm512_add_ps(w1, h1);
+    w2 = _mm512_add_ps(w2, h2);
+    w3 = _mm512_add_ps(w3, h3);
+    w4 = _mm512_add_ps(w4, h4);
+    w5 = _mm512_add_ps(w5, h5);
+    w6 = _mm512_add_ps(w6, h6);
+    w7 = _mm512_add_ps(w7, h7);
+
+    _mm512_store_ps(weight, w0);
+    _mm512_store_ps(weight + 16, w1);
+    _mm512_store_ps(weight + 32, w2);
+    _mm512_store_ps(weight + 48, w3);
+    _mm512_store_ps(weight + 64, w4);
+    _mm512_store_ps(weight + 80, w5);
+    _mm512_store_ps(weight + 96, w6);
+    _mm512_store_ps(weight + 112, w7);
+    #endif
+}
+
+template<typename T, typename V, typename W>
+inline
+void adagradupdatedense(T *weight, V *hessian, const W *grad,
+                        __m512 lr, __m512 eps);
+
+template<>
+inline
+void adagradupdatedense<float, float, float>(float *weight,
+                                             float *hessian,
+                                             const float *grad,
+                                             __m512 lr, __m512 eps) {
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    // hessian += grad**2
+    // weight += grad * lr / (sqrt(hessian) + eps)
+    // lr < 0
+    __m512 h0, h1, h2, h3, h4, h5, h6, h7;
+    __m512 w0, w1, w2, w3, w4, w5, w6, w7;
+    __m512 g0, g1, g2, g3, g4, g5, g6, g7;
+    g0 = _mm512_load_ps(grad);
+    g1 = _mm512_load_ps(grad + 16);
+    g2 = _mm512_load_ps(grad + 32);
+    g3 = _mm512_load_ps(grad + 48);
+    g4 = _mm512_load_ps(grad + 64);
+    g5 = _mm512_load_ps(grad + 80);
+    g6 = _mm512_load_ps(grad + 96);
+    g7 = _mm512_load_ps(grad + 112);
+
+    h0 = _mm512_load_ps(hessian);
+    h1 = _mm512_load_ps(hessian + 16);
+    h2 = _mm512_load_ps(hessian + 32);
+    h3 = _mm512_load_ps(hessian + 48);
+    h4 = _mm512_load_ps(hessian + 64);
+    h5 = _mm512_load_ps(hessian + 80);
+    h6 = _mm512_load_ps(hessian + 96);
+    h7 = _mm512_load_ps(hessian + 112);
+
+    h0 = _mm512_fmadd_ps(g0, g0, h0);
+    h1 = _mm512_fmadd_ps(g1, g1, h1);
+    h2 = _mm512_fmadd_ps(g2, g2, h2);
+    h3 = _mm512_fmadd_ps(g3, g3, h3);
+    h4 = _mm512_fmadd_ps(g4, g4, h4);
+    h5 = _mm512_fmadd_ps(g5, g5, h5);
+    h6 = _mm512_fmadd_ps(g6, g6, h6);
+    h7 = _mm512_fmadd_ps(g7, g7, h7);
+    _mm512_store_ps(hessian, h0);
+    _mm512_store_ps(hessian + 16, h1);
+    _mm512_store_ps(hessian + 32, h2);
+    _mm512_store_ps(hessian + 48, h3);
+    _mm512_store_ps(hessian + 64, h4);
+    _mm512_store_ps(hessian + 80, h5);
+    _mm512_store_ps(hessian + 96, h6);
+    _mm512_store_ps(hessian + 112, h7);
+    w0 = _mm512_load_ps(weight);
+    w1 = _mm512_load_ps(weight + 16);
+    w2 = _mm512_load_ps(weight + 32);
+    w3 = _mm512_load_ps(weight + 48);
+    w4 = _mm512_load_ps(weight + 64);
+    w5 = _mm512_load_ps(weight + 80);
+    w6 = _mm512_load_ps(weight + 96);
+    w7 = _mm512_load_ps(weight + 112);
+    // accurate
+    if (true) {
+    h0 = _mm512_sqrt_ps(h0);
+    h1 = _mm512_sqrt_ps(h1);
+    h2 = _mm512_sqrt_ps(h2);
+    h3 = _mm512_sqrt_ps(h3);
+    h4 = _mm512_sqrt_ps(h4);
+    h5 = _mm512_sqrt_ps(h5);
+    h6 = _mm512_sqrt_ps(h6);
+    h7 = _mm512_sqrt_ps(h7);
+
+    h0 = _mm512_add_ps(h0, eps);
+    h1 = _mm512_add_ps(h1, eps);
+    h2 = _mm512_add_ps(h2, eps);
+    h3 = _mm512_add_ps(h3, eps);
+    h4 = _mm512_add_ps(h4, eps);
+    h5 = _mm512_add_ps(h5, eps);
+    h6 = _mm512_add_ps(h6, eps);
+    h7 = _mm512_add_ps(h7, eps);
+
+    h0 = _mm512_div_ps(lr, h0);
+    h1 = _mm512_div_ps(lr, h1);
+    h2 = _mm512_div_ps(lr, h2);
+    h3 = _mm512_div_ps(lr, h3);
+    h4 = _mm512_div_ps(lr, h4);
+    h5 = _mm512_div_ps(lr, h5);
+    h6 = _mm512_div_ps(lr, h6);
+    h7 = _mm512_div_ps(lr, h7);
+    } else {
+    // fast
+
+    h0 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h0));
+    h1 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h1));
+    h2 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h2));
+    h3 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h3));
+    h4 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h4));
+    h5 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h5));
+    h6 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h6));
+    h7 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h7));
+    }
+    h0 = _mm512_mul_ps(h0, g0);
+    h1 = _mm512_mul_ps(h1, g1);
+    h2 = _mm512_mul_ps(h2, g2);
+    h3 = _mm512_mul_ps(h3, g3);
+    h4 = _mm512_mul_ps(h4, g4);
+    h5 = _mm512_mul_ps(h5, g5);
+    h6 = _mm512_mul_ps(h6, g6);
+    h7 = _mm512_mul_ps(h7, g7);
+    w0 = _mm512_add_ps(w0, h0);
+    w1 = _mm512_add_ps(w1, h1);
+    w2 = _mm512_add_ps(w2, h2);
+    w3 = _mm512_add_ps(w3, h3);
+    w4 = _mm512_add_ps(w4, h4);
+    w5 = _mm512_add_ps(w5, h5);
+    w6 = _mm512_add_ps(w6, h6);
+    w7 = _mm512_add_ps(w7, h7);
+    _mm512_store_ps(weight, w0);
+    _mm512_store_ps(weight + 16, w1);
+    _mm512_store_ps(weight + 32, w2);
+    _mm512_store_ps(weight + 48, w3);
+    _mm512_store_ps(weight + 64, w4);
+    _mm512_store_ps(weight + 80, w5);
+    _mm512_store_ps(weight + 96, w6);
+    _mm512_store_ps(weight + 112, w7);
+    #endif
+}
+
+struct __attribute__((packed, aligned(64))) EMBROW {
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    __m512 a0;
+    __m512 a1;
+    __m512 a2;
+    __m512 a3;
+    __m512 a4;
+    __m512 a5;
+    __m512 a6;
+    __m512 a7;
+    #endif
+} ;
+
+// using SparseEmbeddingBag = emilib::HashMap<EMBIDX, EMBROW>;
+using SparseEmbeddingBag = robin_hood::unordered_map<EMBIDX, EMBROW>;
+
+template<typename T, int64_t emb_dim>
+inline
+void spsfwd_local_cache(const int64_t num_emb,
+                        const int64_t num_chk,
+                        const int64_t gbatch,
+                        const int64_t lbatch,
+                        const int64_t rank,
+                        const int64_t size,
+                        vector<SparseEmbeddingBag> &cache,
+                        const EMBIDX *multi_hots,
+                        const vector<EMBIDX *> &indices,
+                        const EMBIDX *idx_offset,
+                        const EMBIDX *lookup,
+                        const T *sparse_wgt);
+template<>
+inline
+void spsfwd_local_cache<float, 128>(const int64_t num_emb,
+                                    const int64_t num_chk,
+                                    const int64_t gbatch,
+                                    const int64_t lbatch,
+                                    const int64_t rank,
+                                    const int64_t size,
+                                    vector<SparseEmbeddingBag> &embcache,
+                                    const EMBIDX *multi_hots,
+                                    const vector<EMBIDX *> &indices,
+                                    const EMBIDX *idx_offset,
+                                    const EMBIDX *lookup,
+                                    const float *sparse_wgt) {
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    constexpr int64_t emb_dim = 128;
+    const int64_t tbatch = (gbatch - 1) / num_chk + 1;
+    #pragma omp parallel for collapse(2) schedule(guided) shared(embcache)
+    for (int64_t n = 0; n < num_emb; ++n) {
+        for (int64_t nc = 0; nc < num_chk; ++nc) {
+            int64_t tbs = nc * tbatch;
+            int64_t tbe = std::min(tbs + tbatch, gbatch);
+            __m512 x0, x1, x2, x3, x4, x5, x6, x7;
+            EMBIDX *index = indices[n];
+            EMBIDX multi_hot = multi_hots[n];
+            EMBIDX offset = idx_offset[n];
+            for (int64_t gb = tbs; gb < tbe; ++gb) {
+                for (int64_t h = 0; h < multi_hot; ++h) {
+                    EMBIDX emb_idx = index[gb * multi_hot + h] + offset;
+                    // if ((lookup[emb_idx] < 0) && (emb_idx % size == rank)) { need to be fixed for dense
+                    if (emb_idx % size == rank) {
+                        EMBIDX dest = gb / lbatch;
+                        EMBIDX rowi = (gb % lbatch * num_emb) + n;
+                        SparseEmbeddingBag &cache = embcache[dest * num_emb * num_chk +
+                                                             nc * num_emb + n];
+                        EMBIDX loc_se_idx = emb_idx / size;
+                        auto find = cache.find(rowi);
+                        bool miss = (find == cache.end());
+                        const float *wgtPtr = &sparse_wgt[loc_se_idx * emb_dim];
+                        x0 = _mm512_load_ps(wgtPtr);
+                        x1 = _mm512_load_ps(wgtPtr + 16);
+                        x2 = _mm512_load_ps(wgtPtr + 32);
+                        x3 = _mm512_load_ps(wgtPtr + 48);
+                        x4 = _mm512_load_ps(wgtPtr + 64);
+                        x5 = _mm512_load_ps(wgtPtr + 80);
+                        x6 = _mm512_load_ps(wgtPtr + 96);
+                        x7 = _mm512_load_ps(wgtPtr + 112);
+                        if (miss) {
+                            EMBROW w = {x0, x1, x2, x3, x4, x5, x6, x7};
+                            cache.emplace(rowi, w);
+                        } else {
+                            auto &cache_wgt = find->second;
+                            cache_wgt.a0 = _mm512_add_ps(cache_wgt.a0, x0);
+                            cache_wgt.a1 = _mm512_add_ps(cache_wgt.a1, x1);
+                            cache_wgt.a2 = _mm512_add_ps(cache_wgt.a2, x2);
+                            cache_wgt.a3 = _mm512_add_ps(cache_wgt.a3, x3);
+                            cache_wgt.a4 = _mm512_add_ps(cache_wgt.a4, x4);
+                            cache_wgt.a5 = _mm512_add_ps(cache_wgt.a5, x5);
+                            cache_wgt.a6 = _mm512_add_ps(cache_wgt.a6, x6);
+                            cache_wgt.a7 = _mm512_add_ps(cache_wgt.a7, x7);
+                        }
+                    }
+                }
+        }
+        }
+    }
+    #endif
+}
+
+template<int64_t EMBDIM>
+void fwd_cache_merge(const int64_t size, const int64_t num_emb, const int64_t num_chk,
+                     const vector<SparseEmbeddingBag> &cache,
+                     vector<SparseEmbeddingBag> &mcache) {
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    #pragma omp parallel for collapse(2) schedule(guided)
+    for (int64_t dest = 0; dest < size; ++dest) {
+        for (int64_t n = 0; n < num_emb; ++n) {
+            SparseEmbeddingBag &dst_map = mcache[dest * num_emb + n];
+            for (int64_t nc = 0; nc < num_chk; ++nc) {
+                const SparseEmbeddingBag &src_map = cache[dest * num_emb * num_chk +
+                                                          nc * num_emb + n];
+                for (const auto &[k, v]:src_map) {
+                    auto find = dst_map.find(k);
+                    if (find == dst_map.end()) {
+                        // not find
+                        dst_map.emplace(k, v);
+                    } else {
+                        auto &cache_wgt = find->second;
+                        cache_wgt.a0 = _mm512_add_ps(cache_wgt.a0, v.a0);
+                        cache_wgt.a1 = _mm512_add_ps(cache_wgt.a1, v.a1);
+                        cache_wgt.a2 = _mm512_add_ps(cache_wgt.a2, v.a2);
+                        cache_wgt.a3 = _mm512_add_ps(cache_wgt.a3, v.a3);
+                        cache_wgt.a4 = _mm512_add_ps(cache_wgt.a4, v.a4);
+                        cache_wgt.a5 = _mm512_add_ps(cache_wgt.a5, v.a5);
+                        cache_wgt.a6 = _mm512_add_ps(cache_wgt.a6, v.a6);
+                        cache_wgt.a7 = _mm512_add_ps(cache_wgt.a7, v.a7);
+                    }
+                }
+            }
+        }
+    }
+    #endif
+}
+
+/*
+ * V datatype for exchange buffer
+ */
+template<typename V, int64_t EMBDIM>
+inline
+void sparse_local_merge(const int64_t out_dim,
+                        const int64_t inn_dim,
+                        const EMBIDX *split_ptr,
+                        const vector<SparseEmbeddingBag> &cache,
+                        const vector<EMBIDX *> &index,
+                        const vector<V *> &value,
+                        const vector<int64_t *> &offset);
+
+template<>
+inline
+void sparse_local_merge<float, 128>(const int64_t out_dim,
+                                    const int64_t inn_dim,
+                                    const EMBIDX *split_ptr,
+                                    const vector<SparseEmbeddingBag> &cache,
+                                    const vector<EMBIDX *> &index,
+                                    const vector<float *> &data,
+                                    const vector<int64_t *> &offset) {
+    /*
+     * out_dim: mpi size
+     * inn_dim: num of threads or num of embs
+     */
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    constexpr int64_t EMBDIM = 128;
+    #pragma omp parallel for collapse(2)
+    for (int64_t i = 0; i < inn_dim; ++i) {
+        for (int64_t o = 0; o < out_dim; ++o) {
+            if (split_ptr[o] > 0) {
+                size_t j = offset[o][i];
+                for (auto &[key, value] : cache[o * inn_dim + i]) {
+                    index[o][j] = key;
+                    float * bufPtr = &data[o][j * EMBDIM];
+                    _mm512_store_ps(bufPtr, value.a0);
+                    _mm512_store_ps(bufPtr + 16, value.a1);
+                    _mm512_store_ps(bufPtr + 32, value.a2);
+                    _mm512_store_ps(bufPtr + 48, value.a3);
+                    _mm512_store_ps(bufPtr + 64, value.a4);
+                    _mm512_store_ps(bufPtr + 80, value.a5);
+                    _mm512_store_ps(bufPtr + 96, value.a6);
+                    _mm512_store_ps(bufPtr + 112, value.a7);
+                    j++;
+                }
+            }
+        }
+    }
+    #endif
+}
+
+template<>
+inline
+void sparse_local_merge<BF16, 128>(const int64_t out_dim,
+                                   const int64_t inn_dim,
+                                   const EMBIDX *split_ptr,
+                                   const vector<SparseEmbeddingBag> &cache,
+                                   const vector<EMBIDX *> &index,
+                                   const vector<BF16 *> &data,
+                                   const vector<int64_t *> &offset) {
+    /*
+     * out_dim: mpi size
+     * inn_dim: num of threads or num of embs
+     */
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    constexpr int64_t EMBDIM = 128;
+    #pragma omp parallel for collapse(2)
+    for (int64_t i = 0; i < inn_dim; ++i) {
+        for (int64_t o = 0; o < out_dim; ++o) {
+            if (split_ptr[o] > 0) {
+                size_t j = offset[o][i];
+                for (auto &[key, value] : cache[o * inn_dim + i]) {
+                    index[o][j] = key;
+                    BF16 * bufPtr = &data[o][j * EMBDIM];
+                    _mm512_store_si512(bufPtr, (__m512i)_mm512_cvtne2ps_pbh(value.a1, value.a0));
+                    _mm512_store_si512(bufPtr + 32, (__m512i)_mm512_cvtne2ps_pbh(value.a3, value.a2));
+                    _mm512_store_si512(bufPtr + 64, (__m512i)_mm512_cvtne2ps_pbh(value.a5, value.a4));
+                    _mm512_store_si512(bufPtr + 96, (__m512i)_mm512_cvtne2ps_pbh(value.a7, value.a6));
+                    j++;
+                }
+            }
+        }
+    }
+    #endif
+}
+
+/*
+ * X datatype for exchange buffer
+ */
+template<typename X, int64_t emb_dim>
+inline
+void sparse_init_merge(const int64_t out_dim, // mpi size
+                       const int64_t inn_dim, // inner dim to be joined thread or 
+                       EMBIDX *split_ptr,
+                       vector<SparseEmbeddingBag> &cache,
+                       vector<at::Tensor> &sps_idx,
+                       vector<at::Tensor> &sps_val,
+                       vector<at::Tensor> &sps_ofs,
+                       vector<EMBIDX *> &idx_ptr,
+                       vector<X *> &val_ptr,
+                       vector<int64_t *> &ofs_ptr);
+template<>
+inline
+void sparse_init_merge<float, 128>(const int64_t out_dim, // mpi size
+                                   const int64_t inn_dim, // inner dim to be joined thread or 
+                                   EMBIDX *split_ptr,
+                                   vector<SparseEmbeddingBag> &cache,
+                                   vector<at::Tensor> &sps_idx,
+                                   vector<at::Tensor> &sps_val,
+                                   vector<at::Tensor> &sps_ofs,
+                                   vector<EMBIDX *> &idx_ptr,
+                                   vector<float *> &val_ptr, // EMBACC
+                                   vector<int64_t *> &ofs_ptr) {
+    constexpr int64_t emb_dim = 128;
+    /*
+     * out_dim: mpi size
+     * inn_dim: num of threads or num of embs
+     */
+    for (int64_t i = 0; i < out_dim; ++i) {
+        sps_ofs[i] = at::empty({inn_dim + 1}, torch::kInt64);
+        int64_t *ofs_ptr=sps_ofs[i].data_ptr<int64_t>();
+        EMBIDX s = 0;
+        for (int64_t n = 0; n < inn_dim; ++n) {
+            ofs_ptr[n] = s;
+            s += cache[i * inn_dim + n].size();
+        }
+        ofs_ptr[inn_dim] = s;
+        split_ptr[i] = s;
+        // workaround for torchccl bug
+        if (s == 0) {
+            sps_val[i] = at::empty({1}, torch::kFloat);
+            sps_idx[i] = at::empty({1}, torch::kInt64);
+        } else {
+            sps_val[i] = at::empty({s, emb_dim}, torch::kFloat);
+            sps_idx[i] = at::empty({s}, torch::kInt64);
+        }
+    }
+    for (int64_t i = 0; i < out_dim; ++i) {
+        val_ptr[i] = sps_val[i].data_ptr<float>(); // EMBACC
+        idx_ptr[i] = sps_idx[i].data_ptr<EMBIDX>();
+        ofs_ptr[i] = sps_ofs[i].data_ptr<int64_t>();
+    }
+}
+
+template<>
+inline
+void sparse_init_merge<BF16, 128>(const int64_t out_dim, // mpi size
+                                  const int64_t inn_dim, // inner dim to be joined thread or 
+                                  EMBIDX *split_ptr,
+                                  vector<SparseEmbeddingBag> &cache,
+                                  vector<at::Tensor> &sps_idx,
+                                  vector<at::Tensor> &sps_val,
+                                  vector<at::Tensor> &sps_ofs,
+                                  vector<EMBIDX *> &idx_ptr,
+                                  vector<BF16 *> &val_ptr, // EMBACC
+                                  vector<int64_t *> &ofs_ptr) {
+    constexpr int64_t emb_dim = 128;
+    /*
+     * out_dim: mpi size
+     * inn_dim: num of threads or num of embs
+     */
+    for (int64_t i = 0; i < out_dim; ++i) {
+        sps_ofs[i] = at::empty({inn_dim + 1}, torch::kInt64);
+        int64_t *ofs_ptr=sps_ofs[i].data_ptr<int64_t>();
+        EMBIDX s = 0;
+        for (int64_t n = 0; n < inn_dim; ++n) {
+            ofs_ptr[n] = s;
+            s += cache[i * inn_dim + n].size();
+        }
+        ofs_ptr[inn_dim] = s;
+        split_ptr[i] = s;
+        // workaround for torchccl bug
+        if (s == 0) {
+            sps_val[i] = at::empty({1}, torch::kBFloat16);
+            sps_idx[i] = at::empty({1}, torch::kInt64);
+        } else {
+            sps_val[i] = at::empty({s, emb_dim}, torch::kBFloat16);
+            sps_idx[i] = at::empty({s}, torch::kInt64);
+        }
+    }
+    for (int64_t i = 0; i < out_dim; ++i) {
+        val_ptr[i] = sps_val[i].data_ptr<BF16>(); // EMBACC
+        idx_ptr[i] = sps_idx[i].data_ptr<EMBIDX>();
+        ofs_ptr[i] = sps_ofs[i].data_ptr<int64_t>();
+    }
+}
+
+/*
+ * T: datatype for accumulation
+ * V: datatype for result
+ */
+template<typename T, typename V, int64_t emb_dim>
+void
+sparse_forward_merge(const int64_t size,
+                     const int64_t num_emb,
+                     vector<EMBIDX *> &idx_ptr,
+                     vector<T *> &val_ptr,
+                     vector<int64_t *> &ofs_ptr,
+                     V *res_ptr);
+template<>
+void
+sparse_forward_merge<float, float, 128>(const int64_t size,
+                                        const int64_t num_emb,
+                                        vector<EMBIDX *> &idx_ptr,
+                                        vector<float *> &val_ptr, // EMBACC
+                                        vector<int64_t *> &ofs_ptr,
+                                        float *res_ptr) { // EMBRES
+    constexpr int64_t emb_dim = 128;
+#if (defined CPU_CAPABILITY_AVX512_BF16)
+    #pragma omp parallel for
+    for (int64_t i = 0; i < num_emb; ++i) {
+        SparseEmbeddingBag cache;
+        __m512 x0, x1, x2, x3, x4, x5, x6, x7;
+        for (int64_t j = 0; j < size; ++j) {
+            const int64_t ts = ofs_ptr[j][i];
+            const int64_t te = ofs_ptr[j][i + 1];
+            for (int64_t k = ts; k < te; ++k) {
+                EMBIDX rowi = idx_ptr[j][k];
+                const float *accPtr = &val_ptr[j][k * emb_dim]; // EMBACC
+                auto find = cache.find(rowi);
+                bool miss = (find == cache.end());
+                x0 = _mm512_load_ps(accPtr);
+                x1 = _mm512_load_ps(accPtr + 16);
+                x2 = _mm512_load_ps(accPtr + 32);
+                x3 = _mm512_load_ps(accPtr + 48);
+                x4 = _mm512_load_ps(accPtr + 64);
+                x5 = _mm512_load_ps(accPtr + 80);
+                x6 = _mm512_load_ps(accPtr + 96);
+                x7 = _mm512_load_ps(accPtr + 112);
+                if (miss) {
+                    EMBROW w = {x0, x1, x2, x3, x4, x5, x6, x7};
+                    cache.emplace(rowi, w);
+                } else {
+                    auto &cache_acc = find->second;
+                    cache_acc.a0 = _mm512_add_ps(cache_acc.a0, x0);
+                    cache_acc.a1 = _mm512_add_ps(cache_acc.a1, x1);
+                    cache_acc.a2 = _mm512_add_ps(cache_acc.a2, x2);
+                    cache_acc.a3 = _mm512_add_ps(cache_acc.a3, x3);
+                    cache_acc.a4 = _mm512_add_ps(cache_acc.a4, x4);
+                    cache_acc.a5 = _mm512_add_ps(cache_acc.a5, x5);
+                    cache_acc.a6 = _mm512_add_ps(cache_acc.a6, x6);
+                    cache_acc.a7 = _mm512_add_ps(cache_acc.a7, x7);
+                }
+            }
+        }
+        for (auto &[key, value] : cache) {
+            float *dest = &res_ptr[key * emb_dim]; // EMBRES
+            _mm512_store_ps(dest, value.a0);
+            _mm512_store_ps(dest + 16, value.a1);
+            _mm512_store_ps(dest + 32, value.a2);
+            _mm512_store_ps(dest + 48, value.a3);
+            _mm512_store_ps(dest + 64, value.a4);
+            _mm512_store_ps(dest + 80, value.a5);
+            _mm512_store_ps(dest + 96, value.a6);
+            _mm512_store_ps(dest + 112, value.a7);
+        }
+    }
+    #endif
+}
+
+template<>
+void
+sparse_forward_merge<float, BF16, 128>(const int64_t size,
+                                       const int64_t num_emb,
+                                       vector<EMBIDX *> &idx_ptr,
+                                       vector<float *> &val_ptr, // EMBACC
+                                       vector<int64_t *> &ofs_ptr,
+                                       BF16 *res_ptr) { // EMBRES
+    constexpr int64_t emb_dim = 128;
+#if (defined CPU_CAPABILITY_AVX512_BF16)
+    #pragma omp parallel for
+    for (int64_t i = 0; i < num_emb; ++i) {
+        SparseEmbeddingBag cache;
+        __m512 x0, x1, x2, x3, x4, x5, x6, x7;
+        for (int64_t j = 0; j < size; ++j) {
+            const int64_t ts = ofs_ptr[j][i];
+            const int64_t te = ofs_ptr[j][i + 1];
+            for (int64_t k = ts; k < te; ++k) {
+                EMBIDX rowi = idx_ptr[j][k];
+                const float *accPtr = &val_ptr[j][k * emb_dim]; // EMBACC
+                auto find = cache.find(rowi);
+                bool miss = (find == cache.end());
+                x0 = _mm512_load_ps(accPtr);
+                x1 = _mm512_load_ps(accPtr + 16);
+                x2 = _mm512_load_ps(accPtr + 32);
+                x3 = _mm512_load_ps(accPtr + 48);
+                x4 = _mm512_load_ps(accPtr + 64);
+                x5 = _mm512_load_ps(accPtr + 80);
+                x6 = _mm512_load_ps(accPtr + 96);
+                x7 = _mm512_load_ps(accPtr + 112);
+                if (miss) {
+                    EMBROW w = {x0, x1, x2, x3, x4, x5, x6, x7};
+                    cache.emplace(rowi, w);
+                } else {
+                    auto &cache_acc = find->second;
+                    cache_acc.a0 = _mm512_add_ps(cache_acc.a0, x0);
+                    cache_acc.a1 = _mm512_add_ps(cache_acc.a1, x1);
+                    cache_acc.a2 = _mm512_add_ps(cache_acc.a2, x2);
+                    cache_acc.a3 = _mm512_add_ps(cache_acc.a3, x3);
+                    cache_acc.a4 = _mm512_add_ps(cache_acc.a4, x4);
+                    cache_acc.a5 = _mm512_add_ps(cache_acc.a5, x5);
+                    cache_acc.a6 = _mm512_add_ps(cache_acc.a6, x6);
+                    cache_acc.a7 = _mm512_add_ps(cache_acc.a7, x7);
+                }
+            }
+        }
+        for (auto &[key, value] : cache) {
+            BF16 *dest = &res_ptr[key * emb_dim]; // EMBRES
+            _mm512_store_si512(dest, (__m512i)_mm512_cvtne2ps_pbh(value.a1, value.a0));
+            _mm512_store_si512(dest + 32, (__m512i)_mm512_cvtne2ps_pbh(value.a3, value.a2));
+            _mm512_store_si512(dest + 64, (__m512i)_mm512_cvtne2ps_pbh(value.a5, value.a4));
+            _mm512_store_si512(dest + 96, (__m512i)_mm512_cvtne2ps_pbh(value.a7, value.a6));
+        }
+    }
+    #endif
+}
+
+template<>
+void
+sparse_forward_merge<BF16, BF16, 128>(const int64_t size,
+                                      const int64_t num_emb,
+                                      vector<EMBIDX *> &idx_ptr,
+                                      vector<BF16 *> &val_ptr, // EMBACC
+                                      vector<int64_t *> &ofs_ptr,
+                                      BF16 *res_ptr) { // EMBRES
+    constexpr int64_t emb_dim = 128;
+#if (defined CPU_CAPABILITY_AVX512_BF16)
+    #pragma omp parallel for
+    for (int64_t i = 0; i < num_emb; ++i) {
+        SparseEmbeddingBag cache;
+        __m512 x0, x1, x2, x3, x4, x5, x6, x7;
+        __m512i b0, b1, b2, b3;
+        for (int64_t j = 0; j < size; ++j) {
+            const int64_t ts = ofs_ptr[j][i];
+            const int64_t te = ofs_ptr[j][i + 1];
+            for (int64_t k = ts; k < te; ++k) {
+                EMBIDX rowi = idx_ptr[j][k];
+                const BF16 *accPtr = &val_ptr[j][k * emb_dim]; // EMBACC
+                auto find = cache.find(rowi);
+                bool miss = (find == cache.end());
+                b0 = _mm512_load_si512(accPtr);
+                b1 = _mm512_load_si512(accPtr + 32);
+                b2 = _mm512_load_si512(accPtr + 64);
+                b3 = _mm512_load_si512(accPtr + 96);
+                x0 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b0, 0));
+                x1 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b0, 1));
+                x2 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b1, 0));
+                x3 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b1, 1));
+                x4 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b2, 0));
+                x5 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b2, 1));
+                x6 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b3, 0));
+                x7 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b3, 1));
+                if (miss) {
+                    EMBROW w = {x0, x1, x2, x3, x4, x5, x6, x7};
+                    cache.emplace(rowi, w);
+                } else {
+                    auto &cache_acc = find->second;
+                    cache_acc.a0 = _mm512_add_ps(cache_acc.a0, x0);
+                    cache_acc.a1 = _mm512_add_ps(cache_acc.a1, x1);
+                    cache_acc.a2 = _mm512_add_ps(cache_acc.a2, x2);
+                    cache_acc.a3 = _mm512_add_ps(cache_acc.a3, x3);
+                    cache_acc.a4 = _mm512_add_ps(cache_acc.a4, x4);
+                    cache_acc.a5 = _mm512_add_ps(cache_acc.a5, x5);
+                    cache_acc.a6 = _mm512_add_ps(cache_acc.a6, x6);
+                    cache_acc.a7 = _mm512_add_ps(cache_acc.a7, x7);
+                }
+            }
+        }
+        for (auto &[key, value] : cache) {
+            BF16 *dest = &res_ptr[key * emb_dim]; // EMBRES
+            _mm512_store_si512(dest, (__m512i)_mm512_cvtne2ps_pbh(value.a1, value.a0));
+            _mm512_store_si512(dest + 32, (__m512i)_mm512_cvtne2ps_pbh(value.a3, value.a2));
+            _mm512_store_si512(dest + 64, (__m512i)_mm512_cvtne2ps_pbh(value.a5, value.a4));
+            _mm512_store_si512(dest + 96, (__m512i)_mm512_cvtne2ps_pbh(value.a7, value.a6));
+        }
+    }
+    #endif
+}
+
+template<typename T, typename V, int64_t emb_dim>
+void
+dense_forward_local_kern(const int64_t rank,
+                         const int64_t num_emb,
+                         const int64_t lbatch,
+                         const EMBIDX offset,
+                         const EMBIDX multi_hot,
+                         const EMBIDX *index,
+                         const EMBIDX *lookup,
+                         const T *weight,
+                         V *result);
+
+template<>
+void
+dense_forward_local_kern<float, float, 128>(const int64_t rank,
+                                            const int64_t num_emb,
+                                            const int64_t lbatch,
+                                            const EMBIDX offset,
+                                            const EMBIDX multi_hot,
+                                            const EMBIDX *index,
+                                            const EMBIDX *lookup,
+                                            const float *weight,
+                                            float *result) {
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    constexpr int64_t emb_dim = 128;
+    const int64_t res_batch_stride = num_emb * emb_dim;
+    for (int64_t l = 0; l < lbatch; ++l) {
+        for (int64_t h = 0; h < multi_hot; ++h) {
+            const EMBIDX emb_idx = index[l * multi_hot + h] + offset;
+            const EMBIDX den_idx = lookup[emb_idx];
+            if (den_idx >= 0) {
+                const float * wgt = &weight[den_idx * emb_dim]; // EMBWGT
+                float * res = &result[l * res_batch_stride]; // EMBRES
+                copy_add<float, float, emb_dim>(wgt, res); // EMBWGT, EMBRES, emb_dim
+            }
+        }
+    }
+    #endif
+}
+
+template<typename W, typename R, int64_t emb_dim>
+void
+dense_forward_local(const int64_t rank,
+                    const int64_t num_emb,
+                    const int64_t lbatch,
+                    const EMBIDX *multi_hots,
+                    const vector<EMBIDX *> &indices,
+                    const EMBIDX *lookup,
+                    const EMBIDX *idx_offset,
+                    const W *weight,
+                    R *result) {
+    constexpr int32_t mb = 4;
+    if (mb >= lbatch)
+        printf("mb must be smaller than lbatch\n");
+    const int32_t bb = lbatch / mb;
+    #pragma omp parallel for collapse(2) schedule(guided)
+    for (int64_t n = 0; n < num_emb; ++n) {
+        for (int64_t m = 0; m < mb; ++m) {
+            const EMBIDX multi_hot = multi_hots[n];
+            const EMBIDX *index = &indices[n][rank * lbatch * multi_hot + m * bb * multi_hot];
+            const EMBIDX offset = idx_offset[n];
+            R *res_ptr = &result[n * emb_dim + m * bb * num_emb * emb_dim];
+            // dense_forward_local_kern<W, R, emb_dim>(rank, num_emb, bb,
+            //                                         offset, multi_hot,
+            //                                         index, lookup,
+            //                                         weight, res_ptr);
+        }
+    }
+}
+
+/*
+ * frequency embeddingbag sparse forward local
+ */
+tuple<vector<at::Tensor>, vector<at::Tensor>, vector<at::Tensor>> mlperf_freqembbag_spsfwd_local_kernel_impl(
+    int64_t rank, int64_t size, int64_t gbatch,
+    const at::Tensor &input_splits, // need to be fix syk
+    const at::Tensor &multihot,
+    const vector<at::Tensor> &indices,
+    const at::Tensor &idx_offset, const at::Tensor &lookup,
+    const at::Tensor &sparse_weight) {
+    const int64_t lbatch = gbatch / size;
+    const int64_t num_emb = indices.size();
+    constexpr int64_t num_chk = 16;
+    constexpr int64_t emb_dim = 128;
+    EMBIDX *multihot_size = multihot.data_ptr<EMBIDX>();
+    vector<EMBIDX *> index_ptr(num_emb);
+    EMBIDX *idx_ofs_ptr = idx_offset.data_ptr<EMBIDX>();
+    const EMBIDX *lookup_ptr = lookup.data_ptr<EMBIDX>();
+    const EMBWGT *weight_ptr = sparse_weight.data_ptr<EMBWGT>();
+    EMBIDX *split_ptr = input_splits.data_ptr<EMBIDX>();
+    for (int64_t i = 0; i < num_emb; ++i) {
+         index_ptr[i] = indices[i].data_ptr<EMBIDX>();
+    }
+    vector<SparseEmbeddingBag> cache(size * num_chk * num_emb);
+    spsfwd_local_cache<EMBWGT, emb_dim>(num_emb, num_chk, gbatch, lbatch, rank, size,
+                                        cache, multihot_size, index_ptr,
+                                        idx_ofs_ptr, lookup_ptr, weight_ptr);
+    vector<at::Tensor> sps_idx(size);
+    vector<at::Tensor> sps_val(size);
+    vector<at::Tensor> sps_ofs(size);
+    vector<SparseEmbeddingBag> mcache(size * num_emb);
+    fwd_cache_merge<emb_dim>(size, num_emb, num_chk, cache, mcache);
+    vector<EMBACC *> res_ptr(size);
+    vector<EMBIDX *> idx_ptr(size);
+    vector<int64_t *> ofs_ptr(size);
+    sparse_init_merge<EMBACC, emb_dim>(size, num_emb, split_ptr, mcache,
+                                       sps_idx, sps_val, sps_ofs,
+                                       idx_ptr, res_ptr, ofs_ptr);
+    sparse_local_merge<EMBACC, emb_dim>(size, num_emb, split_ptr, mcache,
+                                        idx_ptr, res_ptr, ofs_ptr);
+    return {sps_idx, sps_val, sps_ofs};
+}
+
+/*
+ * frequency embeddingbag sparse forward merge
+ */
+void mlperf_freqembbag_spsfwd_merge_kernel_impl(
+    const int64_t size, const int64_t num_emb,
+    const at::Tensor& output_splits,
+    const vector<at::Tensor> &sps_idx,
+    const vector<at::Tensor> &sps_val,
+    const vector<at::Tensor> &sps_ofs,
+    const at::Tensor& result) {
+    constexpr int64_t emb_dim = 128;
+    const EMBIDX * split_ptr = output_splits.data_ptr<EMBIDX>();
+    EMBRES *res_ptr = result.data_ptr<EMBRES>();
+    vector<EMBIDX *> idx_ptr(size);
+    vector<EMBACC *> val_ptr(size);
+    vector<int64_t *> ofs_ptr(size);
+    for (int i = 0; i < size; ++i) {
+        idx_ptr[i] = sps_idx[i].data_ptr<EMBIDX>();
+        val_ptr[i] = sps_val[i].data_ptr<EMBACC>();
+        ofs_ptr[i] = sps_ofs[i].data_ptr<int64_t>();
+    }
+    sparse_forward_merge<EMBACC, EMBRES, emb_dim>(size, num_emb, idx_ptr, val_ptr, ofs_ptr, res_ptr);
+}
+
+/*
+ * frequency embeddingbag dense forward
+ */
+void mlperf_freqembbag_denfwd_kernel_impl(
+    int64_t rank, int64_t lbatch,
+    const at::Tensor &multihot,
+    const std::vector<at::Tensor> &indices,
+    const at::Tensor &idx_offset,
+    const at::Tensor &lookup,
+    const at::Tensor &dense_weight,
+    const at::Tensor &result) {
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    constexpr int64_t emb_dim = 128;
+    const int64_t num_emb = multihot.size(0);
+    EMBRES * res_ptr = result.data_ptr<EMBRES>();
+    EMBWGT * wgt_ptr = dense_weight.data_ptr<EMBWGT>();
+    EMBIDX * hot_ptr = multihot.data_ptr<EMBIDX>();
+    EMBIDX * idx_ptr = idx_offset.data_ptr<EMBIDX>();
+    EMBIDX * look_ptr = lookup.data_ptr<EMBIDX>();
+    vector<EMBIDX *> index_ptr(num_emb);
+    for (int i = 0; i < num_emb; ++i) {
+        index_ptr[i] = indices[i].data_ptr<EMBIDX>();
+    }
+    dense_forward_local<EMBWGT, EMBRES, emb_dim>(rank, num_emb, lbatch,
+                                                 hot_ptr, index_ptr, look_ptr,
+                                                 idx_ptr, wgt_ptr, res_ptr);
+    #endif
+}
+
+/*
+ * T: gradient data type
+ */
+template<typename T, int64_t emb_dim>
+inline
+void spsbwd_local_cache(const int64_t num_thd,
+                        const int64_t num_emb,
+                        const int64_t lbatch,
+                        const int64_t rank,
+                        const int64_t size,
+                        vector<SparseEmbeddingBag> &embcache,
+                        const EMBIDX *multi_hots,
+                        const vector<const EMBIDX *> &indices,
+                        const EMBIDX *idx_offset,
+                        const EMBIDX *lookup,
+                        const T *grad);
+template<>
+inline
+void spsbwd_local_cache<float, 128>(const int64_t num_thd,
+                                    const int64_t num_emb,
+                                    const int64_t lbatch,
+                                    const int64_t rank,
+                                    const int64_t size,
+                                    vector<SparseEmbeddingBag> &embcache,
+                                    const EMBIDX *multi_hots,
+                                    const vector<const EMBIDX *> &indices,
+                                    const EMBIDX *idx_offset,
+                                    const EMBIDX *lookup,
+                                    const float *grad) { // EMBGRD
+    constexpr int64_t  emb_dim = 128;
+    // store in size * num_thd cache
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+#pragma omp parallel
+    {
+        int64_t tid = omp_get_thread_num();
+        for (int64_t n = 0; n < num_emb; ++n) {
+            __m512 x0, x1, x2, x3, x4, x5, x6, x7;
+            __m512i b0, b1, b2, b3;
+            const EMBIDX *index = indices[n];
+            EMBIDX multi_hot = multi_hots[n];
+            EMBIDX offset = idx_offset[n];
+            for (int64_t b = 0; b < lbatch; ++b) {
+                for (int64_t h = 0; h < multi_hot; ++h) {
+                    const EMBIDX emb_idx = index[b * multi_hot + h + rank * lbatch * multi_hot] + offset;
+                    if (lookup[emb_idx] < 0 && ((emb_idx / size ) % num_thd == tid)) {
+                        // if sparse row, only send to emb_idx % size
+                        EMBIDX dest = emb_idx % size;
+                        SparseEmbeddingBag &cache = embcache[dest * num_thd + tid];
+                        auto find = cache.find(emb_idx);
+                        bool miss = (find == cache.end());
+                        const float *grdPtr = &grad[(b * num_emb + n)* emb_dim]; // EMBGRD
+                        x0 = _mm512_load_ps(grdPtr);
+                        x1 = _mm512_load_ps(grdPtr + 16);
+                        x2 = _mm512_load_ps(grdPtr + 32);
+                        x3 = _mm512_load_ps(grdPtr + 48);
+                        x4 = _mm512_load_ps(grdPtr + 64);
+                        x5 = _mm512_load_ps(grdPtr + 80);
+                        x6 = _mm512_load_ps(grdPtr + 96);
+                        x7 = _mm512_load_ps(grdPtr + 112);
+                        if (miss) {
+                            EMBROW g = {x0, x1, x2, x3, x4, x5, x6, x7};
+                            cache.emplace(emb_idx, g);
+                        } else {
+                            auto &cache_grd = find->second;
+                            cache_grd.a0 = _mm512_add_ps(cache_grd.a0, x0);
+                            cache_grd.a1 = _mm512_add_ps(cache_grd.a1, x1);
+                            cache_grd.a2 = _mm512_add_ps(cache_grd.a2, x2);
+                            cache_grd.a3 = _mm512_add_ps(cache_grd.a3, x3);
+                            cache_grd.a4 = _mm512_add_ps(cache_grd.a4, x4);
+                            cache_grd.a5 = _mm512_add_ps(cache_grd.a5, x5);
+                            cache_grd.a6 = _mm512_add_ps(cache_grd.a6, x6);
+                            cache_grd.a7 = _mm512_add_ps(cache_grd.a7, x7);
+                        }
+                    }
+                }
+            }
+        }
+    }
+    #endif
+}
+
+template<>
+inline
+void spsbwd_local_cache<BF16, 128>(const int64_t num_thd,
+                                   const int64_t num_emb,
+                                   const int64_t lbatch,
+                                   const int64_t rank,
+                                   const int64_t size,
+                                   vector<SparseEmbeddingBag> &embcache,
+                                   const EMBIDX *multi_hots,
+                                   const vector<const EMBIDX *> &indices,
+                                   const EMBIDX *idx_offset,
+                                   const EMBIDX *lookup,
+                                   const BF16 *grad) {
+    constexpr int64_t  emb_dim = 128;
+    // store in size * num_thd cache
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+#pragma omp parallel shared(indices)
+    {
+        int64_t tid = omp_get_thread_num();
+        for (int64_t n = 0; n < num_emb; ++n) {
+            // int64_t n = k % num_emb;
+            const EMBIDX multi_hot = multi_hots[n];
+            const EMBIDX *index = &indices[n][rank * lbatch * multi_hot];
+            const EMBIDX offset = idx_offset[n];
+            for (int64_t b = 0; b < lbatch; ++b) {
+                for (int64_t h = 0; h < multi_hot; h += 64) {
+                    _mm_prefetch(&index[(b + 3) * multi_hot + h], _MM_HINT_NTA);
+                }
+                for (int64_t h = 0; h < multi_hot; ++h) {
+                    const EMBIDX emb_idx = index[b * multi_hot + h] + offset;
+                    // bool do_lookup = (lookup[emb_idx] < 0 && ((emb_idx / size ) % num_thd == tid)); need to be fixed to enable dense syk
+                    const bool do_lookup = ((emb_idx / size ) % num_thd == tid);
+                    if (do_lookup) {
+                        // if sparse row, only send to emb_idx % size
+                        __m512 x0, x1, x2, x3, x4, x5, x6, x7;
+                        __m512i b0, b1, b2, b3;
+                        EMBIDX dest = emb_idx % size;
+                        SparseEmbeddingBag &cache = embcache[dest * num_thd + tid];
+                        auto find = cache.find(emb_idx);
+                        bool miss = (find == cache.end());
+                        const BF16 *grdPtr = &grad[(b * num_emb + n) * emb_dim];
+                        b0 = _mm512_load_si512(grdPtr);
+                        b1 = _mm512_load_si512(grdPtr + 32);
+                        b2 = _mm512_load_si512(grdPtr + 64);
+                        b3 = _mm512_load_si512(grdPtr + 96);
+                        x0 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b0, 0));
+                        x1 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b0, 1));
+                        x2 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b1, 0));
+                        x3 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b1, 1));
+                        x4 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b2, 0));
+                        x5 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b2, 1));
+                        x6 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b3, 0));
+                        x7 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b3, 1));
+                        if (miss) {
+                            EMBROW g = {x0, x1, x2, x3, x4, x5, x6, x7};
+                            cache.emplace(emb_idx, g);
+                        } else {
+                            auto &cache_grd = find->second;
+                            cache_grd.a0 = _mm512_add_ps(cache_grd.a0, x0);
+                            cache_grd.a1 = _mm512_add_ps(cache_grd.a1, x1);
+                            cache_grd.a2 = _mm512_add_ps(cache_grd.a2, x2);
+                            cache_grd.a3 = _mm512_add_ps(cache_grd.a3, x3);
+                            cache_grd.a4 = _mm512_add_ps(cache_grd.a4, x4);
+                            cache_grd.a5 = _mm512_add_ps(cache_grd.a5, x5);
+                            cache_grd.a6 = _mm512_add_ps(cache_grd.a6, x6);
+                            cache_grd.a7 = _mm512_add_ps(cache_grd.a7, x7);
+                        }
+                    }
+                }
+            }
+        }
+    }
+    #endif
+}
+
+/*
+ * T datatype for exchange buffer
+ */
+template<typename T, int64_t emb_dim>
+void sparse_backward_merge(const int64_t size,
+                           vector<SparseEmbeddingBag> &thdcache,
+                           const int64_t *split_ptr,
+                           const vector<EMBIDX *> &sps_idx,
+                           const vector<T *> &sps_val,
+                           const vector<int64_t *> &sps_ofs);
+
+template<>
+void sparse_backward_merge<float, 128>(const int64_t size,
+                                       vector<SparseEmbeddingBag> &thdcache,
+                                       const int64_t *split_ptr,
+                                       const vector<EMBIDX *> &sps_idx,
+                                       const vector<float *> &sps_val,
+                                       const vector<int64_t *> &sps_ofs) {
+    /*
+     * sps_idx: array of list index
+     * sps_val: array of list of gradient
+     * sps_ofs: offset of index between threads
+     */
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    constexpr int64_t emb_dim = 128;
+    #pragma omp parallel shared(thdcache)
+    {
+        const int64_t numthd = omp_get_num_threads();
+        const int64_t thdidx = omp_get_thread_num();
+        __m512 x0, x1, x2, x3, x4, x5, x6, x7;
+        SparseEmbeddingBag &cache = thdcache[thdidx];
+        for (int64_t i = 0; i < size; ++i) {
+            int64_t ts = sps_ofs[i][thdidx];
+            int64_t te = sps_ofs[i][thdidx + 1];
+            const EMBIDX *idx = sps_idx[i];
+            const float *val = sps_val[i];
+            // j is the index of current gradient being processed in exchange buffer (sps_val)
+            for (int64_t j = ts; j < te; ++j) {
+                const EMBIDX emb_idx = idx[j];
+                const EMBIDX row_idx = emb_idx / size;
+                    auto find = cache.find(row_idx);
+                    bool miss = find == cache.end();
+                    const float *grad = &val[j * emb_dim];
+                    x0 = _mm512_load_ps(grad);
+                    x1 = _mm512_load_ps(grad + 16);
+                    x2 = _mm512_load_ps(grad + 32);
+                    x3 = _mm512_load_ps(grad + 48);
+                    x4 = _mm512_load_ps(grad + 64);
+                    x5 = _mm512_load_ps(grad + 80);
+                    x6 = _mm512_load_ps(grad + 96);
+                    x7 = _mm512_load_ps(grad + 112);
+                    if (miss) {
+                        EMBROW g = {x0, x1, x2, x3, x4, x5, x6, x7};
+                        cache.emplace(row_idx, g);
+                    } else {
+                        auto &cache_grd = find->second;
+                        cache_grd.a0 = _mm512_add_ps(cache_grd.a0, x0);
+                        cache_grd.a1 = _mm512_add_ps(cache_grd.a1, x1);
+                        cache_grd.a2 = _mm512_add_ps(cache_grd.a2, x2);
+                        cache_grd.a3 = _mm512_add_ps(cache_grd.a3, x3);
+                        cache_grd.a4 = _mm512_add_ps(cache_grd.a4, x4);
+                        cache_grd.a5 = _mm512_add_ps(cache_grd.a5, x5);
+                        cache_grd.a6 = _mm512_add_ps(cache_grd.a6, x6);
+                        cache_grd.a7 = _mm512_add_ps(cache_grd.a7, x7);
+                }
+            }
+        }
+    }
+    #endif
+}
+
+template<>
+void sparse_backward_merge<BF16, 128>(const int64_t size,
+                                      vector<SparseEmbeddingBag> &thdcache,
+                                      const int64_t *split_ptr,
+                                      const vector<EMBIDX *> &sps_idx,
+                                      const vector<BF16 *> &sps_val,
+                                      const vector<int64_t *> &sps_ofs) {
+    /*
+     * sps_idx: array of list index
+     * sps_val: array of list of gradient
+     * sps_ofs: offset of index between threads
+     */
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    constexpr int64_t emb_dim = 128;
+    #pragma omp parallel shared(thdcache)
+    {
+        const int64_t numthd = omp_get_num_threads();
+        const int64_t thdidx = omp_get_thread_num();
+        __m512 x0, x1, x2, x3, x4, x5, x6, x7;
+        __m512i b0, b1, b2, b3;
+        SparseEmbeddingBag &cache = thdcache[thdidx];
+        for (int64_t i = 0; i < size; ++i) {
+            const int64_t ts = sps_ofs[i][thdidx];
+            const int64_t te = sps_ofs[i][thdidx + 1];
+            const EMBIDX *idx = sps_idx[i];
+            const BF16 *val = sps_val[i];
+            // j is the index of current gradient being processed in exchange buffer (sps_val)
+            for (int64_t j = ts; j < te; ++j) {
+                const EMBIDX emb_idx = idx[j];
+                const EMBIDX row_idx = emb_idx / size;
+                auto find = cache.find(row_idx);
+                bool miss = find == cache.end();
+                const BF16 *grad = &val[j * emb_dim];
+                b0 = _mm512_load_si512(grad);
+                b1 = _mm512_load_si512(grad + 32);
+                b2 = _mm512_load_si512(grad + 64);
+                b3 = _mm512_load_si512(grad + 96);
+                if (j < te - 1) {
+                    const BF16 *grad2 = grad + 2 * emb_dim;
+                    _mm_prefetch(grad2, _MM_HINT_NTA);
+                    _mm_prefetch(grad2 + 32, _MM_HINT_NTA);
+                    _mm_prefetch(grad2 + 64, _MM_HINT_NTA);
+                    _mm_prefetch(grad2 + 96, _MM_HINT_NTA);
+                }
+                x0 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b0, 0));
+                x1 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b0, 1));
+                x2 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b1, 0));
+                x3 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b1, 1));
+                x4 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b2, 0));
+                x5 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b2, 1));
+                x6 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b3, 0));
+                x7 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(b3, 1));
+                if (miss) {
+                    EMBROW g = {x0, x1, x2, x3, x4, x5, x6, x7};
+                    cache.emplace(row_idx, g);
+                } else {
+                    auto &cache_grd = find->second;
+                    cache_grd.a0 = _mm512_add_ps(cache_grd.a0, x0);
+                    cache_grd.a1 = _mm512_add_ps(cache_grd.a1, x1);
+                    cache_grd.a2 = _mm512_add_ps(cache_grd.a2, x2);
+                    cache_grd.a3 = _mm512_add_ps(cache_grd.a3, x3);
+                    cache_grd.a4 = _mm512_add_ps(cache_grd.a4, x4);
+                    cache_grd.a5 = _mm512_add_ps(cache_grd.a5, x5);
+                    cache_grd.a6 = _mm512_add_ps(cache_grd.a6, x6);
+                    cache_grd.a7 = _mm512_add_ps(cache_grd.a7, x7);
+                }
+            }
+        }
+    }
+    #endif
+}
+
+/*
+ * W datatype for weight
+ * H datatype for hessian
+ */
+template<typename W, typename H, int64_t emb_dim>
+void sparse_backward_adagrad(int64_t size,
+                             float lr,
+                             float eps,
+                             vector<SparseEmbeddingBag> &thdcache,
+                             W *sps_wgt,
+                             H *sps_hes);
+
+template<>
+void sparse_backward_adagrad<float, float, 128>(int64_t size,
+                                                float lr,
+                                                float eps,
+                                                vector<SparseEmbeddingBag> &thdcache,
+                                                float *sps_wgt, // EMBWGT
+                                                float *sps_hes) { // EMBHES
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    constexpr int64_t emb_dim = 128;
+    __m512 lrv = _mm512_set1_ps(lr);
+    __m512 epsv = _mm512_set1_ps(eps);
+    #pragma omp parallel shared(thdcache)
+    {
+        const int64_t thdidx = omp_get_thread_num();
+        SparseEmbeddingBag &cache = thdcache[thdidx];
+        for (auto &[k, v] : cache) {
+            int64_t lrid = k;
+            float *hes_ptr = &sps_hes[lrid * emb_dim];
+            float *wgt_ptr = &sps_wgt[lrid * emb_dim];
+            adagradupdatesparse(wgt_ptr,
+                                hes_ptr,
+                                lrv, epsv,
+                                v.a0, v.a1, v.a2, v.a3,
+                                v.a4, v.a5, v.a6, v.a7);
+        }
+    }
+    #endif
+}
+
+template<>
+void sparse_backward_adagrad<float, BF16, 128>(int64_t size,
+                                               float lr,
+                                               float eps,
+                                               vector<SparseEmbeddingBag> &thdcache,
+                                               float *sps_wgt, // EMBWGT
+                                               BF16 *sps_hes) { // EMBHES
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    constexpr int64_t emb_dim = 128;
+    __m512 lrv = _mm512_set1_ps(lr);
+    __m512 epsv = _mm512_set1_ps(eps);
+    #pragma omp parallel shared(thdcache)
+    {
+        const int64_t thdidx = omp_get_thread_num();
+        SparseEmbeddingBag &cache = thdcache[thdidx];
+        for (auto &[k, v] : cache) {
+            int64_t lrid = k;
+            BF16 *hes_ptr = &sps_hes[lrid * emb_dim];
+            float *wgt_ptr = &sps_wgt[lrid * emb_dim];
+            adagradupdatesparse(wgt_ptr,
+                                hes_ptr,
+                                lrv, epsv,
+                                v.a0, v.a1, v.a2, v.a3,
+                                v.a4, v.a5, v.a6, v.a7);
+        }
+    }
+    #endif
+}
+
+template<typename T, int64_t emb_dim>
+void dense_backward_local(const int64_t rank,
+                          const int64_t size,
+                          const int64_t lbatch,
+                          const EMBIDX *multi_hots,
+                          const vector<EMBIDX *> &indices,
+                          const EMBIDX *lookup,
+                          const EMBIDX *idx_offset,
+                          const T *grad,
+                          T *ds_grad);
+
+template<>
+void dense_backward_local<float, 128>(const int64_t rank,
+                                      const int64_t size,
+                                      const int64_t lbatch,
+                                      const EMBIDX *multi_hots,
+                                      const vector<EMBIDX *> &indices,
+                                      const EMBIDX *lookup,
+                                      const EMBIDX *idx_offset,
+                                      const float *grad,
+                                      float *ds_grad) {
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    constexpr int64_t emb_dim = 128;
+    const int64_t num_emb = indices.size();
+    const int64_t max_thd = omp_get_max_threads();
+    #pragma omp parallel for
+    for (int64_t n = 0; n < num_emb; ++n) {
+        const EMBIDX *index = indices[n];
+        const EMBIDX multi_hot = multi_hots[n];
+        const EMBIDX offset = idx_offset[n];
+        const EMBIDX bch_offst = rank * lbatch * multi_hot;
+        for (int64_t l = 0; l < lbatch; ++l) {
+            for (int64_t h = 0; h < multi_hot; ++h) {
+                const EMBIDX gi = l * multi_hot + h + bch_offst;
+                const EMBIDX emb_idx = index[gi] + offset;
+                const EMBIDX ds_idx = lookup[emb_idx];
+                float *ds_ptr = &ds_grad[ds_idx * emb_dim]; // EMBGRD
+                const float *gr_ptr = &grad[l * num_emb * emb_dim + n *emb_dim]; // EMBGRD
+                if (ds_idx >= 0) {
+                    copy_add<float, float, emb_dim>(gr_ptr, ds_ptr);
+                }
+            }
+        }
+    }
+    #endif
+}
+
+/*
+ * frequency embeddingbag sparse backward local
+ */
+tuple<vector<at::Tensor>, vector<at::Tensor>, vector<at::Tensor>> mlperf_freqembbag_spsbwd_local_kernel_impl(
+    int64_t rank, int64_t size, int64_t lbatch,
+    const at::Tensor&input_splits,
+    const at::Tensor &multihot,
+    const std::vector<at::Tensor> &indices,
+    const at::Tensor &idx_offset,
+    const at::Tensor &lookup,
+    const at::Tensor &grad) {
+    constexpr int64_t emb_dim = 128;
+    const int64_t num_emb = indices.size();
+    EMBIDX * multihot_size = multihot.data_ptr<EMBIDX>();
+    vector<const EMBIDX *> index_ptr(num_emb);
+    EMBIDX *idx_ofs_ptr = idx_offset.data_ptr<EMBIDX>();
+    const EMBIDX * lookup_ptr = lookup.data_ptr<EMBIDX>();
+    const EMBGRD *grad_ptr = grad.data_ptr<EMBGRD>();
+    const int64_t num_thd = omp_get_max_threads();
+    EMBIDX * split_ptr = input_splits.data_ptr<EMBIDX>();
+    for (int64_t i = 0; i < num_emb; ++i) {
+        index_ptr[i] = indices[i].data_ptr<EMBIDX>();
+    }
+
+    vector<SparseEmbeddingBag> cache(size * num_thd);
+    spsbwd_local_cache<EMBGRD, emb_dim>(num_thd, num_emb, lbatch, rank, size,
+                                        cache, multihot_size, index_ptr,
+                                        idx_ofs_ptr, lookup_ptr, grad_ptr);
+    vector<at::Tensor> sps_idx(size);
+    vector<at::Tensor> sps_val(size);
+    vector<at::Tensor> sps_ofs(size);
+    vector<EMBACC *> grd_ptr(size);
+    vector<EMBIDX *> idx_ptr(size);
+    vector<int64_t *> ofs_ptr(size);
+    sparse_init_merge<EMBACC, emb_dim>(size, num_thd, split_ptr, cache,
+                                       sps_idx, sps_val, sps_ofs,
+                                       idx_ptr, grd_ptr, ofs_ptr);
+    sparse_local_merge<EMBACC, emb_dim>(size, num_thd, split_ptr, cache,
+                                        idx_ptr, grd_ptr, ofs_ptr);
+    return {sps_idx, sps_val, sps_ofs};
+}
+
+void mlperf_freqembbag_spsbwd_adagrad_kernel_impl(
+    int64_t size, double lr, double eps,
+    const at::Tensor &output_splits,
+    const std::vector<at::Tensor> &sps_idx,
+    const std::vector<at::Tensor> &sps_grd,
+    const std::vector<at::Tensor> &sps_ofs,
+    const at::Tensor&sps_wgt,
+    const at::Tensor&sps_hes) {
+    constexpr int64_t emb_dim = 128;
+    const int64_t num_thd = omp_get_max_threads();
+    vector<SparseEmbeddingBag> cache(num_thd);
+    EMBIDX * split_ptr= output_splits.data_ptr<EMBIDX>();
+    vector<EMBIDX *> idx_ptr(size);
+    vector<EMBACC *> grd_ptr(size);
+    vector<int64_t *> ofs_ptr(size);
+    EMBWGT * wgt_ptr = sps_wgt.data_ptr<EMBWGT>();
+    EMBHES * hes_ptr = sps_hes.data_ptr<EMBHES>();
+    for (int64_t i = 0; i < size; ++i) {
+        idx_ptr[i] = sps_idx[i].data_ptr<EMBIDX>();
+        grd_ptr[i] = sps_grd[i].data_ptr<EMBACC>();
+        ofs_ptr[i] = sps_ofs[i].data_ptr<int64_t>();
+    }
+    sparse_backward_merge<EMBACC, emb_dim>(size, cache, split_ptr, idx_ptr, grd_ptr, ofs_ptr);
+    sparse_backward_adagrad<EMBWGT, EMBHES, emb_dim>(size, lr, eps, cache, wgt_ptr, hes_ptr);
+}
+
+void mlperf_freqembbag_denbwd_local_kernel_impl(
+    int64_t rank,
+    int64_t size,
+    int64_t lbatch,
+    const at::Tensor &multihot,
+    const std::vector<at::Tensor> &indices,
+    const at::Tensor &lookup,
+    const at::Tensor &idx_offset,
+    const at::Tensor &grad,
+    const at::Tensor &ds_grad) {
+    constexpr int64_t emb_dim = 128;
+    EMBIDX * hot_ptr = multihot.data_ptr<EMBIDX>();
+    vector<EMBIDX *> index_ptr(indices.size());
+    EMBIDX * look_ptr = lookup.data_ptr<EMBIDX>();
+    EMBIDX * idx_ptr = idx_offset.data_ptr<EMBIDX>();
+    EMBGRD * grad_ptr = grad.data_ptr<EMBGRD>();
+    EMBGRD * dg_ptr = ds_grad.data_ptr<EMBGRD>();
+    for (int i = 0; i < indices.size(); ++i) {
+        index_ptr[i] = indices[i].data_ptr<EMBIDX>();
+    }
+    // dense_backward_local<EMBGRD, emb_dim>(rank, size, lbatch, hot_ptr,
+    //                                       index_ptr, look_ptr, idx_ptr,
+    //                                       grad_ptr, dg_ptr);
+}
+
+template<typename T, typename V, typename W, int64_t emb_dim>
+void dense_backward_adagrad(const double lr,
+                            const double eps,
+                            const int64_t num_row,
+                            const T *grd_ptr,
+                            V *wgt_ptr,
+                            W *hes_ptr);
+
+template<>
+void dense_backward_adagrad<float, float, float, 128>(const double lr,
+                                                      const double eps,
+                                                      const int64_t num_row,
+                                                      const float *grd_ptr,
+                                                      float *wgt_ptr,
+                                                      float *hes_ptr) {
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    constexpr int64_t emb_dim = 128;
+    __m512 lrv = _mm512_set1_ps(lr);
+    __m512 epsv = _mm512_set1_ps(eps);
+    for (int i = 0; i < num_row; ++i) {
+        const float * grd = &grd_ptr[i * emb_dim]; // EMBGRD
+        float * wgt = &wgt_ptr[i * emb_dim]; // EMBWGT
+        float * hes = &hes_ptr[i * emb_dim]; // EMBHES
+        adagradupdatedense(wgt, hes, grd, lrv, epsv);
+    }
+    #endif
+}
+
+void mlperf_freqembbag_denbwd_adagrad_kernel_impl(
+    double lr,
+    double eps,
+    const at::Tensor &dense_grad,
+    const at::Tensor &dense_weight,
+    const at::Tensor &dense_hessian) {
+    EMBIDX ds_row = dense_weight.size(0);
+    EMBGRD * grd_ptr = dense_grad.data_ptr<EMBGRD>();
+    EMBWGT * wgt_ptr = dense_weight.data_ptr<EMBWGT>();
+    EMBHES * hes_ptr = dense_hessian.data_ptr<EMBHES>();
+    const int64_t num_row = dense_weight.size(0);
+    // dense_backward_adagrad<EMBGRD, EMBWGT, EMBHES, 128>(lr, eps, num_row, grd_ptr, wgt_ptr, hes_ptr);
+}
+} // anonymous namespace
+
+REGISTER_DISPATCH(mlperf_freqembbag_spsfwd_local_kernel_stub, &mlperf_freqembbag_spsfwd_local_kernel_impl);
+REGISTER_DISPATCH(mlperf_freqembbag_spsfwd_merge_kernel_stub, &mlperf_freqembbag_spsfwd_merge_kernel_impl);
+REGISTER_DISPATCH(mlperf_freqembbag_denfwd_kernel_stub, &mlperf_freqembbag_denfwd_kernel_impl);
+
+REGISTER_DISPATCH(mlperf_freqembbag_spsbwd_local_kernel_stub, &mlperf_freqembbag_spsbwd_local_kernel_impl);
+REGISTER_DISPATCH(mlperf_freqembbag_spsbwd_adagrad_kernel_stub, &mlperf_freqembbag_spsbwd_adagrad_kernel_impl);
+REGISTER_DISPATCH(mlperf_freqembbag_denbwd_local_kernel_stub, &mlperf_freqembbag_denbwd_local_kernel_impl);
+REGISTER_DISPATCH(mlperf_freqembbag_denbwd_adagrad_kernel_stub, &mlperf_freqembbag_denbwd_adagrad_kernel_impl);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/MLPerfInteractionKrnl.cpp b/csrc/cpu/aten/kernels/MLPerfInteractionKrnl.cpp
new file mode 100644
index 000000000..ec663c13d
--- /dev/null
+++ b/csrc/cpu/aten/kernels/MLPerfInteractionKrnl.cpp
@@ -0,0 +1,150 @@
+#include <aten/MLPerfInteraction.h>
+#include <torch/csrc/autograd/function.h>
+#include <torch/all.h>
+#include <ATen/Tensor.h>
+#include "vec/vec.h"
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+
+typedef at::BFloat16 BF16;
+
+#if (defined CPU_CAPABILITY_AVX512_BF16)
+inline __m512
+_mm512_cvtpbh_ps(__m256i x) {
+    return (__m512)_mm512_slli_epi32(_mm512_cvtepu16_epi32(x), 0x10);
+}
+#endif
+
+template<int len>
+void lowrankcnpost_fwd_kernel(const BF16 *x0, const BF16 *xlw, const BF16 *xl,
+                              BF16 *res) {
+    static_assert(((len * sizeof(BF16)) % 64) == 0 && "len * datatype  must be dividable to cacheline");
+#if (defined CPU_CAPABILITY_AVX512_BF16)
+    __m512i x0_bf16, xlw_bf16, xl_bf16, r_bf16;
+    __m512 x0_fp32_0, x0_fp32_1, xlw_fp32_0, xlw_fp32_1, xl_fp32_0, xl_fp32_1;
+#pragma GCC unroll 4
+    for (int i = 0; i < len; i += 64 / sizeof(BF16)) {
+        x0_bf16 = _mm512_load_si512(&x0[i]);
+        xlw_bf16 = _mm512_load_si512(&xlw[i]);
+        xl_bf16 = _mm512_load_si512(&xl[i]);
+        _mm_prefetch(&x0[i + 64 / sizeof(BF16)], _MM_HINT_T0);
+        _mm_prefetch(&xlw[i + 64 / sizeof(BF16)], _MM_HINT_T0);
+        _mm_prefetch(&xl[i + 64 / sizeof(BF16)], _MM_HINT_T0);
+        x0_fp32_0 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x0_bf16, 0));
+        x0_fp32_1 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x0_bf16, 1));
+        xlw_fp32_0 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(xlw_bf16, 0));
+        xlw_fp32_1 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(xlw_bf16, 1));
+        xl_fp32_0 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(xl_bf16, 0));
+        xl_fp32_1 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(xl_bf16, 1));
+        x0_fp32_0 = _mm512_fmadd_ps(x0_fp32_0, xlw_fp32_0, xl_fp32_0);
+        x0_fp32_1 = _mm512_fmadd_ps(x0_fp32_1, xlw_fp32_1, xl_fp32_1);
+        r_bf16 = (__m512i)_mm512_cvtne2ps_pbh(x0_fp32_1, x0_fp32_0);
+        _mm512_stream_si512((__m512i *)&res[i], r_bf16);
+    }
+#endif
+}
+
+void lowrankcnpo_fwd(const size_t N, const BF16 *x0,
+                     const BF16 *xlw, const BF16 *xl, BF16 *res) {
+    constexpr int STEP = 128;
+    #pragma omp parallel for
+    for (size_t i = 0; i < N; i += STEP) {
+        lowrankcnpost_fwd_kernel<STEP>(x0 + i, xlw + i, xl + i, res + i);
+    }
+}
+
+at::Tensor mlperf_interaction_forward_kernel_impl(
+    const at::Tensor& x0,
+    const at::Tensor& xlw,
+    const at::Tensor& xl) {
+    int32_t batch = x0.size(0);
+    int32_t ifdim = 3456;
+
+    at::Tensor result = at::empty_like(x0);
+    result.set_requires_grad(true);
+    BF16 *result_data = result.data_ptr<BF16>();
+
+    BF16 *x0_bf16 = x0.data_ptr<BF16>();
+    BF16 *xlw_bf16 = xlw.data_ptr<BF16>();
+    BF16 *xl_bf16 = xl.data_ptr<BF16>();
+    lowrankcnpo_fwd(batch * ifdim, x0_bf16, xlw_bf16, xl_bf16, result_data);
+    return result;
+}
+
+template<int len>
+void lowrankcnpost_bwd_kernel(const BF16 *x0, const BF16 *xlw,
+                              const BF16 *dff_ot, BF16 *dff_x0,
+                              BF16 *dff_xlw) {
+    static_assert(((len * sizeof(BF16)) % 64) == 0 && "len * datatype  must be dividable to cacheline");
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    __m512i x0_bf16, xlw_bf16, dffot_bf16, dffxlw_bf16, dffx0_bf16;
+    __m512 dffot_fp32_0, dffot_fp32_1, x0_fp32_0, x0_fp32_1;
+    __m512 dffxlw_fp32_0, dffxlw_fp32_1, dffx0_fp32_0, dffx0_fp32_1;
+    __m512 xlw_fp32_0, xlw_fp32_1;
+    #pragma GCC unroll 4
+    for (int i = 0; i < len; i += 64 / sizeof(BF16)) {
+        dffot_bf16 = _mm512_load_si512(&dff_ot[i]);
+        x0_bf16 = _mm512_load_si512(&x0[i]);
+        xlw_bf16 = _mm512_load_si512(&xlw[i]);
+        // _mm_prefetch(&dff_ot[i + 64 / sizeof(BF16)], _MM_HINT_T0);
+        // _mm_prefetch(&x0[i + 64 / sizeof(BF16)], _MM_HINT_T0);
+        // _mm_prefetch(&xlw[i + 64 / sizeof(BF16)], _MM_HINT_T0);
+        dffot_fp32_0 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(dffot_bf16, 0));
+        dffot_fp32_1 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(dffot_bf16, 1));
+        x0_fp32_0 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x0_bf16, 0));
+        x0_fp32_1 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x0_bf16, 1));
+        xlw_fp32_0 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(xlw_bf16, 0));
+        xlw_fp32_1 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(xlw_bf16, 1));
+        dffxlw_fp32_0 = _mm512_mul_ps(dffot_fp32_0, x0_fp32_0);
+        dffxlw_fp32_1 = _mm512_mul_ps(dffot_fp32_1, x0_fp32_1);
+        dffx0_fp32_0 = _mm512_mul_ps(dffot_fp32_0, xlw_fp32_0);
+        dffx0_fp32_1 = _mm512_mul_ps(dffot_fp32_1, xlw_fp32_1);
+        dffxlw_bf16 = (__m512i)_mm512_cvtne2ps_pbh(dffxlw_fp32_1, dffxlw_fp32_0);
+        dffx0_bf16 = (__m512i)_mm512_cvtne2ps_pbh(dffx0_fp32_1, dffx0_fp32_0);
+        _mm512_stream_si512((__m512i *)&dff_x0[i], dffx0_bf16);
+        _mm512_stream_si512((__m512i *)&dff_xlw[i], dffxlw_bf16);
+    }
+    #endif
+}
+void lowrankcnpo_bwd(const size_t N, const BF16 *x0,
+                     const BF16 *xlw, const BF16 *dff_ot,
+                     BF16 *dff_x0, BF16 *dff_xlw) {
+    constexpr int STEP = 128;
+    #pragma omp parallel for
+    for (size_t i = 0; i < N; i += STEP) {
+        lowrankcnpost_bwd_kernel<STEP>(x0 + i, xlw + i, dff_ot + i,
+                                       dff_x0 + i, dff_xlw + i);
+    }
+}
+
+std::tuple<at::Tensor, at::Tensor> mlperf_interaction_backward_kernel_impl(
+    const at::Tensor& x0,
+    const at::Tensor& xlw,
+    const at::Tensor& xl,
+    const at::Tensor& diffresult) {
+    int32_t batch = x0.size(0);
+    int32_t ifdim = 3456;
+    BF16 *x0_bf16 = x0.data_ptr<BF16>();
+    BF16 *xlw_bf16 = xlw.data_ptr<BF16>();
+    BF16 *xl_bf16 = xl.data_ptr<BF16>();
+    at::Tensor x0_diff = at::empty_like(x0);
+    at::Tensor xlw_diff = at::empty_like(xlw);
+
+    BF16 *diff_ot_bf16 = diffresult.data_ptr<BF16>();
+    BF16 *diff_x0_bf16 = x0_diff.data_ptr<BF16>();
+    BF16 *diff_xlw_bf16 = xlw_diff.data_ptr<BF16>();
+    lowrankcnpo_bwd(batch * ifdim, x0_bf16, xlw_bf16, diff_ot_bf16,
+                    diff_x0_bf16, diff_xlw_bf16);
+    return std::make_tuple(x0_diff, xlw_diff);
+}
+
+} // anonymous namespace
+
+REGISTER_DISPATCH(mlperf_interaction_forward_kernel_stub, &mlperf_interaction_forward_kernel_impl);
+REGISTER_DISPATCH(mlperf_interaction_backward_kernel_stub, &mlperf_interaction_backward_kernel_impl);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/MLPerfMergedEmbeddingBagKrnl.cpp b/csrc/cpu/aten/kernels/MLPerfMergedEmbeddingBagKrnl.cpp
new file mode 100644
index 000000000..899df4bcc
--- /dev/null
+++ b/csrc/cpu/aten/kernels/MLPerfMergedEmbeddingBagKrnl.cpp
@@ -0,0 +1,701 @@
+#include <aten/MLPerfMergedEmbeddingBag.h>
+#include <torch/csrc/autograd/function.h>
+#include <torch/all.h>
+#include <ATen/Tensor.h>
+#include "vec/vec.h"
+#include "aten/kernels/robin_hood.h"
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+
+typedef at::BFloat16 BF16;
+
+template<int32_t num_emb, int32_t emb_dim,  int32_t num_hot>
+void embeddingbag_fwd_kern(const int32_t batch, const int32_t *index,
+                           const float *weight, BF16 *result) {
+    // this kernel process 1 embedding
+    // input
+    // batch int
+    // index index of 1 embedding table [batch, num_hot]
+    // offset no need
+    // weight of this embedding table [len_emb, emb_dim] fp32
+    // output
+    // result [batch, num_emb, emb_dim] bf16
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    for (size_t i = 0; i < batch; ++i) {
+        __m512 x0, x1, x2, x3, x4, x5, x6, x7;
+        size_t ofst_idx0 = i * num_hot;
+        int64_t idx = index[ofst_idx0];
+        idx *= emb_dim;
+        // load first row
+        x0 = _mm512_load_ps(&weight[idx]);
+        x1 = _mm512_load_ps(&weight[idx] + 16);
+        x2 = _mm512_load_ps(&weight[idx] + 32);
+        x3 = _mm512_load_ps(&weight[idx] + 48);
+        x4 = _mm512_load_ps(&weight[idx] + 64);
+        x5 = _mm512_load_ps(&weight[idx] + 80);
+        x6 = _mm512_load_ps(&weight[idx] + 96);
+        x7 = _mm512_load_ps(&weight[idx] + 112);
+        #pragma GCC unroll 8
+        for (size_t j = 1; j < num_hot; ++j) {
+            idx = index[ofst_idx0 + j];
+            idx *= emb_dim;
+            x0 = _mm512_add_ps(x0, _mm512_load_ps(&weight[idx]));
+            x1 = _mm512_add_ps(x1, _mm512_load_ps(&weight[idx] + 16));
+            x2 = _mm512_add_ps(x2, _mm512_load_ps(&weight[idx] + 32));
+            x3 = _mm512_add_ps(x3, _mm512_load_ps(&weight[idx] + 48));
+            x4 = _mm512_add_ps(x4, _mm512_load_ps(&weight[idx] + 64));
+            x5 = _mm512_add_ps(x5, _mm512_load_ps(&weight[idx] + 80));
+            x6 = _mm512_add_ps(x6, _mm512_load_ps(&weight[idx] + 96));
+            x7 = _mm512_add_ps(x7, _mm512_load_ps(&weight[idx] + 112));
+        }
+        _mm512_store_si512(result, (__m512i)_mm512_cvtne2ps_pbh(x1, x0));
+        _mm512_store_si512(result + 32, (__m512i)_mm512_cvtne2ps_pbh(x3, x2));
+        _mm512_store_si512(result + 64, (__m512i)_mm512_cvtne2ps_pbh(x5, x4));
+        _mm512_store_si512(result + 96, (__m512i)_mm512_cvtne2ps_pbh(x7, x6));
+        // output in [batch, num_emb, emb_dim]
+        result += num_emb * emb_dim;
+    }
+    #endif
+}
+
+void embeddingbag_fwd(const int32_t batch, const int32_t num_emb,
+                      const int32_t emb_dim, const int64_t *num_hot,
+                      const int32_t **index, const int32_t **offset,
+                      const float **weight, BF16 *result) {
+    // for (size_t j = 0; j<num_emb; ++j) {
+    //     printf("j: %ld, r: %ld\n", j, j * emb_dim);
+    //     embeddingbag_fwd_kern<26, 128>(batch, num_hot[j], index[j], weight[j], &result[j * emb_dim]);
+    // }
+    constexpr int32_t mb = 32;
+    const int32_t bb = batch / mb;
+    // #pragma omp parallel for collapse(2)
+    // for (size_t n = 0; n < num_emb; ++n) {
+    //     for (int m = 0; m < mb; ++m) {
+    //         embeddingbag_fwd_kern<26, 128>(bb, num_hot[n], &index[n][m * bb * num_hot[n]],
+    //                                        weight[n],
+    //                                        &result[m * bb * num_emb * emb_dim + n * emb_dim]);
+    //     }
+    // }
+    #pragma omp parallel for schedule(dynamic)
+    for (size_t j = 0; j<num_emb * mb; ++j) {
+        int32_t n = j / mb;
+        int32_t m = j % mb;
+        int32_t i = m * bb * static_cast<int>(num_hot[n]);
+        const int32_t *i_ptr = &index[n][i];
+        const float *w_ptr = weight[n];
+        BF16 *r_ptr = &result[m * bb * num_emb * emb_dim + n * emb_dim];
+        switch(num_hot[n]) {
+        case 1:
+            embeddingbag_fwd_kern<26, 128, 1>(bb, i_ptr, w_ptr ,r_ptr);
+            break;
+        case 2:
+            embeddingbag_fwd_kern<26, 128, 2>(bb, i_ptr, w_ptr ,r_ptr);
+            break;
+        case 3:
+            embeddingbag_fwd_kern<26, 128, 3>(bb, i_ptr, w_ptr ,r_ptr);
+            break;
+        case 5:
+            embeddingbag_fwd_kern<26, 128, 5>(bb, i_ptr, w_ptr ,r_ptr);
+            break;
+        case 6:
+            embeddingbag_fwd_kern<26, 128, 6>(bb, i_ptr, w_ptr ,r_ptr);
+            break;
+        case 7:
+            embeddingbag_fwd_kern<26, 128, 7>(bb, i_ptr, w_ptr ,r_ptr);
+            break;
+        case 8:
+            embeddingbag_fwd_kern<26, 128, 8>(bb, i_ptr, w_ptr ,r_ptr);
+            break;
+        case 9:
+            embeddingbag_fwd_kern<26, 128, 9>(bb, i_ptr, w_ptr ,r_ptr);
+            break;
+        case 10:
+            embeddingbag_fwd_kern<26, 128, 10>(bb, i_ptr, w_ptr ,r_ptr);
+            break;
+        case 12:
+            embeddingbag_fwd_kern<26, 128, 12>(bb, i_ptr, w_ptr ,r_ptr);
+            break;
+        case 27:
+            embeddingbag_fwd_kern<26, 128, 27>(bb, i_ptr, w_ptr ,r_ptr);
+            break;
+        case 100:
+            embeddingbag_fwd_kern<26, 128, 100>(bb, i_ptr, w_ptr ,r_ptr);
+            break;
+        }
+
+    }
+}
+
+at::Tensor mlperf_merged_embeddingbag_forward_kernel_impl(
+    const std::vector<at::Tensor>& indices,
+    const std::vector<at::Tensor>& offsets,
+    const std::vector<at::Tensor>& weights,
+    const std::vector<int64_t>& multihot_size) {
+    int32_t batch = indices[2].size(0);
+    int32_t emb_dim = 128;
+    // const std::vector<int64_t> multihot_size {3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1};
+    int32_t num_emb = multihot_size.size();
+    // const int32_t multihot_size[26] = {3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1};
+    const float *weight_data[num_emb];
+    const int32_t *index_data[num_emb];
+    const int32_t *offset_data[num_emb];
+
+    const std::vector<int64_t> shape {batch, num_emb, emb_dim};
+    at::Tensor result = at::empty(shape, weights[0].options().dtype(at::kBFloat16));
+    result.set_requires_grad(weights[0].requires_grad());
+    for (int i = 0; i < num_emb; ++i) {
+        weight_data[i] = weights[i].data_ptr<float>();
+        index_data[i] = indices[i].data_ptr<int32_t>();
+        offset_data[i] = offsets[i].data_ptr<int32_t>();
+    }
+    BF16 *result_data = result.data_ptr<BF16>();
+    embeddingbag_fwd(batch, num_emb, emb_dim,
+                     multihot_size.data(), index_data, offset_data,
+                     weight_data, result_data);
+    return result;
+}
+
+#if (defined CPU_CAPABILITY_AVX512_BF16)
+inline __m512
+_mm512_cvtpbh_ps(__m256i x) {
+    return (__m512)_mm512_slli_epi32(_mm512_cvtepu16_epi32(x), 0x10);
+}
+
+inline
+void adagradupdatesparse(float *weight, float *hessian,
+                         __m512 lr, __m512 eps,
+                         __m512 g0, __m512 g1, __m512 g2, __m512 g3,
+                         __m512 g4, __m512 g5, __m512 g6, __m512 g7) {
+    // hessian += grad**2
+    // weight += grad * lr / (sqrt(hessian) + eps)
+    // lr < 0
+    __m512 h0, h1, h2, h3, h4, h5, h6, h7;
+    __m512 w0, w1, w2, w3, w4, w5, w6, w7;
+    h0 = _mm512_load_ps(hessian);
+    h1 = _mm512_load_ps(hessian + 16);
+    h2 = _mm512_load_ps(hessian + 32);
+    h3 = _mm512_load_ps(hessian + 48);
+    h4 = _mm512_load_ps(hessian + 64);
+    h5 = _mm512_load_ps(hessian + 80);
+    h6 = _mm512_load_ps(hessian + 96);
+    h7 = _mm512_load_ps(hessian + 112);
+
+    h0 = _mm512_fmadd_ps(g0, g0, h0);
+    h1 = _mm512_fmadd_ps(g1, g1, h1);
+    h2 = _mm512_fmadd_ps(g2, g2, h2);
+    h3 = _mm512_fmadd_ps(g3, g3, h3);
+    h4 = _mm512_fmadd_ps(g4, g4, h4);
+    h5 = _mm512_fmadd_ps(g5, g5, h5);
+    h6 = _mm512_fmadd_ps(g6, g6, h6);
+    h7 = _mm512_fmadd_ps(g7, g7, h7);
+
+    _mm512_store_ps(hessian, h0);
+    _mm512_store_ps(hessian + 16, h1);
+    _mm512_store_ps(hessian + 32, h2);
+    _mm512_store_ps(hessian + 48, h3);
+    _mm512_store_ps(hessian + 64, h4);
+    _mm512_store_ps(hessian + 80, h5);
+    _mm512_store_ps(hessian + 96, h6);
+    _mm512_store_ps(hessian + 112, h7);
+
+    w0 = _mm512_load_ps(weight);
+    w1 = _mm512_load_ps(weight + 16);
+    w2 = _mm512_load_ps(weight + 32);
+    w3 = _mm512_load_ps(weight + 48);
+    w4 = _mm512_load_ps(weight + 64);
+    w5 = _mm512_load_ps(weight + 80);
+    w6 = _mm512_load_ps(weight + 96);
+    w7 = _mm512_load_ps(weight + 112);
+    // accurate
+    // if (true) {
+    h0 = _mm512_sqrt_ps(h0);
+    h1 = _mm512_sqrt_ps(h1);
+    h2 = _mm512_sqrt_ps(h2);
+    h3 = _mm512_sqrt_ps(h3);
+    h4 = _mm512_sqrt_ps(h4);
+    h5 = _mm512_sqrt_ps(h5);
+    h6 = _mm512_sqrt_ps(h6);
+    h7 = _mm512_sqrt_ps(h7);
+
+    h0 = _mm512_add_ps(h0, eps);
+    h1 = _mm512_add_ps(h1, eps);
+    h2 = _mm512_add_ps(h2, eps);
+    h3 = _mm512_add_ps(h3, eps);
+    h4 = _mm512_add_ps(h4, eps);
+    h5 = _mm512_add_ps(h5, eps);
+    h6 = _mm512_add_ps(h6, eps);
+    h7 = _mm512_add_ps(h7, eps);
+
+    h0 = _mm512_div_ps(lr, h0);
+    h1 = _mm512_div_ps(lr, h1);
+    h2 = _mm512_div_ps(lr, h2);
+    h3 = _mm512_div_ps(lr, h3);
+    h4 = _mm512_div_ps(lr, h4);
+    h5 = _mm512_div_ps(lr, h5);
+    h6 = _mm512_div_ps(lr, h6);
+    h7 = _mm512_div_ps(lr, h7);
+    // }//  else {
+    // // fast
+    // h0 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h0));
+    // h1 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h1));
+    // h2 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h2));
+    // h3 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h3));
+    // h4 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h4));
+    // h5 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h5));
+    // h6 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h6));
+    // h7 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h7));
+    // }
+    h0 = _mm512_mul_ps(h0, g0);
+    h1 = _mm512_mul_ps(h1, g1);
+    h2 = _mm512_mul_ps(h2, g2);
+    h3 = _mm512_mul_ps(h3, g3);
+    h4 = _mm512_mul_ps(h4, g4);
+    h5 = _mm512_mul_ps(h5, g5);
+    h6 = _mm512_mul_ps(h6, g6);
+    h7 = _mm512_mul_ps(h7, g7);
+
+    w0 = _mm512_add_ps(w0, h0);
+    w1 = _mm512_add_ps(w1, h1);
+    w2 = _mm512_add_ps(w2, h2);
+    w3 = _mm512_add_ps(w3, h3);
+    w4 = _mm512_add_ps(w4, h4);
+    w5 = _mm512_add_ps(w5, h5);
+    w6 = _mm512_add_ps(w6, h6);
+    w7 = _mm512_add_ps(w7, h7);
+
+    _mm512_store_ps(weight, w0);
+    _mm512_store_ps(weight + 16, w1);
+    _mm512_store_ps(weight + 32, w2);
+    _mm512_store_ps(weight + 48, w3);
+    _mm512_store_ps(weight + 64, w4);
+    _mm512_store_ps(weight + 80, w5);
+    _mm512_store_ps(weight + 96, w6);
+    _mm512_store_ps(weight + 112, w7);
+}
+
+struct __attribute__((packed, aligned(64))) EMBROW {
+    __m512 a0;
+    __m512 a1;
+    __m512 a2;
+    __m512 a3;
+    __m512 a4;
+    __m512 a5;
+    __m512 a6;
+    __m512 a7;
+} ;
+using SparseEmbeddingGrad = robin_hood::unordered_map<int32_t, EMBROW>; // faster
+// using SparseEmbeddingGrad = std::unordered_map<int32_t, EMBROW>;
+using HitRec = std::bitset<40000>;
+template<int32_t num_emb, int32_t emb_dim>
+void embeddingbag_bwd_sparse_acc_kern(const int32_t batch, const int32_t num_hot,
+                                      const int32_t *index, SparseEmbeddingGrad &sg,
+                                      const BF16 *diffresult) {
+    // only accumulate wid % numthd == thdidx
+    const int32_t thdidx = omp_get_thread_num();
+    const int32_t numthd = omp_get_num_threads();
+
+    __m512i x0, x1, x2, x3;
+    __m512 y0, y1, y2, y3, y4, y5, y6, y7;
+    for (int32_t i = 0; i < batch; ++i) {
+        const BF16 * gradPtr = &diffresult[i * num_emb * emb_dim];
+        const BF16 * gradPtr1 = gradPtr + num_emb * emb_dim;
+        _mm_prefetch(gradPtr1, _MM_HINT_T0);
+        _mm_prefetch(gradPtr1+32, _MM_HINT_T0);
+        _mm_prefetch(gradPtr1+64, _MM_HINT_T0);
+        _mm_prefetch(gradPtr1+96, _MM_HINT_T0);
+        for (int32_t j = 0; j < num_hot; ++j) {
+            int32_t wid = index[i * num_hot + j]; // weight index
+            if (wid % numthd == thdidx) {
+                auto find = sg.find(wid);
+                bool miss = find == sg.end();
+                x0 = _mm512_load_si512(gradPtr);
+                y0 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x0, 0));
+                y1 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x0, 1));
+
+                x1 = _mm512_load_si512(gradPtr + 32);
+                y2 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x1, 0));
+                y3 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x1, 1));
+
+                x2 = _mm512_load_si512(gradPtr + 64);
+                y4 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x2, 0));
+                y5 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x2, 1));
+
+                x3 = _mm512_load_si512(gradPtr + 96);
+                y6 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x3, 0));
+                y7 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x3, 1));
+                if (miss) {
+                    EMBROW a = {y0, y1, y2, y3, y4, y5, y6, y7};
+                    sg.emplace(wid, a);
+                } else {
+                    auto &cached_grad = find->second;
+                    cached_grad.a0 = _mm512_add_ps(cached_grad.a0, y0);
+                    cached_grad.a1 = _mm512_add_ps(cached_grad.a1, y1);
+                    cached_grad.a2 = _mm512_add_ps(cached_grad.a2, y2);
+                    cached_grad.a3 = _mm512_add_ps(cached_grad.a3, y3);
+                    cached_grad.a4 = _mm512_add_ps(cached_grad.a4, y4);
+                    cached_grad.a5 = _mm512_add_ps(cached_grad.a5, y5);
+                    cached_grad.a6 = _mm512_add_ps(cached_grad.a6, y6);
+                    cached_grad.a7 = _mm512_add_ps(cached_grad.a7, y7);
+                }
+            }
+        }
+    }
+}
+
+inline
+void adagradupdatedense(float *weight, float *hessian, const float *grad,
+                        __m512 lr, __m512 eps) {
+    // hessian += grad**2
+    // weight += grad * lr / (sqrt(hessian) + eps)
+    // lr < 0
+    __m512 h0, h1, h2, h3, h4, h5, h6, h7;
+    __m512 w0, w1, w2, w3, w4, w5, w6, w7;
+    __m512 g0, g1, g2, g3, g4, g5, g6, g7;
+    g0 = _mm512_load_ps(grad);
+    g1 = _mm512_load_ps(grad + 16);
+    g2 = _mm512_load_ps(grad + 32);
+    g3 = _mm512_load_ps(grad + 48);
+    g4 = _mm512_load_ps(grad + 64);
+    g5 = _mm512_load_ps(grad + 80);
+    g6 = _mm512_load_ps(grad + 96);
+    g7 = _mm512_load_ps(grad + 112);
+
+    h0 = _mm512_load_ps(hessian);
+    h1 = _mm512_load_ps(hessian + 16);
+    h2 = _mm512_load_ps(hessian + 32);
+    h3 = _mm512_load_ps(hessian + 48);
+    h4 = _mm512_load_ps(hessian + 64);
+    h5 = _mm512_load_ps(hessian + 80);
+    h6 = _mm512_load_ps(hessian + 96);
+    h7 = _mm512_load_ps(hessian + 112);
+
+    h0 = _mm512_fmadd_ps(g0, g0, h0);
+    h1 = _mm512_fmadd_ps(g1, g1, h1);
+    h2 = _mm512_fmadd_ps(g2, g2, h2);
+    h3 = _mm512_fmadd_ps(g3, g3, h3);
+    h4 = _mm512_fmadd_ps(g4, g4, h4);
+    h5 = _mm512_fmadd_ps(g5, g5, h5);
+    h6 = _mm512_fmadd_ps(g6, g6, h6);
+    h7 = _mm512_fmadd_ps(g7, g7, h7);
+    _mm512_store_ps(hessian, h0);
+    _mm512_store_ps(hessian + 16, h1);
+    _mm512_store_ps(hessian + 32, h2);
+    _mm512_store_ps(hessian + 48, h3);
+    _mm512_store_ps(hessian + 64, h4);
+    _mm512_store_ps(hessian + 80, h5);
+    _mm512_store_ps(hessian + 96, h6);
+    _mm512_store_ps(hessian + 112, h7);
+    w0 = _mm512_load_ps(weight);
+    w1 = _mm512_load_ps(weight + 16);
+    w2 = _mm512_load_ps(weight + 32);
+    w3 = _mm512_load_ps(weight + 48);
+    w4 = _mm512_load_ps(weight + 64);
+    w5 = _mm512_load_ps(weight + 80);
+    w6 = _mm512_load_ps(weight + 96);
+    w7 = _mm512_load_ps(weight + 112);
+    // accurate
+    // if (true) {
+    h0 = _mm512_sqrt_ps(h0);
+    h1 = _mm512_sqrt_ps(h1);
+    h2 = _mm512_sqrt_ps(h2);
+    h3 = _mm512_sqrt_ps(h3);
+    h4 = _mm512_sqrt_ps(h4);
+    h5 = _mm512_sqrt_ps(h5);
+    h6 = _mm512_sqrt_ps(h6);
+    h7 = _mm512_sqrt_ps(h7);
+
+    h0 = _mm512_add_ps(h0, eps);
+    h1 = _mm512_add_ps(h1, eps);
+    h2 = _mm512_add_ps(h2, eps);
+    h3 = _mm512_add_ps(h3, eps);
+    h4 = _mm512_add_ps(h4, eps);
+    h5 = _mm512_add_ps(h5, eps);
+    h6 = _mm512_add_ps(h6, eps);
+    h7 = _mm512_add_ps(h7, eps);
+
+    h0 = _mm512_div_ps(lr, h0);
+    h1 = _mm512_div_ps(lr, h1);
+    h2 = _mm512_div_ps(lr, h2);
+    h3 = _mm512_div_ps(lr, h3);
+    h4 = _mm512_div_ps(lr, h4);
+    h5 = _mm512_div_ps(lr, h5);
+    h6 = _mm512_div_ps(lr, h6);
+    h7 = _mm512_div_ps(lr, h7);
+    // }//  else {
+    // // fast
+    // h0 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h0));
+    // h1 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h1));
+    // h2 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h2));
+    // h3 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h3));
+    // h4 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h4));
+    // h5 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h5));
+    // h6 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h6));
+    // h7 = _mm512_mul_ps(lr, _mm512_rsqrt14_ps(h7));
+    // }
+    h0 = _mm512_mul_ps(h0, g0);
+    h1 = _mm512_mul_ps(h1, g1);
+    h2 = _mm512_mul_ps(h2, g2);
+    h3 = _mm512_mul_ps(h3, g3);
+    h4 = _mm512_mul_ps(h4, g4);
+    h5 = _mm512_mul_ps(h5, g5);
+    h6 = _mm512_mul_ps(h6, g6);
+    h7 = _mm512_mul_ps(h7, g7);
+    w0 = _mm512_add_ps(w0, h0);
+    w1 = _mm512_add_ps(w1, h1);
+    w2 = _mm512_add_ps(w2, h2);
+    w3 = _mm512_add_ps(w3, h3);
+    w4 = _mm512_add_ps(w4, h4);
+    w5 = _mm512_add_ps(w5, h5);
+    w6 = _mm512_add_ps(w6, h6);
+    w7 = _mm512_add_ps(w7, h7);
+    _mm512_store_ps(weight, w0);
+    _mm512_store_ps(weight + 16, w1);
+    _mm512_store_ps(weight + 32, w2);
+    _mm512_store_ps(weight + 48, w3);
+    _mm512_store_ps(weight + 64, w4);
+    _mm512_store_ps(weight + 80, w5);
+    _mm512_store_ps(weight + 96, w6);
+    _mm512_store_ps(weight + 112, w7);
+}
+
+template<int32_t num_emb, int32_t emb_dim>
+void embeddingbag_bwd_dense_acc_kern(const int32_t batch, const int32_t num_hot,
+                                     const int32_t *index, float *acc, HitRec &hit,
+                                     const BF16 *diffresult) {
+    __m512i x0, x1, x2, x3;
+    __m512 y0, y1, y2, y3, y4, y5, y6, y7;
+    for (int32_t i = 0; i < batch; ++i) {
+        const BF16 * gradPtr = &diffresult[i * num_emb * emb_dim];
+        _mm_prefetch(gradPtr, _MM_HINT_T0);
+        _mm_prefetch(gradPtr+32, _MM_HINT_T0);
+        _mm_prefetch(gradPtr+64, _MM_HINT_T0);
+        _mm_prefetch(gradPtr+96, _MM_HINT_T0);
+
+        x0 = _mm512_load_si512(gradPtr);
+        y0 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x0, 0));
+        y1 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x0, 1));
+
+        x1 = _mm512_load_si512(gradPtr + 32);
+        y2 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x1, 0));
+        y3 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x1, 1));
+
+        x2 = _mm512_load_si512(gradPtr + 64);
+        y4 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x2, 0));
+        y5 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x2, 1));
+
+        x3 = _mm512_load_si512(gradPtr + 96);
+        y6 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x3, 0));
+        y7 = _mm512_cvtpbh_ps(_mm512_extracti32x8_epi32(x3, 1));
+
+        for (int32_t j = 0; j < num_hot; ++j) {
+            size_t wid = index[i * num_hot + j]; // weight index
+            float *accPtr = &acc[wid * emb_dim];
+            if (hit[wid]) {
+                _mm512_store_ps(accPtr,
+                                _mm512_add_ps(
+                                    _mm512_load_ps(accPtr), y0));
+                _mm512_store_ps(accPtr + 16,
+                                _mm512_add_ps(
+                                    _mm512_load_ps(accPtr + 16), y1));
+                _mm512_store_ps(accPtr + 32,
+                                _mm512_add_ps(
+                                    _mm512_load_ps(accPtr + 32), y2));
+                _mm512_store_ps(accPtr + 48,
+                                _mm512_add_ps(
+                                    _mm512_load_ps(accPtr + 48), y3));
+                _mm512_store_ps(accPtr + 64,
+                                _mm512_add_ps(
+                                    _mm512_load_ps(accPtr + 64), y4));
+                _mm512_store_ps(accPtr + 80,
+                                _mm512_add_ps(
+                                    _mm512_load_ps(accPtr + 80), y5));
+                _mm512_store_ps(accPtr + 96,
+                                _mm512_add_ps(
+                                    _mm512_load_ps(accPtr + 96), y6));
+                _mm512_store_ps(accPtr + 112,
+                                _mm512_add_ps(
+                                    _mm512_load_ps(accPtr + 112), y7));
+            } else {
+                _mm512_store_ps(accPtr, y0);
+                _mm512_store_ps(accPtr + 16, y1);
+                _mm512_store_ps(accPtr + 32, y2);
+                _mm512_store_ps(accPtr + 48, y3);
+                _mm512_store_ps(accPtr + 64, y4);
+                _mm512_store_ps(accPtr + 80, y5);
+                _mm512_store_ps(accPtr + 96, y6);
+                _mm512_store_ps(accPtr + 112, y7);
+                hit[wid] = 1;
+            }
+        }
+    }
+}
+
+template<int32_t emb_dim>
+void adagrad_update_dense_weight(float *weight, float *hessian,
+                                 const float *acc, HitRec &hit,
+                                 const int32_t len_emb,
+                                 const float lr, const float eps) {
+    // lr < 0
+    __m512 eps_v = _mm512_set1_ps(eps);
+    __m512 lr_v = _mm512_set1_ps(lr);
+    for (size_t i = 0; i < len_emb; ++i) {
+        if (hit[i])
+            adagradupdatedense(&weight[i * emb_dim], &hessian[i * emb_dim],
+                               &acc[i *emb_dim],
+                               lr_v, eps_v);
+    }
+}
+
+template<int32_t emb_dim>
+void adagrad_update_sparse_weight(float *weight, float *hessian,
+                                  SparseEmbeddingGrad &sg,
+                                  const int32_t len_emb,
+                                  const float lr, const float eps) {
+    // lr < 0
+    __m512 eps_v = _mm512_set1_ps(eps);
+    __m512 lr_v = _mm512_set1_ps(lr);
+    for (auto &it : sg) {
+        size_t idx = it.first;
+        auto &grad = it.second;
+        adagradupdatesparse(&weight[idx * emb_dim], &hessian[idx * emb_dim],
+                            lr_v, eps_v,
+                            grad.a0, grad.a1, grad.a2, grad.a3,
+                            grad.a4, grad.a5, grad.a6, grad.a7);
+    }
+}
+
+template<int32_t num_emb, int32_t emb_dim, bool sparse>
+void embeddingbag_bwd_kern(const int32_t batch,
+                           const int32_t num_hot, const int32_t len_emb,
+                           const int32_t *index,
+                           float *weight, float *hessian,
+                           const BF16 *diffresult,
+                           const float lr, const float eps) {
+    if (sparse) {
+        SparseEmbeddingGrad sg;
+        embeddingbag_bwd_sparse_acc_kern<num_emb, emb_dim>(batch, num_hot,
+                                                           index, sg,
+                                                           diffresult);
+        adagrad_update_sparse_weight<emb_dim>(weight, hessian, sg,
+                                              len_emb, lr, eps);
+    } else {
+        // dense here
+        // this buffer can be reused, no need to create/free every time
+        // float *acc = new_data<float>(len_emb * emb_dim * sizeof(float));
+        float *acc = (float *)_mm_malloc(len_emb * emb_dim * sizeof(float), 64);
+        HitRec hit;
+        // save row index and gradient ptr to array for sort
+        embeddingbag_bwd_dense_acc_kern<num_emb, emb_dim>(batch, num_hot,
+                                                          index, acc, hit,
+                                                          diffresult);
+        adagrad_update_dense_weight<emb_dim>(weight, hessian, acc, hit, len_emb, lr,eps);
+        free(acc);
+    }
+}
+
+void embeddingbag_bwd(const int32_t batch, const int32_t num_emb,
+                      const int32_t emb_dim, const int64_t *emb_len,
+                      const int64_t *num_hot,
+                      const int32_t **index, const int32_t **offset,
+                      float **weight, float **hessian,
+                      const BF16 *diffresult,
+                      const float lr, const float eps) {
+    // weight, weight of embedding bag
+    // hessian, approximation of hessian matrix
+    // lr learning rate
+    // eps epislon for grad calculation to avoid divide by zero
+    // #pragma omp parallel for
+    // for (size_t n = 0; n < num_emb; ++n) {
+    //     embeddingbag_bwd_kern<26, 128, true>(batch, num_hot[n], index[n], weight[n], hessian[n],
+    //                                          &diffresult[n *emb_dim],lr, eps);
+    // }
+    std::vector<int32_t> dense_emb_idx;
+    std::vector<int32_t> sparse_emb_idx;
+    for (int32_t i = 0; i < num_emb; ++i) {
+        if (emb_len[i] < 10000) {
+            dense_emb_idx.push_back(i);
+        } else {
+            sparse_emb_idx.push_back(i);
+        }
+    }
+    const int32_t num_dense_emb = dense_emb_idx.size();
+    const int32_t num_sparse_emb = sparse_emb_idx.size();
+    // constexpr int32_t mb = 32768;
+    // const int32_t bb = batch / mb;
+#pragma omp parallel for schedule(guided)
+    for (int32_t j = 0; j < num_dense_emb; ++j) {
+        size_t n = dense_emb_idx[j];
+        size_t index_d = n * emb_dim;
+        embeddingbag_bwd_kern<26, 128, false>(batch,
+                                              static_cast<int>(num_hot[n]),
+                                              static_cast<int>(emb_len[n]),
+                                              index[n],
+                                              weight[n], hessian[n],
+                                              &diffresult[index_d],
+                                              lr, eps);
+    }
+#pragma omp parallel
+    {
+        for (int32_t j = 0; j < num_sparse_emb; ++j) {
+            size_t n = sparse_emb_idx[j];
+            size_t index_d = n * emb_dim;
+            embeddingbag_bwd_kern<26, 128, true>(batch,
+                                                 static_cast<int>(num_hot[n]),
+                                                 static_cast<int>(emb_len[n]),
+                                                 index[n],
+                                                 weight[n], hessian[n],
+                                                 &diffresult[index_d],
+                                                 lr, eps);
+        }
+    }
+}
+#endif
+
+void mlperf_merged_embeddingbag_backward_kernel_impl(
+    const std::vector<at::Tensor>& indices,
+    const std::vector<at::Tensor>& offsets,
+    const std::vector<at::Tensor>& weights,
+    const std::vector<at::Tensor>& hessian,
+    const at::Tensor& diffresult,
+    const std::vector<int64_t>& multihot_size,
+    const std::vector<int64_t>& embedding_size,
+    double lr,
+    double eps) {
+    int32_t batch = indices[2].size(0);
+    int32_t emb_dim = 128;
+    // const std::vector<int64_t> multihot_size {3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1};
+    int32_t num_emb = multihot_size.size();
+    // const std::vector<int64_t> embedding_size {40000000,39060,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36};
+    float *weight_data[num_emb];
+    float *hessian_data[num_emb];
+    const int32_t *index_data[num_emb];
+    const int32_t *offset_data[num_emb];
+
+    for (int i = 0; i < num_emb; ++i) {
+        weight_data[i] = weights[i].data_ptr<float>();
+        index_data[i] = indices[i].data_ptr<int32_t>();
+        offset_data[i] = offsets[i].data_ptr<int32_t>();
+        hessian_data[i] = hessian[i].data_ptr<float>();
+    }
+    BF16 *diffresult_data = diffresult.data_ptr<BF16>();
+
+    #if (defined CPU_CAPABILITY_AVX512_BF16)
+    embeddingbag_bwd(batch, num_emb, emb_dim, embedding_size.data(),
+                     multihot_size.data(), index_data, offset_data,
+                     weight_data, hessian_data, diffresult_data,
+                     lr, eps);
+    #endif
+
+    return;
+}
+
+} // anonymous namespace
+
+REGISTER_DISPATCH(mlperf_merged_embeddingbag_forward_kernel_stub, &mlperf_merged_embeddingbag_forward_kernel_impl);
+REGISTER_DISPATCH(mlperf_merged_embeddingbag_backward_kernel_stub, &mlperf_merged_embeddingbag_backward_kernel_impl);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/robin_hood.h b/csrc/cpu/aten/kernels/robin_hood.h
new file mode 100644
index 000000000..0af031f5f
--- /dev/null
+++ b/csrc/cpu/aten/kernels/robin_hood.h
@@ -0,0 +1,2544 @@
+//                 ______  _____                 ______                _________
+//  ______________ ___  /_ ___(_)_______         ___  /_ ______ ______ ______  /
+//  __  ___/_  __ \__  __ \__  / __  __ \        __  __ \_  __ \_  __ \_  __  /
+//  _  /    / /_/ /_  /_/ /_  /  _  / / /        _  / / // /_/ // /_/ // /_/ /
+//  /_/     \____/ /_.___/ /_/   /_/ /_/ ________/_/ /_/ \____/ \____/ \__,_/
+//                                      _/_____/
+//
+// Fast & memory efficient hashtable based on robin hood hashing for C++11/14/17/20
+// https://github.com/martinus/robin-hood-hashing
+//
+// Licensed under the MIT License <http://opensource.org/licenses/MIT>.
+// SPDX-License-Identifier: MIT
+// Copyright (c) 2018-2021 Martin Ankerl <http://martin.ankerl.com>
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in all
+// copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+// SOFTWARE.
+
+#ifndef ROBIN_HOOD_H_INCLUDED
+#define ROBIN_HOOD_H_INCLUDED
+
+// see https://semver.org/
+#define ROBIN_HOOD_VERSION_MAJOR 3  // for incompatible API changes
+#define ROBIN_HOOD_VERSION_MINOR 11 // for adding functionality in a backwards-compatible manner
+#define ROBIN_HOOD_VERSION_PATCH 5  // for backwards-compatible bug fixes
+
+#include <algorithm>
+#include <cstdlib>
+#include <cstring>
+#include <functional>
+#include <limits>
+#include <memory> // only to support hash of smart pointers
+#include <stdexcept>
+#include <string>
+#include <type_traits>
+#include <utility>
+#if __cplusplus >= 201703L
+#    include <string_view>
+#endif
+
+// #define ROBIN_HOOD_LOG_ENABLED
+#ifdef ROBIN_HOOD_LOG_ENABLED
+#    include <iostream>
+#    define ROBIN_HOOD_LOG(...) \
+        std::cout << __FUNCTION__ << "@" << __LINE__ << ": " << __VA_ARGS__ << std::endl;
+#else
+#    define ROBIN_HOOD_LOG(x)
+#endif
+
+// #define ROBIN_HOOD_TRACE_ENABLED
+#ifdef ROBIN_HOOD_TRACE_ENABLED
+#    include <iostream>
+#    define ROBIN_HOOD_TRACE(...) \
+        std::cout << __FUNCTION__ << "@" << __LINE__ << ": " << __VA_ARGS__ << std::endl;
+#else
+#    define ROBIN_HOOD_TRACE(x)
+#endif
+
+// #define ROBIN_HOOD_COUNT_ENABLED
+#ifdef ROBIN_HOOD_COUNT_ENABLED
+#    include <iostream>
+#    define ROBIN_HOOD_COUNT(x) ++counts().x;
+namespace robin_hood {
+struct Counts {
+    uint64_t shiftUp{};
+    uint64_t shiftDown{};
+};
+inline std::ostream& operator<<(std::ostream& os, Counts const& c) {
+    return os << c.shiftUp << " shiftUp" << std::endl << c.shiftDown << " shiftDown" << std::endl;
+}
+
+static Counts& counts() {
+    static Counts counts{};
+    return counts;
+}
+} // namespace robin_hood
+#else
+#    define ROBIN_HOOD_COUNT(x)
+#endif
+
+// all non-argument macros should use this facility. See
+// https://www.fluentcpp.com/2019/05/28/better-macros-better-flags/
+#define ROBIN_HOOD(x) ROBIN_HOOD_PRIVATE_DEFINITION_##x()
+
+// mark unused members with this macro
+#define ROBIN_HOOD_UNUSED(identifier)
+
+// bitness
+#if SIZE_MAX == UINT32_MAX
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_BITNESS() 32
+#elif SIZE_MAX == UINT64_MAX
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_BITNESS() 64
+#else
+#    error Unsupported bitness
+#endif
+
+// endianess
+#ifdef _MSC_VER
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_LITTLE_ENDIAN() 1
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_BIG_ENDIAN() 0
+#else
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_LITTLE_ENDIAN() \
+        (__BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_BIG_ENDIAN() (__BYTE_ORDER__ == __ORDER_BIG_ENDIAN__)
+#endif
+
+// inline
+#ifdef _MSC_VER
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_NOINLINE() __declspec(noinline)
+#else
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_NOINLINE() __attribute__((noinline))
+#endif
+
+// exceptions
+#if !defined(__cpp_exceptions) && !defined(__EXCEPTIONS) && !defined(_CPPUNWIND)
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_HAS_EXCEPTIONS() 0
+#else
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_HAS_EXCEPTIONS() 1
+#endif
+
+// count leading/trailing bits
+#if !defined(ROBIN_HOOD_DISABLE_INTRINSICS)
+#    ifdef _MSC_VER
+#        if ROBIN_HOOD(BITNESS) == 32
+#            define ROBIN_HOOD_PRIVATE_DEFINITION_BITSCANFORWARD() _BitScanForward
+#        else
+#            define ROBIN_HOOD_PRIVATE_DEFINITION_BITSCANFORWARD() _BitScanForward64
+#        endif
+#        include <intrin.h>
+#        pragma intrinsic(ROBIN_HOOD(BITSCANFORWARD))
+#        define ROBIN_HOOD_COUNT_TRAILING_ZEROES(x)                                       \
+            [](size_t mask) noexcept -> int {                                             \
+                unsigned long index;                                                      \
+                return ROBIN_HOOD(BITSCANFORWARD)(&index, mask) ? static_cast<int>(index) \
+                                                                : ROBIN_HOOD(BITNESS);    \
+            }(x)
+#    else
+#        if ROBIN_HOOD(BITNESS) == 32
+#            define ROBIN_HOOD_PRIVATE_DEFINITION_CTZ() __builtin_ctzl
+#            define ROBIN_HOOD_PRIVATE_DEFINITION_CLZ() __builtin_clzl
+#        else
+#            define ROBIN_HOOD_PRIVATE_DEFINITION_CTZ() __builtin_ctzll
+#            define ROBIN_HOOD_PRIVATE_DEFINITION_CLZ() __builtin_clzll
+#        endif
+#        define ROBIN_HOOD_COUNT_LEADING_ZEROES(x) ((x) ? ROBIN_HOOD(CLZ)(x) : ROBIN_HOOD(BITNESS))
+#        define ROBIN_HOOD_COUNT_TRAILING_ZEROES(x) ((x) ? ROBIN_HOOD(CTZ)(x) : ROBIN_HOOD(BITNESS))
+#    endif
+#endif
+
+// fallthrough
+#ifndef __has_cpp_attribute // For backwards compatibility
+#    define __has_cpp_attribute(x) 0
+#endif
+#if __has_cpp_attribute(clang::fallthrough)
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_FALLTHROUGH() [[clang::fallthrough]]
+#elif __has_cpp_attribute(gnu::fallthrough)
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_FALLTHROUGH() [[gnu::fallthrough]]
+#else
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_FALLTHROUGH()
+#endif
+
+// likely/unlikely
+#ifdef _MSC_VER
+#    define ROBIN_HOOD_LIKELY(condition) condition
+#    define ROBIN_HOOD_UNLIKELY(condition) condition
+#else
+#    define ROBIN_HOOD_LIKELY(condition) __builtin_expect(condition, 1)
+#    define ROBIN_HOOD_UNLIKELY(condition) __builtin_expect(condition, 0)
+#endif
+
+// detect if native wchar_t type is availiable in MSVC
+#ifdef _MSC_VER
+#    ifdef _NATIVE_WCHAR_T_DEFINED
+#        define ROBIN_HOOD_PRIVATE_DEFINITION_HAS_NATIVE_WCHART() 1
+#    else
+#        define ROBIN_HOOD_PRIVATE_DEFINITION_HAS_NATIVE_WCHART() 0
+#    endif
+#else
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_HAS_NATIVE_WCHART() 1
+#endif
+
+// detect if MSVC supports the pair(std::piecewise_construct_t,...) consructor being constexpr
+#ifdef _MSC_VER
+#    if _MSC_VER <= 1900
+#        define ROBIN_HOOD_PRIVATE_DEFINITION_BROKEN_CONSTEXPR() 1
+#    else
+#        define ROBIN_HOOD_PRIVATE_DEFINITION_BROKEN_CONSTEXPR() 0
+#    endif
+#else
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_BROKEN_CONSTEXPR() 0
+#endif
+
+// workaround missing "is_trivially_copyable" in g++ < 5.0
+// See https://stackoverflow.com/a/31798726/48181
+#if defined(__GNUC__) && __GNUC__ < 5
+#    define ROBIN_HOOD_IS_TRIVIALLY_COPYABLE(...) __has_trivial_copy(__VA_ARGS__)
+#else
+#    define ROBIN_HOOD_IS_TRIVIALLY_COPYABLE(...) std::is_trivially_copyable<__VA_ARGS__>::value
+#endif
+
+// helpers for C++ versions, see https://gcc.gnu.org/onlinedocs/cpp/Standard-Predefined-Macros.html
+#define ROBIN_HOOD_PRIVATE_DEFINITION_CXX() __cplusplus
+#define ROBIN_HOOD_PRIVATE_DEFINITION_CXX98() 199711L
+#define ROBIN_HOOD_PRIVATE_DEFINITION_CXX11() 201103L
+#define ROBIN_HOOD_PRIVATE_DEFINITION_CXX14() 201402L
+#define ROBIN_HOOD_PRIVATE_DEFINITION_CXX17() 201703L
+
+#if ROBIN_HOOD(CXX) >= ROBIN_HOOD(CXX17)
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_NODISCARD() [[nodiscard]]
+#else
+#    define ROBIN_HOOD_PRIVATE_DEFINITION_NODISCARD()
+#endif
+
+namespace robin_hood {
+
+#if ROBIN_HOOD(CXX) >= ROBIN_HOOD(CXX14)
+#    define ROBIN_HOOD_STD std
+#else
+
+// c++11 compatibility layer
+namespace ROBIN_HOOD_STD {
+template <class T>
+struct alignment_of
+    : std::integral_constant<std::size_t, alignof(typename std::remove_all_extents<T>::type)> {};
+
+template <class T, T... Ints>
+class integer_sequence {
+public:
+    using value_type = T;
+    static_assert(std::is_integral<value_type>::value, "not integral type");
+    static constexpr std::size_t size() noexcept {
+        return sizeof...(Ints);
+    }
+};
+template <std::size_t... Inds>
+using index_sequence = integer_sequence<std::size_t, Inds...>;
+
+namespace detail_ {
+template <class T, T Begin, T End, bool>
+struct IntSeqImpl {
+    using TValue = T;
+    static_assert(std::is_integral<TValue>::value, "not integral type");
+    static_assert(Begin >= 0 && Begin < End, "unexpected argument (Begin<0 || Begin<=End)");
+
+    template <class, class>
+    struct IntSeqCombiner;
+
+    template <TValue... Inds0, TValue... Inds1>
+    struct IntSeqCombiner<integer_sequence<TValue, Inds0...>, integer_sequence<TValue, Inds1...>> {
+        using TResult = integer_sequence<TValue, Inds0..., Inds1...>;
+    };
+
+    using TResult =
+        typename IntSeqCombiner<typename IntSeqImpl<TValue, Begin, Begin + (End - Begin) / 2,
+                                                    (End - Begin) / 2 == 1>::TResult,
+                                typename IntSeqImpl<TValue, Begin + (End - Begin) / 2, End,
+                                                    (End - Begin + 1) / 2 == 1>::TResult>::TResult;
+};
+
+template <class T, T Begin>
+struct IntSeqImpl<T, Begin, Begin, false> {
+    using TValue = T;
+    static_assert(std::is_integral<TValue>::value, "not integral type");
+    static_assert(Begin >= 0, "unexpected argument (Begin<0)");
+    using TResult = integer_sequence<TValue>;
+};
+
+template <class T, T Begin, T End>
+struct IntSeqImpl<T, Begin, End, true> {
+    using TValue = T;
+    static_assert(std::is_integral<TValue>::value, "not integral type");
+    static_assert(Begin >= 0, "unexpected argument (Begin<0)");
+    using TResult = integer_sequence<TValue, Begin>;
+};
+} // namespace detail_
+
+template <class T, T N>
+using make_integer_sequence = typename detail_::IntSeqImpl<T, 0, N, (N - 0) == 1>::TResult;
+
+template <std::size_t N>
+using make_index_sequence = make_integer_sequence<std::size_t, N>;
+
+template <class... T>
+using index_sequence_for = make_index_sequence<sizeof...(T)>;
+
+} // namespace ROBIN_HOOD_STD
+
+#endif
+
+namespace detail {
+
+// make sure we static_cast to the correct type for hash_int
+#if ROBIN_HOOD(BITNESS) == 64
+using SizeT = uint64_t;
+#else
+using SizeT = uint32_t;
+#endif
+
+template <typename T>
+T rotr(T x, unsigned k) {
+    return (x >> k) | (x << (8U * sizeof(T) - k));
+}
+
+// This cast gets rid of warnings like "cast from 'uint8_t*' {aka 'unsigned char*'} to
+// 'uint64_t*' {aka 'long unsigned int*'} increases required alignment of target type". Use with
+// care!
+template <typename T>
+inline T reinterpret_cast_no_cast_align_warning(void* ptr) noexcept {
+    return reinterpret_cast<T>(ptr);
+}
+
+template <typename T>
+inline T reinterpret_cast_no_cast_align_warning(void const* ptr) noexcept {
+    return reinterpret_cast<T>(ptr);
+}
+
+// make sure this is not inlined as it is slow and dramatically enlarges code, thus making other
+// inlinings more difficult. Throws are also generally the slow path.
+template <typename E, typename... Args>
+[[noreturn]] ROBIN_HOOD(NOINLINE)
+#if ROBIN_HOOD(HAS_EXCEPTIONS)
+    void doThrow(Args&&... args) {
+    // NOLINTNEXTLINE(cppcoreguidelines-pro-bounds-array-to-pointer-decay)
+    throw E(std::forward<Args>(args)...);
+}
+#else
+    void doThrow(Args&&... ROBIN_HOOD_UNUSED(args) /*unused*/) {
+    abort();
+}
+#endif
+
+template <typename E, typename T, typename... Args>
+T* assertNotNull(T* t, Args&&... args) {
+    if (ROBIN_HOOD_UNLIKELY(nullptr == t)) {
+        doThrow<E>(std::forward<Args>(args)...);
+    }
+    return t;
+}
+
+template <typename T>
+inline T unaligned_load(void const* ptr) noexcept {
+    // using memcpy so we don't get into unaligned load problems.
+    // compiler should optimize this very well anyways.
+    T t;
+    std::memcpy(&t, ptr, sizeof(T));
+    return t;
+}
+
+// Allocates bulks of memory for objects of type T. This deallocates the memory in the destructor,
+// and keeps a linked list of the allocated memory around. Overhead per allocation is the size of a
+// pointer.
+template <typename T, size_t MinNumAllocs = 4, size_t MaxNumAllocs = 256>
+class BulkPoolAllocator {
+public:
+    BulkPoolAllocator() noexcept = default;
+
+    // does not copy anything, just creates a new allocator.
+    BulkPoolAllocator(const BulkPoolAllocator& ROBIN_HOOD_UNUSED(o) /*unused*/) noexcept
+        : mHead(nullptr)
+        , mListForFree(nullptr) {}
+
+    BulkPoolAllocator(BulkPoolAllocator&& o) noexcept
+        : mHead(o.mHead)
+        , mListForFree(o.mListForFree) {
+        o.mListForFree = nullptr;
+        o.mHead = nullptr;
+    }
+
+    BulkPoolAllocator& operator=(BulkPoolAllocator&& o) noexcept {
+        reset();
+        mHead = o.mHead;
+        mListForFree = o.mListForFree;
+        o.mListForFree = nullptr;
+        o.mHead = nullptr;
+        return *this;
+    }
+
+    BulkPoolAllocator&
+    // NOLINTNEXTLINE(bugprone-unhandled-self-assignment,cert-oop54-cpp)
+    operator=(const BulkPoolAllocator& ROBIN_HOOD_UNUSED(o) /*unused*/) noexcept {
+        // does not do anything
+        return *this;
+    }
+
+    ~BulkPoolAllocator() noexcept {
+        reset();
+    }
+
+    // Deallocates all allocated memory.
+    void reset() noexcept {
+        while (mListForFree) {
+            T* tmp = *mListForFree;
+            ROBIN_HOOD_LOG("std::free")
+            std::free(mListForFree);
+            mListForFree = reinterpret_cast_no_cast_align_warning<T**>(tmp);
+        }
+        mHead = nullptr;
+    }
+
+    // allocates, but does NOT initialize. Use in-place new constructor, e.g.
+    //   T* obj = pool.allocate();
+    //   ::new (static_cast<void*>(obj)) T();
+    T* allocate() {
+        T* tmp = mHead;
+        if (!tmp) {
+            tmp = performAllocation();
+        }
+
+        mHead = *reinterpret_cast_no_cast_align_warning<T**>(tmp);
+        return tmp;
+    }
+
+    // does not actually deallocate but puts it in store.
+    // make sure you have already called the destructor! e.g. with
+    //  obj->~T();
+    //  pool.deallocate(obj);
+    void deallocate(T* obj) noexcept {
+        *reinterpret_cast_no_cast_align_warning<T**>(obj) = mHead;
+        mHead = obj;
+    }
+
+    // Adds an already allocated block of memory to the allocator. This allocator is from now on
+    // responsible for freeing the data (with free()). If the provided data is not large enough to
+    // make use of, it is immediately freed. Otherwise it is reused and freed in the destructor.
+    void addOrFree(void* ptr, const size_t numBytes) noexcept {
+        // calculate number of available elements in ptr
+        if (numBytes < ALIGNMENT + ALIGNED_SIZE) {
+            // not enough data for at least one element. Free and return.
+            ROBIN_HOOD_LOG("std::free")
+            std::free(ptr);
+        } else {
+            ROBIN_HOOD_LOG("add to buffer")
+            add(ptr, numBytes);
+        }
+    }
+
+    void swap(BulkPoolAllocator<T, MinNumAllocs, MaxNumAllocs>& other) noexcept {
+        using std::swap;
+        swap(mHead, other.mHead);
+        swap(mListForFree, other.mListForFree);
+    }
+
+private:
+    // iterates the list of allocated memory to calculate how many to alloc next.
+    // Recalculating this each time saves us a size_t member.
+    // This ignores the fact that memory blocks might have been added manually with addOrFree. In
+    // practice, this should not matter much.
+    ROBIN_HOOD(NODISCARD) size_t calcNumElementsToAlloc() const noexcept {
+        auto tmp = mListForFree;
+        size_t numAllocs = MinNumAllocs;
+
+        while (numAllocs * 2 <= MaxNumAllocs && tmp) {
+            auto x = reinterpret_cast<T***>(tmp);
+            tmp = *x;
+            numAllocs *= 2;
+        }
+
+        return numAllocs;
+    }
+
+    // WARNING: Underflow if numBytes < ALIGNMENT! This is guarded in addOrFree().
+    void add(void* ptr, const size_t numBytes) noexcept {
+        const size_t numElements = (numBytes - ALIGNMENT) / ALIGNED_SIZE;
+
+        auto data = reinterpret_cast<T**>(ptr);
+
+        // link free list
+        auto x = reinterpret_cast<T***>(data);
+        *x = mListForFree;
+        mListForFree = data;
+
+        // create linked list for newly allocated data
+        auto* const headT =
+            reinterpret_cast_no_cast_align_warning<T*>(reinterpret_cast<char*>(ptr) + ALIGNMENT);
+
+        auto* const head = reinterpret_cast<char*>(headT);
+
+        // Visual Studio compiler automatically unrolls this loop, which is pretty cool
+        for (size_t i = 0; i < numElements; ++i) {
+            *reinterpret_cast_no_cast_align_warning<char**>(head + i * ALIGNED_SIZE) =
+                head + (i + 1) * ALIGNED_SIZE;
+        }
+
+        // last one points to 0
+        *reinterpret_cast_no_cast_align_warning<T**>(head + (numElements - 1) * ALIGNED_SIZE) =
+            mHead;
+        mHead = headT;
+    }
+
+    // Called when no memory is available (mHead == 0).
+    // Don't inline this slow path.
+    ROBIN_HOOD(NOINLINE) T* performAllocation() {
+        size_t const numElementsToAlloc = calcNumElementsToAlloc();
+
+        // alloc new memory: [prev |T, T, ... T]
+        size_t const bytes = ALIGNMENT + ALIGNED_SIZE * numElementsToAlloc;
+        ROBIN_HOOD_LOG("std::malloc " << bytes << " = " << ALIGNMENT << " + " << ALIGNED_SIZE
+                                      << " * " << numElementsToAlloc)
+        add(assertNotNull<std::bad_alloc>(std::malloc(bytes)), bytes);
+        return mHead;
+    }
+
+    // enforce byte alignment of the T's
+#if ROBIN_HOOD(CXX) >= ROBIN_HOOD(CXX14)
+    static constexpr size_t ALIGNMENT =
+        (std::max)(std::alignment_of<T>::value, std::alignment_of<T*>::value);
+#else
+    static const size_t ALIGNMENT =
+        (ROBIN_HOOD_STD::alignment_of<T>::value > ROBIN_HOOD_STD::alignment_of<T*>::value)
+            ? ROBIN_HOOD_STD::alignment_of<T>::value
+            : +ROBIN_HOOD_STD::alignment_of<T*>::value; // the + is for walkarround
+#endif
+
+    static constexpr size_t ALIGNED_SIZE = ((sizeof(T) - 1) / ALIGNMENT + 1) * ALIGNMENT;
+
+    static_assert(MinNumAllocs >= 1, "MinNumAllocs");
+    static_assert(MaxNumAllocs >= MinNumAllocs, "MaxNumAllocs");
+    static_assert(ALIGNED_SIZE >= sizeof(T*), "ALIGNED_SIZE");
+    static_assert(0 == (ALIGNED_SIZE % sizeof(T*)), "ALIGNED_SIZE mod");
+    static_assert(ALIGNMENT >= sizeof(T*), "ALIGNMENT");
+
+    T* mHead{nullptr};
+    T** mListForFree{nullptr};
+};
+
+template <typename T, size_t MinSize, size_t MaxSize, bool IsFlat>
+struct NodeAllocator;
+
+// dummy allocator that does nothing
+template <typename T, size_t MinSize, size_t MaxSize>
+struct NodeAllocator<T, MinSize, MaxSize, true> {
+
+    // we are not using the data, so just free it.
+    void addOrFree(void* ptr, size_t ROBIN_HOOD_UNUSED(numBytes) /*unused*/) noexcept {
+        ROBIN_HOOD_LOG("std::free")
+        std::free(ptr);
+    }
+};
+
+template <typename T, size_t MinSize, size_t MaxSize>
+struct NodeAllocator<T, MinSize, MaxSize, false> : public BulkPoolAllocator<T, MinSize, MaxSize> {};
+
+// c++14 doesn't have is_nothrow_swappable, and clang++ 6.0.1 doesn't like it either, so I'm making
+// my own here.
+namespace swappable {
+#if ROBIN_HOOD(CXX) < ROBIN_HOOD(CXX17)
+using std::swap;
+template <typename T>
+struct nothrow {
+    static const bool value = noexcept(swap(std::declval<T&>(), std::declval<T&>()));
+};
+#else
+template <typename T>
+struct nothrow {
+    static const bool value = std::is_nothrow_swappable<T>::value;
+};
+#endif
+} // namespace swappable
+
+} // namespace detail
+
+struct is_transparent_tag {};
+
+// A custom pair implementation is used in the map because std::pair is not is_trivially_copyable,
+// which means it would  not be allowed to be used in std::memcpy. This struct is copyable, which is
+// also tested.
+template <typename T1, typename T2>
+struct pair {
+    using first_type = T1;
+    using second_type = T2;
+
+    template <typename U1 = T1, typename U2 = T2,
+              typename = typename std::enable_if<std::is_default_constructible<U1>::value &&
+                                                 std::is_default_constructible<U2>::value>::type>
+    constexpr pair() noexcept(noexcept(U1()) && noexcept(U2()))
+        : first()
+        , second() {}
+
+    // pair constructors are explicit so we don't accidentally call this ctor when we don't have to.
+    explicit constexpr pair(std::pair<T1, T2> const& o) noexcept(
+        noexcept(T1(std::declval<T1 const&>())) && noexcept(T2(std::declval<T2 const&>())))
+        : first(o.first)
+        , second(o.second) {}
+
+    // pair constructors are explicit so we don't accidentally call this ctor when we don't have to.
+    explicit constexpr pair(std::pair<T1, T2>&& o) noexcept(noexcept(
+        T1(std::move(std::declval<T1&&>()))) && noexcept(T2(std::move(std::declval<T2&&>()))))
+        : first(std::move(o.first))
+        , second(std::move(o.second)) {}
+
+    constexpr pair(T1&& a, T2&& b) noexcept(noexcept(
+        T1(std::move(std::declval<T1&&>()))) && noexcept(T2(std::move(std::declval<T2&&>()))))
+        : first(std::move(a))
+        , second(std::move(b)) {}
+
+    template <typename U1, typename U2>
+    constexpr pair(U1&& a, U2&& b) noexcept(noexcept(T1(std::forward<U1>(
+        std::declval<U1&&>()))) && noexcept(T2(std::forward<U2>(std::declval<U2&&>()))))
+        : first(std::forward<U1>(a))
+        , second(std::forward<U2>(b)) {}
+
+    template <typename... U1, typename... U2>
+    // MSVC 2015 produces error "C2476: constexpr constructor does not initialize all members"
+    // if this constructor is constexpr
+#if !ROBIN_HOOD(BROKEN_CONSTEXPR)
+    constexpr
+#endif
+        pair(std::piecewise_construct_t /*unused*/, std::tuple<U1...> a,
+             std::tuple<U2...>
+                 b) noexcept(noexcept(pair(std::declval<std::tuple<U1...>&>(),
+                                           std::declval<std::tuple<U2...>&>(),
+                                           ROBIN_HOOD_STD::index_sequence_for<U1...>(),
+                                           ROBIN_HOOD_STD::index_sequence_for<U2...>())))
+        : pair(a, b, ROBIN_HOOD_STD::index_sequence_for<U1...>(),
+               ROBIN_HOOD_STD::index_sequence_for<U2...>()) {
+    }
+
+    // constructor called from the std::piecewise_construct_t ctor
+    template <typename... U1, size_t... I1, typename... U2, size_t... I2>
+    pair(std::tuple<U1...>& a, std::tuple<U2...>& b, ROBIN_HOOD_STD::index_sequence<I1...> /*unused*/, ROBIN_HOOD_STD::index_sequence<I2...> /*unused*/) noexcept(
+        noexcept(T1(std::forward<U1>(std::get<I1>(
+            std::declval<std::tuple<
+                U1...>&>()))...)) && noexcept(T2(std::
+                                                     forward<U2>(std::get<I2>(
+                                                         std::declval<std::tuple<U2...>&>()))...)))
+        : first(std::forward<U1>(std::get<I1>(a))...)
+        , second(std::forward<U2>(std::get<I2>(b))...) {
+        // make visual studio compiler happy about warning about unused a & b.
+        // Visual studio's pair implementation disables warning 4100.
+        (void)a;
+        (void)b;
+    }
+
+    void swap(pair<T1, T2>& o) noexcept((detail::swappable::nothrow<T1>::value) &&
+                                        (detail::swappable::nothrow<T2>::value)) {
+        using std::swap;
+        swap(first, o.first);
+        swap(second, o.second);
+    }
+
+    T1 first;  // NOLINT(misc-non-private-member-variables-in-classes)
+    T2 second; // NOLINT(misc-non-private-member-variables-in-classes)
+};
+
+template <typename A, typename B>
+inline void swap(pair<A, B>& a, pair<A, B>& b) noexcept(
+    noexcept(std::declval<pair<A, B>&>().swap(std::declval<pair<A, B>&>()))) {
+    a.swap(b);
+}
+
+template <typename A, typename B>
+inline constexpr bool operator==(pair<A, B> const& x, pair<A, B> const& y) {
+    return (x.first == y.first) && (x.second == y.second);
+}
+template <typename A, typename B>
+inline constexpr bool operator!=(pair<A, B> const& x, pair<A, B> const& y) {
+    return !(x == y);
+}
+template <typename A, typename B>
+inline constexpr bool operator<(pair<A, B> const& x, pair<A, B> const& y) noexcept(noexcept(
+    std::declval<A const&>() < std::declval<A const&>()) && noexcept(std::declval<B const&>() <
+                                                                     std::declval<B const&>())) {
+    return x.first < y.first || (!(y.first < x.first) && x.second < y.second);
+}
+template <typename A, typename B>
+inline constexpr bool operator>(pair<A, B> const& x, pair<A, B> const& y) {
+    return y < x;
+}
+template <typename A, typename B>
+inline constexpr bool operator<=(pair<A, B> const& x, pair<A, B> const& y) {
+    return !(x > y);
+}
+template <typename A, typename B>
+inline constexpr bool operator>=(pair<A, B> const& x, pair<A, B> const& y) {
+    return !(x < y);
+}
+
+inline size_t hash_bytes(void const* ptr, size_t len) noexcept {
+    static constexpr uint64_t m = UINT64_C(0xc6a4a7935bd1e995);
+    static constexpr uint64_t seed = UINT64_C(0xe17a1465);
+    static constexpr unsigned int r = 47;
+
+    auto const* const data64 = static_cast<uint64_t const*>(ptr);
+    uint64_t h = seed ^ (len * m);
+
+    size_t const n_blocks = len / 8;
+    for (size_t i = 0; i < n_blocks; ++i) {
+        auto k = detail::unaligned_load<uint64_t>(data64 + i);
+
+        k *= m;
+        k ^= k >> r;
+        k *= m;
+
+        h ^= k;
+        h *= m;
+    }
+
+    auto const* const data8 = reinterpret_cast<uint8_t const*>(data64 + n_blocks);
+    switch (len & 7U) {
+    case 7:
+        h ^= static_cast<uint64_t>(data8[6]) << 48U;
+        ROBIN_HOOD(FALLTHROUGH); // FALLTHROUGH
+    case 6:
+        h ^= static_cast<uint64_t>(data8[5]) << 40U;
+        ROBIN_HOOD(FALLTHROUGH); // FALLTHROUGH
+    case 5:
+        h ^= static_cast<uint64_t>(data8[4]) << 32U;
+        ROBIN_HOOD(FALLTHROUGH); // FALLTHROUGH
+    case 4:
+        h ^= static_cast<uint64_t>(data8[3]) << 24U;
+        ROBIN_HOOD(FALLTHROUGH); // FALLTHROUGH
+    case 3:
+        h ^= static_cast<uint64_t>(data8[2]) << 16U;
+        ROBIN_HOOD(FALLTHROUGH); // FALLTHROUGH
+    case 2:
+        h ^= static_cast<uint64_t>(data8[1]) << 8U;
+        ROBIN_HOOD(FALLTHROUGH); // FALLTHROUGH
+    case 1:
+        h ^= static_cast<uint64_t>(data8[0]);
+        h *= m;
+        ROBIN_HOOD(FALLTHROUGH); // FALLTHROUGH
+    default:
+        break;
+    }
+
+    h ^= h >> r;
+
+    // not doing the final step here, because this will be done by keyToIdx anyways
+    // h *= m;
+    // h ^= h >> r;
+    return static_cast<size_t>(h);
+}
+
+inline size_t hash_int(uint64_t x) noexcept {
+    // tried lots of different hashes, let's stick with murmurhash3. It's simple, fast, well tested,
+    // and doesn't need any special 128bit operations.
+    x ^= x >> 33U;
+    x *= UINT64_C(0xff51afd7ed558ccd);
+    x ^= x >> 33U;
+
+    // not doing the final step here, because this will be done by keyToIdx anyways
+    // x *= UINT64_C(0xc4ceb9fe1a85ec53);
+    // x ^= x >> 33U;
+    return static_cast<size_t>(x);
+}
+
+// A thin wrapper around std::hash, performing an additional simple mixing step of the result.
+template <typename T, typename Enable = void>
+struct hash : public std::hash<T> {
+    size_t operator()(T const& obj) const
+        noexcept(noexcept(std::declval<std::hash<T>>().operator()(std::declval<T const&>()))) {
+        // call base hash
+        auto result = std::hash<T>::operator()(obj);
+        // return mixed of that, to be save against identity has
+        return hash_int(static_cast<detail::SizeT>(result));
+    }
+};
+
+template <typename CharT>
+struct hash<std::basic_string<CharT>> {
+    size_t operator()(std::basic_string<CharT> const& str) const noexcept {
+        return hash_bytes(str.data(), sizeof(CharT) * str.size());
+    }
+};
+
+#if ROBIN_HOOD(CXX) >= ROBIN_HOOD(CXX17)
+template <typename CharT>
+struct hash<std::basic_string_view<CharT>> {
+    size_t operator()(std::basic_string_view<CharT> const& sv) const noexcept {
+        return hash_bytes(sv.data(), sizeof(CharT) * sv.size());
+    }
+};
+#endif
+
+template <class T>
+struct hash<T*> {
+    size_t operator()(T* ptr) const noexcept {
+        return hash_int(reinterpret_cast<detail::SizeT>(ptr));
+    }
+};
+
+template <class T>
+struct hash<std::unique_ptr<T>> {
+    size_t operator()(std::unique_ptr<T> const& ptr) const noexcept {
+        return hash_int(reinterpret_cast<detail::SizeT>(ptr.get()));
+    }
+};
+
+template <class T>
+struct hash<std::shared_ptr<T>> {
+    size_t operator()(std::shared_ptr<T> const& ptr) const noexcept {
+        return hash_int(reinterpret_cast<detail::SizeT>(ptr.get()));
+    }
+};
+
+template <typename Enum>
+struct hash<Enum, typename std::enable_if<std::is_enum<Enum>::value>::type> {
+    size_t operator()(Enum e) const noexcept {
+        using Underlying = typename std::underlying_type<Enum>::type;
+        return hash<Underlying>{}(static_cast<Underlying>(e));
+    }
+};
+
+#define ROBIN_HOOD_HASH_INT(T)                           \
+    template <>                                          \
+    struct hash<T> {                                     \
+        size_t operator()(T const& obj) const noexcept { \
+            return hash_int(static_cast<uint64_t>(obj)); \
+        }                                                \
+    }
+
+#if defined(__GNUC__) && !defined(__clang__)
+#    pragma GCC diagnostic push
+#    pragma GCC diagnostic ignored "-Wuseless-cast"
+#endif
+// see https://en.cppreference.com/w/cpp/utility/hash
+ROBIN_HOOD_HASH_INT(bool);
+ROBIN_HOOD_HASH_INT(char);
+ROBIN_HOOD_HASH_INT(signed char);
+ROBIN_HOOD_HASH_INT(unsigned char);
+ROBIN_HOOD_HASH_INT(char16_t);
+ROBIN_HOOD_HASH_INT(char32_t);
+#if ROBIN_HOOD(HAS_NATIVE_WCHART)
+ROBIN_HOOD_HASH_INT(wchar_t);
+#endif
+ROBIN_HOOD_HASH_INT(short);
+ROBIN_HOOD_HASH_INT(unsigned short);
+ROBIN_HOOD_HASH_INT(int);
+ROBIN_HOOD_HASH_INT(unsigned int);
+ROBIN_HOOD_HASH_INT(long);
+ROBIN_HOOD_HASH_INT(long long);
+ROBIN_HOOD_HASH_INT(unsigned long);
+ROBIN_HOOD_HASH_INT(unsigned long long);
+#if defined(__GNUC__) && !defined(__clang__)
+#    pragma GCC diagnostic pop
+#endif
+namespace detail {
+
+template <typename T>
+struct void_type {
+    using type = void;
+};
+
+template <typename T, typename = void>
+struct has_is_transparent : public std::false_type {};
+
+template <typename T>
+struct has_is_transparent<T, typename void_type<typename T::is_transparent>::type>
+    : public std::true_type {};
+
+// using wrapper classes for hash and key_equal prevents the diamond problem when the same type
+// is used. see https://stackoverflow.com/a/28771920/48181
+template <typename T>
+struct WrapHash : public T {
+    WrapHash() = default;
+    explicit WrapHash(T const& o) noexcept(noexcept(T(std::declval<T const&>())))
+        : T(o) {}
+};
+
+template <typename T>
+struct WrapKeyEqual : public T {
+    WrapKeyEqual() = default;
+    explicit WrapKeyEqual(T const& o) noexcept(noexcept(T(std::declval<T const&>())))
+        : T(o) {}
+};
+
+// A highly optimized hashmap implementation, using the Robin Hood algorithm.
+//
+// In most cases, this map should be usable as a drop-in replacement for std::unordered_map, but
+// be about 2x faster in most cases and require much less allocations.
+//
+// This implementation uses the following memory layout:
+//
+// [Node, Node, ... Node | info, info, ... infoSentinel ]
+//
+// * Node: either a DataNode that directly has the std::pair<key, val> as member,
+//   or a DataNode with a pointer to std::pair<key,val>. Which DataNode representation to use
+//   depends on how fast the swap() operation is. Heuristically, this is automatically choosen
+//   based on sizeof(). there are always 2^n Nodes.
+//
+// * info: Each Node in the map has a corresponding info byte, so there are 2^n info bytes.
+//   Each byte is initialized to 0, meaning the corresponding Node is empty. Set to 1 means the
+//   corresponding node contains data. Set to 2 means the corresponding Node is filled, but it
+//   actually belongs to the previous position and was pushed out because that place is already
+//   taken.
+//
+// * infoSentinel: Sentinel byte set to 1, so that iterator's ++ can stop at end() without the
+//   need for a idx variable.
+//
+// According to STL, order of templates has effect on throughput. That's why I've moved the
+// boolean to the front.
+// https://www.reddit.com/r/cpp/comments/ahp6iu/compile_time_binary_size_reductions_and_cs_future/eeguck4/
+template <bool IsFlat, size_t MaxLoadFactor100, typename Key, typename T, typename Hash,
+          typename KeyEqual>
+class Table
+    : public WrapHash<Hash>,
+      public WrapKeyEqual<KeyEqual>,
+      detail::NodeAllocator<
+          typename std::conditional<
+              std::is_void<T>::value, Key,
+              robin_hood::pair<typename std::conditional<IsFlat, Key, Key const>::type, T>>::type,
+          4, 16384, IsFlat> {
+public:
+    static constexpr bool is_flat = IsFlat;
+    static constexpr bool is_map = !std::is_void<T>::value;
+    static constexpr bool is_set = !is_map;
+    static constexpr bool is_transparent =
+        has_is_transparent<Hash>::value && has_is_transparent<KeyEqual>::value;
+
+    using key_type = Key;
+    using mapped_type = T;
+    using value_type = typename std::conditional<
+        is_set, Key,
+        robin_hood::pair<typename std::conditional<is_flat, Key, Key const>::type, T>>::type;
+    using size_type = size_t;
+    using hasher = Hash;
+    using key_equal = KeyEqual;
+    using Self = Table<IsFlat, MaxLoadFactor100, key_type, mapped_type, hasher, key_equal>;
+
+private:
+    static_assert(MaxLoadFactor100 > 10 && MaxLoadFactor100 < 100,
+                  "MaxLoadFactor100 needs to be >10 && < 100");
+
+    using WHash = WrapHash<Hash>;
+    using WKeyEqual = WrapKeyEqual<KeyEqual>;
+
+    // configuration defaults
+
+    // make sure we have 8 elements, needed to quickly rehash mInfo
+    static constexpr size_t InitialNumElements = sizeof(uint64_t);
+    static constexpr uint32_t InitialInfoNumBits = 5;
+    static constexpr uint8_t InitialInfoInc = 1U << InitialInfoNumBits;
+    static constexpr size_t InfoMask = InitialInfoInc - 1U;
+    static constexpr uint8_t InitialInfoHashShift = 0;
+    using DataPool = detail::NodeAllocator<value_type, 4, 16384, IsFlat>;
+
+    // type needs to be wider than uint8_t.
+    using InfoType = uint32_t;
+
+    // DataNode ////////////////////////////////////////////////////////
+
+    // Primary template for the data node. We have special implementations for small and big
+    // objects. For large objects it is assumed that swap() is fairly slow, so we allocate these
+    // on the heap so swap merely swaps a pointer.
+    template <typename M, bool>
+    class DataNode {};
+
+    // Small: just allocate on the stack.
+    template <typename M>
+    class DataNode<M, true> final {
+    public:
+        template <typename... Args>
+        explicit DataNode(M& ROBIN_HOOD_UNUSED(map) /*unused*/, Args&&... args) noexcept(
+            noexcept(value_type(std::forward<Args>(args)...)))
+            : mData(std::forward<Args>(args)...) {}
+
+        DataNode(M& ROBIN_HOOD_UNUSED(map) /*unused*/, DataNode<M, true>&& n) noexcept(
+            std::is_nothrow_move_constructible<value_type>::value)
+            : mData(std::move(n.mData)) {}
+
+        // doesn't do anything
+        void destroy(M& ROBIN_HOOD_UNUSED(map) /*unused*/) noexcept {}
+        void destroyDoNotDeallocate() noexcept {}
+
+        value_type const* operator->() const noexcept {
+            return &mData;
+        }
+        value_type* operator->() noexcept {
+            return &mData;
+        }
+
+        const value_type& operator*() const noexcept {
+            return mData;
+        }
+
+        value_type& operator*() noexcept {
+            return mData;
+        }
+
+        template <typename VT = value_type>
+        ROBIN_HOOD(NODISCARD)
+        typename std::enable_if<is_map, typename VT::first_type&>::type getFirst() noexcept {
+            return mData.first;
+        }
+        template <typename VT = value_type>
+        ROBIN_HOOD(NODISCARD)
+        typename std::enable_if<is_set, VT&>::type getFirst() noexcept {
+            return mData;
+        }
+
+        template <typename VT = value_type>
+        ROBIN_HOOD(NODISCARD)
+        typename std::enable_if<is_map, typename VT::first_type const&>::type
+            getFirst() const noexcept {
+            return mData.first;
+        }
+        template <typename VT = value_type>
+        ROBIN_HOOD(NODISCARD)
+        typename std::enable_if<is_set, VT const&>::type getFirst() const noexcept {
+            return mData;
+        }
+
+        template <typename MT = mapped_type>
+        ROBIN_HOOD(NODISCARD)
+        typename std::enable_if<is_map, MT&>::type getSecond() noexcept {
+            return mData.second;
+        }
+
+        template <typename MT = mapped_type>
+        ROBIN_HOOD(NODISCARD)
+        typename std::enable_if<is_set, MT const&>::type getSecond() const noexcept {
+            return mData.second;
+        }
+
+        void swap(DataNode<M, true>& o) noexcept(
+            noexcept(std::declval<value_type>().swap(std::declval<value_type>()))) {
+            mData.swap(o.mData);
+        }
+
+    private:
+        value_type mData;
+    };
+
+    // big object: allocate on heap.
+    template <typename M>
+    class DataNode<M, false> {
+    public:
+        template <typename... Args>
+        explicit DataNode(M& map, Args&&... args)
+            : mData(map.allocate()) {
+            ::new (static_cast<void*>(mData)) value_type(std::forward<Args>(args)...);
+        }
+
+        DataNode(M& ROBIN_HOOD_UNUSED(map) /*unused*/, DataNode<M, false>&& n) noexcept
+            : mData(std::move(n.mData)) {}
+
+        void destroy(M& map) noexcept {
+            // don't deallocate, just put it into list of datapool.
+            mData->~value_type();
+            map.deallocate(mData);
+        }
+
+        void destroyDoNotDeallocate() noexcept {
+            mData->~value_type();
+        }
+
+        value_type const* operator->() const noexcept {
+            return mData;
+        }
+
+        value_type* operator->() noexcept {
+            return mData;
+        }
+
+        const value_type& operator*() const {
+            return *mData;
+        }
+
+        value_type& operator*() {
+            return *mData;
+        }
+
+        template <typename VT = value_type>
+        ROBIN_HOOD(NODISCARD)
+        typename std::enable_if<is_map, typename VT::first_type&>::type getFirst() noexcept {
+            return mData->first;
+        }
+        template <typename VT = value_type>
+        ROBIN_HOOD(NODISCARD)
+        typename std::enable_if<is_set, VT&>::type getFirst() noexcept {
+            return *mData;
+        }
+
+        template <typename VT = value_type>
+        ROBIN_HOOD(NODISCARD)
+        typename std::enable_if<is_map, typename VT::first_type const&>::type
+            getFirst() const noexcept {
+            return mData->first;
+        }
+        template <typename VT = value_type>
+        ROBIN_HOOD(NODISCARD)
+        typename std::enable_if<is_set, VT const&>::type getFirst() const noexcept {
+            return *mData;
+        }
+
+        template <typename MT = mapped_type>
+        ROBIN_HOOD(NODISCARD)
+        typename std::enable_if<is_map, MT&>::type getSecond() noexcept {
+            return mData->second;
+        }
+
+        template <typename MT = mapped_type>
+        ROBIN_HOOD(NODISCARD)
+        typename std::enable_if<is_map, MT const&>::type getSecond() const noexcept {
+            return mData->second;
+        }
+
+        void swap(DataNode<M, false>& o) noexcept {
+            using std::swap;
+            swap(mData, o.mData);
+        }
+
+    private:
+        value_type* mData;
+    };
+
+    using Node = DataNode<Self, IsFlat>;
+
+    // helpers for insertKeyPrepareEmptySpot: extract first entry (only const required)
+    ROBIN_HOOD(NODISCARD) key_type const& getFirstConst(Node const& n) const noexcept {
+        return n.getFirst();
+    }
+
+    // in case we have void mapped_type, we are not using a pair, thus we just route k through.
+    // No need to disable this because it's just not used if not applicable.
+    ROBIN_HOOD(NODISCARD) key_type const& getFirstConst(key_type const& k) const noexcept {
+        return k;
+    }
+
+    // in case we have non-void mapped_type, we have a standard robin_hood::pair
+    template <typename Q = mapped_type>
+    ROBIN_HOOD(NODISCARD)
+    typename std::enable_if<!std::is_void<Q>::value, key_type const&>::type
+        getFirstConst(value_type const& vt) const noexcept {
+        return vt.first;
+    }
+
+    // Cloner //////////////////////////////////////////////////////////
+
+    template <typename M, bool UseMemcpy>
+    struct Cloner;
+
+    // fast path: Just copy data, without allocating anything.
+    template <typename M>
+    struct Cloner<M, true> {
+        void operator()(M const& source, M& target) const {
+            auto const* const src = reinterpret_cast<char const*>(source.mKeyVals);
+            auto* tgt = reinterpret_cast<char*>(target.mKeyVals);
+            auto const numElementsWithBuffer = target.calcNumElementsWithBuffer(target.mMask + 1);
+            std::copy(src, src + target.calcNumBytesTotal(numElementsWithBuffer), tgt);
+        }
+    };
+
+    template <typename M>
+    struct Cloner<M, false> {
+        void operator()(M const& s, M& t) const {
+            auto const numElementsWithBuffer = t.calcNumElementsWithBuffer(t.mMask + 1);
+            std::copy(s.mInfo, s.mInfo + t.calcNumBytesInfo(numElementsWithBuffer), t.mInfo);
+
+            for (size_t i = 0; i < numElementsWithBuffer; ++i) {
+                if (t.mInfo[i]) {
+                    ::new (static_cast<void*>(t.mKeyVals + i)) Node(t, *s.mKeyVals[i]);
+                }
+            }
+        }
+    };
+
+    // Destroyer ///////////////////////////////////////////////////////
+
+    template <typename M, bool IsFlatAndTrivial>
+    struct Destroyer {};
+
+    template <typename M>
+    struct Destroyer<M, true> {
+        void nodes(M& m) const noexcept {
+            m.mNumElements = 0;
+        }
+
+        void nodesDoNotDeallocate(M& m) const noexcept {
+            m.mNumElements = 0;
+        }
+    };
+
+    template <typename M>
+    struct Destroyer<M, false> {
+        void nodes(M& m) const noexcept {
+            m.mNumElements = 0;
+            // clear also resets mInfo to 0, that's sometimes not necessary.
+            auto const numElementsWithBuffer = m.calcNumElementsWithBuffer(m.mMask + 1);
+
+            for (size_t idx = 0; idx < numElementsWithBuffer; ++idx) {
+                if (0 != m.mInfo[idx]) {
+                    Node& n = m.mKeyVals[idx];
+                    n.destroy(m);
+                    n.~Node();
+                }
+            }
+        }
+
+        void nodesDoNotDeallocate(M& m) const noexcept {
+            m.mNumElements = 0;
+            // clear also resets mInfo to 0, that's sometimes not necessary.
+            auto const numElementsWithBuffer = m.calcNumElementsWithBuffer(m.mMask + 1);
+            for (size_t idx = 0; idx < numElementsWithBuffer; ++idx) {
+                if (0 != m.mInfo[idx]) {
+                    Node& n = m.mKeyVals[idx];
+                    n.destroyDoNotDeallocate();
+                    n.~Node();
+                }
+            }
+        }
+    };
+
+    // Iter ////////////////////////////////////////////////////////////
+
+    struct fast_forward_tag {};
+
+    // generic iterator for both const_iterator and iterator.
+    template <bool IsConst>
+    // NOLINTNEXTLINE(hicpp-special-member-functions,cppcoreguidelines-special-member-functions)
+    class Iter {
+    private:
+        using NodePtr = typename std::conditional<IsConst, Node const*, Node*>::type;
+
+    public:
+        using difference_type = std::ptrdiff_t;
+        using value_type = typename Self::value_type;
+        using reference = typename std::conditional<IsConst, value_type const&, value_type&>::type;
+        using pointer = typename std::conditional<IsConst, value_type const*, value_type*>::type;
+        using iterator_category = std::forward_iterator_tag;
+
+        // default constructed iterator can be compared to itself, but WON'T return true when
+        // compared to end().
+        Iter() = default;
+
+        // Rule of zero: nothing specified. The conversion constructor is only enabled for
+        // iterator to const_iterator, so it doesn't accidentally work as a copy ctor.
+
+        // Conversion constructor from iterator to const_iterator.
+        template <bool OtherIsConst,
+                  typename = typename std::enable_if<IsConst && !OtherIsConst>::type>
+        // NOLINTNEXTLINE(hicpp-explicit-conversions)
+        Iter(Iter<OtherIsConst> const& other) noexcept
+            : mKeyVals(other.mKeyVals)
+            , mInfo(other.mInfo) {}
+
+        Iter(NodePtr valPtr, uint8_t const* infoPtr) noexcept
+            : mKeyVals(valPtr)
+            , mInfo(infoPtr) {}
+
+        Iter(NodePtr valPtr, uint8_t const* infoPtr,
+             fast_forward_tag ROBIN_HOOD_UNUSED(tag) /*unused*/) noexcept
+            : mKeyVals(valPtr)
+            , mInfo(infoPtr) {
+            fastForward();
+        }
+
+        template <bool OtherIsConst,
+                  typename = typename std::enable_if<IsConst && !OtherIsConst>::type>
+        Iter& operator=(Iter<OtherIsConst> const& other) noexcept {
+            mKeyVals = other.mKeyVals;
+            mInfo = other.mInfo;
+            return *this;
+        }
+
+        // prefix increment. Undefined behavior if we are at end()!
+        Iter& operator++() noexcept {
+            mInfo++;
+            mKeyVals++;
+            fastForward();
+            return *this;
+        }
+
+        Iter operator++(int) noexcept {
+            Iter tmp = *this;
+            ++(*this);
+            return tmp;
+        }
+
+        reference operator*() const {
+            return **mKeyVals;
+        }
+
+        pointer operator->() const {
+            return &**mKeyVals;
+        }
+
+        template <bool O>
+        bool operator==(Iter<O> const& o) const noexcept {
+            return mKeyVals == o.mKeyVals;
+        }
+
+        template <bool O>
+        bool operator!=(Iter<O> const& o) const noexcept {
+            return mKeyVals != o.mKeyVals;
+        }
+
+    private:
+        // fast forward to the next non-free info byte
+        // I've tried a few variants that don't depend on intrinsics, but unfortunately they are
+        // quite a bit slower than this one. So I've reverted that change again. See map_benchmark.
+        void fastForward() noexcept {
+            size_t n = 0;
+            while (0U == (n = detail::unaligned_load<size_t>(mInfo))) {
+                mInfo += sizeof(size_t);
+                mKeyVals += sizeof(size_t);
+            }
+#if defined(ROBIN_HOOD_DISABLE_INTRINSICS)
+            // we know for certain that within the next 8 bytes we'll find a non-zero one.
+            if (ROBIN_HOOD_UNLIKELY(0U == detail::unaligned_load<uint32_t>(mInfo))) {
+                mInfo += 4;
+                mKeyVals += 4;
+            }
+            if (ROBIN_HOOD_UNLIKELY(0U == detail::unaligned_load<uint16_t>(mInfo))) {
+                mInfo += 2;
+                mKeyVals += 2;
+            }
+            if (ROBIN_HOOD_UNLIKELY(0U == *mInfo)) {
+                mInfo += 1;
+                mKeyVals += 1;
+            }
+#else
+#    if ROBIN_HOOD(LITTLE_ENDIAN)
+            auto inc = ROBIN_HOOD_COUNT_TRAILING_ZEROES(n) / 8;
+#    else
+            auto inc = ROBIN_HOOD_COUNT_LEADING_ZEROES(n) / 8;
+#    endif
+            mInfo += inc;
+            mKeyVals += inc;
+#endif
+        }
+
+        friend class Table<IsFlat, MaxLoadFactor100, key_type, mapped_type, hasher, key_equal>;
+        NodePtr mKeyVals{nullptr};
+        uint8_t const* mInfo{nullptr};
+    };
+
+    ////////////////////////////////////////////////////////////////////
+
+    // highly performance relevant code.
+    // Lower bits are used for indexing into the array (2^n size)
+    // The upper 1-5 bits need to be a reasonable good hash, to save comparisons.
+    template <typename HashKey>
+    void keyToIdx(HashKey&& key, size_t* idx, InfoType* info) const {
+        // In addition to whatever hash is used, add another mul & shift so we get better hashing.
+        // This serves as a bad hash prevention, if the given data is
+        // badly mixed.
+        auto h = static_cast<uint64_t>(WHash::operator()(key));
+
+        h *= mHashMultiplier;
+        h ^= h >> 33U;
+
+        // the lower InitialInfoNumBits are reserved for info.
+        *info = mInfoInc + static_cast<InfoType>((h & InfoMask) >> mInfoHashShift);
+        *idx = (static_cast<size_t>(h) >> InitialInfoNumBits) & mMask;
+    }
+
+    // forwards the index by one, wrapping around at the end
+    void next(InfoType* info, size_t* idx) const noexcept {
+        *idx = *idx + 1;
+        *info += mInfoInc;
+    }
+
+    void nextWhileLess(InfoType* info, size_t* idx) const noexcept {
+        // unrolling this by hand did not bring any speedups.
+        while (*info < mInfo[*idx]) {
+            next(info, idx);
+        }
+    }
+
+    // Shift everything up by one element. Tries to move stuff around.
+    void
+    shiftUp(size_t startIdx,
+            size_t const insertion_idx) noexcept(std::is_nothrow_move_assignable<Node>::value) {
+        auto idx = startIdx;
+        ::new (static_cast<void*>(mKeyVals + idx)) Node(std::move(mKeyVals[idx - 1]));
+        while (--idx != insertion_idx) {
+            mKeyVals[idx] = std::move(mKeyVals[idx - 1]);
+        }
+
+        idx = startIdx;
+        while (idx != insertion_idx) {
+            ROBIN_HOOD_COUNT(shiftUp)
+            mInfo[idx] = static_cast<uint8_t>(mInfo[idx - 1] + mInfoInc);
+            if (ROBIN_HOOD_UNLIKELY(mInfo[idx] + mInfoInc > 0xFF)) {
+                mMaxNumElementsAllowed = 0;
+            }
+            --idx;
+        }
+    }
+
+    void shiftDown(size_t idx) noexcept(std::is_nothrow_move_assignable<Node>::value) {
+        // until we find one that is either empty or has zero offset.
+        // TODO(martinus) we don't need to move everything, just the last one for the same
+        // bucket.
+        mKeyVals[idx].destroy(*this);
+
+        // until we find one that is either empty or has zero offset.
+        while (mInfo[idx + 1] >= 2 * mInfoInc) {
+            ROBIN_HOOD_COUNT(shiftDown)
+            mInfo[idx] = static_cast<uint8_t>(mInfo[idx + 1] - mInfoInc);
+            mKeyVals[idx] = std::move(mKeyVals[idx + 1]);
+            ++idx;
+        }
+
+        mInfo[idx] = 0;
+        // don't destroy, we've moved it
+        // mKeyVals[idx].destroy(*this);
+        mKeyVals[idx].~Node();
+    }
+
+    // copy of find(), except that it returns iterator instead of const_iterator.
+    template <typename Other>
+    ROBIN_HOOD(NODISCARD)
+    size_t findIdx(Other const& key) const {
+        size_t idx{};
+        InfoType info{};
+        keyToIdx(key, &idx, &info);
+
+        do {
+            // unrolling this twice gives a bit of a speedup. More unrolling did not help.
+            if (info == mInfo[idx] &&
+                ROBIN_HOOD_LIKELY(WKeyEqual::operator()(key, mKeyVals[idx].getFirst()))) {
+                return idx;
+            }
+            next(&info, &idx);
+            if (info == mInfo[idx] &&
+                ROBIN_HOOD_LIKELY(WKeyEqual::operator()(key, mKeyVals[idx].getFirst()))) {
+                return idx;
+            }
+            next(&info, &idx);
+        } while (info <= mInfo[idx]);
+
+        // nothing found!
+        return mMask == 0 ? 0
+                          : static_cast<size_t>(std::distance(
+                                mKeyVals, reinterpret_cast_no_cast_align_warning<Node*>(mInfo)));
+    }
+
+    void cloneData(const Table& o) {
+        Cloner<Table, IsFlat && ROBIN_HOOD_IS_TRIVIALLY_COPYABLE(Node)>()(o, *this);
+    }
+
+    // inserts a keyval that is guaranteed to be new, e.g. when the hashmap is resized.
+    // @return True on success, false if something went wrong
+    void insert_move(Node&& keyval) {
+        // we don't retry, fail if overflowing
+        // don't need to check max num elements
+        if (0 == mMaxNumElementsAllowed && !try_increase_info()) {
+            throwOverflowError();
+        }
+
+        size_t idx{};
+        InfoType info{};
+        keyToIdx(keyval.getFirst(), &idx, &info);
+
+        // skip forward. Use <= because we are certain that the element is not there.
+        while (info <= mInfo[idx]) {
+            idx = idx + 1;
+            info += mInfoInc;
+        }
+
+        // key not found, so we are now exactly where we want to insert it.
+        auto const insertion_idx = idx;
+        auto const insertion_info = static_cast<uint8_t>(info);
+        if (ROBIN_HOOD_UNLIKELY(insertion_info + mInfoInc > 0xFF)) {
+            mMaxNumElementsAllowed = 0;
+        }
+
+        // find an empty spot
+        while (0 != mInfo[idx]) {
+            next(&info, &idx);
+        }
+
+        auto& l = mKeyVals[insertion_idx];
+        if (idx == insertion_idx) {
+            ::new (static_cast<void*>(&l)) Node(std::move(keyval));
+        } else {
+            shiftUp(idx, insertion_idx);
+            l = std::move(keyval);
+        }
+
+        // put at empty spot
+        mInfo[insertion_idx] = insertion_info;
+
+        ++mNumElements;
+    }
+
+public:
+    using iterator = Iter<false>;
+    using const_iterator = Iter<true>;
+
+    Table() noexcept(noexcept(Hash()) && noexcept(KeyEqual()))
+        : WHash()
+        , WKeyEqual() {
+        ROBIN_HOOD_TRACE(this)
+    }
+
+    // Creates an empty hash map. Nothing is allocated yet, this happens at the first insert.
+    // This tremendously speeds up ctor & dtor of a map that never receives an element. The
+    // penalty is payed at the first insert, and not before. Lookup of this empty map works
+    // because everybody points to DummyInfoByte::b. parameter bucket_count is dictated by the
+    // standard, but we can ignore it.
+    explicit Table(
+        size_t ROBIN_HOOD_UNUSED(bucket_count) /*unused*/, const Hash& h = Hash{},
+        const KeyEqual& equal = KeyEqual{}) noexcept(noexcept(Hash(h)) && noexcept(KeyEqual(equal)))
+        : WHash(h)
+        , WKeyEqual(equal) {
+        ROBIN_HOOD_TRACE(this)
+    }
+
+    template <typename Iter>
+    Table(Iter first, Iter last, size_t ROBIN_HOOD_UNUSED(bucket_count) /*unused*/ = 0,
+          const Hash& h = Hash{}, const KeyEqual& equal = KeyEqual{})
+        : WHash(h)
+        , WKeyEqual(equal) {
+        ROBIN_HOOD_TRACE(this)
+        insert(first, last);
+    }
+
+    Table(std::initializer_list<value_type> initlist,
+          size_t ROBIN_HOOD_UNUSED(bucket_count) /*unused*/ = 0, const Hash& h = Hash{},
+          const KeyEqual& equal = KeyEqual{})
+        : WHash(h)
+        , WKeyEqual(equal) {
+        ROBIN_HOOD_TRACE(this)
+        insert(initlist.begin(), initlist.end());
+    }
+
+    Table(Table&& o) noexcept
+        : WHash(std::move(static_cast<WHash&>(o)))
+        , WKeyEqual(std::move(static_cast<WKeyEqual&>(o)))
+        , DataPool(std::move(static_cast<DataPool&>(o))) {
+        ROBIN_HOOD_TRACE(this)
+        if (o.mMask) {
+            mHashMultiplier = std::move(o.mHashMultiplier);
+            mKeyVals = std::move(o.mKeyVals);
+            mInfo = std::move(o.mInfo);
+            mNumElements = std::move(o.mNumElements);
+            mMask = std::move(o.mMask);
+            mMaxNumElementsAllowed = std::move(o.mMaxNumElementsAllowed);
+            mInfoInc = std::move(o.mInfoInc);
+            mInfoHashShift = std::move(o.mInfoHashShift);
+            // set other's mask to 0 so its destructor won't do anything
+            o.init();
+        }
+    }
+
+    Table& operator=(Table&& o) noexcept {
+        ROBIN_HOOD_TRACE(this)
+        if (&o != this) {
+            if (o.mMask) {
+                // only move stuff if the other map actually has some data
+                destroy();
+                mHashMultiplier = std::move(o.mHashMultiplier);
+                mKeyVals = std::move(o.mKeyVals);
+                mInfo = std::move(o.mInfo);
+                mNumElements = std::move(o.mNumElements);
+                mMask = std::move(o.mMask);
+                mMaxNumElementsAllowed = std::move(o.mMaxNumElementsAllowed);
+                mInfoInc = std::move(o.mInfoInc);
+                mInfoHashShift = std::move(o.mInfoHashShift);
+                WHash::operator=(std::move(static_cast<WHash&>(o)));
+                WKeyEqual::operator=(std::move(static_cast<WKeyEqual&>(o)));
+                DataPool::operator=(std::move(static_cast<DataPool&>(o)));
+
+                o.init();
+
+            } else {
+                // nothing in the other map => just clear us.
+                clear();
+            }
+        }
+        return *this;
+    }
+
+    Table(const Table& o)
+        : WHash(static_cast<const WHash&>(o))
+        , WKeyEqual(static_cast<const WKeyEqual&>(o))
+        , DataPool(static_cast<const DataPool&>(o)) {
+        ROBIN_HOOD_TRACE(this)
+        if (!o.empty()) {
+            // not empty: create an exact copy. it is also possible to just iterate through all
+            // elements and insert them, but copying is probably faster.
+
+            auto const numElementsWithBuffer = calcNumElementsWithBuffer(o.mMask + 1);
+            auto const numBytesTotal = calcNumBytesTotal(numElementsWithBuffer);
+
+            ROBIN_HOOD_LOG("std::malloc " << numBytesTotal << " = calcNumBytesTotal("
+                                          << numElementsWithBuffer << ")")
+            mHashMultiplier = o.mHashMultiplier;
+            mKeyVals = static_cast<Node*>(
+                detail::assertNotNull<std::bad_alloc>(std::malloc(numBytesTotal)));
+            // no need for calloc because clonData does memcpy
+            mInfo = reinterpret_cast<uint8_t*>(mKeyVals + numElementsWithBuffer);
+            mNumElements = o.mNumElements;
+            mMask = o.mMask;
+            mMaxNumElementsAllowed = o.mMaxNumElementsAllowed;
+            mInfoInc = o.mInfoInc;
+            mInfoHashShift = o.mInfoHashShift;
+            cloneData(o);
+        }
+    }
+
+    // Creates a copy of the given map. Copy constructor of each entry is used.
+    // Not sure why clang-tidy thinks this doesn't handle self assignment, it does
+    // NOLINTNEXTLINE(bugprone-unhandled-self-assignment,cert-oop54-cpp)
+    Table& operator=(Table const& o) {
+        ROBIN_HOOD_TRACE(this)
+        if (&o == this) {
+            // prevent assigning of itself
+            return *this;
+        }
+
+        // we keep using the old allocator and not assign the new one, because we want to keep
+        // the memory available. when it is the same size.
+        if (o.empty()) {
+            if (0 == mMask) {
+                // nothing to do, we are empty too
+                return *this;
+            }
+
+            // not empty: destroy what we have there
+            // clear also resets mInfo to 0, that's sometimes not necessary.
+            destroy();
+            init();
+            WHash::operator=(static_cast<const WHash&>(o));
+            WKeyEqual::operator=(static_cast<const WKeyEqual&>(o));
+            DataPool::operator=(static_cast<DataPool const&>(o));
+
+            return *this;
+        }
+
+        // clean up old stuff
+        Destroyer<Self, IsFlat && std::is_trivially_destructible<Node>::value>{}.nodes(*this);
+
+        if (mMask != o.mMask) {
+            // no luck: we don't have the same array size allocated, so we need to realloc.
+            if (0 != mMask) {
+                // only deallocate if we actually have data!
+                ROBIN_HOOD_LOG("std::free")
+                std::free(mKeyVals);
+            }
+
+            auto const numElementsWithBuffer = calcNumElementsWithBuffer(o.mMask + 1);
+            auto const numBytesTotal = calcNumBytesTotal(numElementsWithBuffer);
+            ROBIN_HOOD_LOG("std::malloc " << numBytesTotal << " = calcNumBytesTotal("
+                                          << numElementsWithBuffer << ")")
+            mKeyVals = static_cast<Node*>(
+                detail::assertNotNull<std::bad_alloc>(std::malloc(numBytesTotal)));
+
+            // no need for calloc here because cloneData performs a memcpy.
+            mInfo = reinterpret_cast<uint8_t*>(mKeyVals + numElementsWithBuffer);
+            // sentinel is set in cloneData
+        }
+        WHash::operator=(static_cast<const WHash&>(o));
+        WKeyEqual::operator=(static_cast<const WKeyEqual&>(o));
+        DataPool::operator=(static_cast<DataPool const&>(o));
+        mHashMultiplier = o.mHashMultiplier;
+        mNumElements = o.mNumElements;
+        mMask = o.mMask;
+        mMaxNumElementsAllowed = o.mMaxNumElementsAllowed;
+        mInfoInc = o.mInfoInc;
+        mInfoHashShift = o.mInfoHashShift;
+        cloneData(o);
+
+        return *this;
+    }
+
+    // Swaps everything between the two maps.
+    void swap(Table& o) {
+        ROBIN_HOOD_TRACE(this)
+        using std::swap;
+        swap(o, *this);
+    }
+
+    // Clears all data, without resizing.
+    void clear() {
+        ROBIN_HOOD_TRACE(this)
+        if (empty()) {
+            // don't do anything! also important because we don't want to write to
+            // DummyInfoByte::b, even though we would just write 0 to it.
+            return;
+        }
+
+        Destroyer<Self, IsFlat && std::is_trivially_destructible<Node>::value>{}.nodes(*this);
+
+        auto const numElementsWithBuffer = calcNumElementsWithBuffer(mMask + 1);
+        // clear everything, then set the sentinel again
+        uint8_t const z = 0;
+        std::fill(mInfo, mInfo + calcNumBytesInfo(numElementsWithBuffer), z);
+        mInfo[numElementsWithBuffer] = 1;
+
+        mInfoInc = InitialInfoInc;
+        mInfoHashShift = InitialInfoHashShift;
+    }
+
+    // Destroys the map and all it's contents.
+    ~Table() {
+        ROBIN_HOOD_TRACE(this)
+        destroy();
+    }
+
+    // Checks if both tables contain the same entries. Order is irrelevant.
+    bool operator==(const Table& other) const {
+        ROBIN_HOOD_TRACE(this)
+        if (other.size() != size()) {
+            return false;
+        }
+        for (auto const& otherEntry : other) {
+            if (!has(otherEntry)) {
+                return false;
+            }
+        }
+
+        return true;
+    }
+
+    bool operator!=(const Table& other) const {
+        ROBIN_HOOD_TRACE(this)
+        return !operator==(other);
+    }
+
+    template <typename Q = mapped_type>
+    typename std::enable_if<!std::is_void<Q>::value, Q&>::type operator[](const key_type& key) {
+        ROBIN_HOOD_TRACE(this)
+        auto idxAndState = insertKeyPrepareEmptySpot(key);
+        switch (idxAndState.second) {
+        case InsertionState::key_found:
+            break;
+
+        case InsertionState::new_node:
+            ::new (static_cast<void*>(&mKeyVals[idxAndState.first]))
+                Node(*this, std::piecewise_construct, std::forward_as_tuple(key),
+                     std::forward_as_tuple());
+            break;
+
+        case InsertionState::overwrite_node:
+            mKeyVals[idxAndState.first] = Node(*this, std::piecewise_construct,
+                                               std::forward_as_tuple(key), std::forward_as_tuple());
+            break;
+
+        case InsertionState::overflow_error:
+            throwOverflowError();
+        }
+
+        return mKeyVals[idxAndState.first].getSecond();
+    }
+
+    template <typename Q = mapped_type>
+    typename std::enable_if<!std::is_void<Q>::value, Q&>::type operator[](key_type&& key) {
+        ROBIN_HOOD_TRACE(this)
+        auto idxAndState = insertKeyPrepareEmptySpot(key);
+        switch (idxAndState.second) {
+        case InsertionState::key_found:
+            break;
+
+        case InsertionState::new_node:
+            ::new (static_cast<void*>(&mKeyVals[idxAndState.first]))
+                Node(*this, std::piecewise_construct, std::forward_as_tuple(std::move(key)),
+                     std::forward_as_tuple());
+            break;
+
+        case InsertionState::overwrite_node:
+            mKeyVals[idxAndState.first] =
+                Node(*this, std::piecewise_construct, std::forward_as_tuple(std::move(key)),
+                     std::forward_as_tuple());
+            break;
+
+        case InsertionState::overflow_error:
+            throwOverflowError();
+        }
+
+        return mKeyVals[idxAndState.first].getSecond();
+    }
+
+    template <typename Iter>
+    void insert(Iter first, Iter last) {
+        for (; first != last; ++first) {
+            // value_type ctor needed because this might be called with std::pair's
+            insert(value_type(*first));
+        }
+    }
+
+    void insert(std::initializer_list<value_type> ilist) {
+        for (auto&& vt : ilist) {
+            insert(std::move(vt));
+        }
+    }
+
+    template <typename... Args>
+    std::pair<iterator, bool> emplace(Args&&... args) {
+        ROBIN_HOOD_TRACE(this)
+        Node n{*this, std::forward<Args>(args)...};
+        auto idxAndState = insertKeyPrepareEmptySpot(getFirstConst(n));
+        switch (idxAndState.second) {
+        case InsertionState::key_found:
+            n.destroy(*this);
+            break;
+
+        case InsertionState::new_node:
+            ::new (static_cast<void*>(&mKeyVals[idxAndState.first])) Node(*this, std::move(n));
+            break;
+
+        case InsertionState::overwrite_node:
+            mKeyVals[idxAndState.first] = std::move(n);
+            break;
+
+        case InsertionState::overflow_error:
+            n.destroy(*this);
+            throwOverflowError();
+            break;
+        }
+
+        return std::make_pair(iterator(mKeyVals + idxAndState.first, mInfo + idxAndState.first),
+                              InsertionState::key_found != idxAndState.second);
+    }
+
+    template <typename... Args>
+    iterator emplace_hint(const_iterator position, Args&&... args) {
+        (void)position;
+        return emplace(std::forward<Args>(args)...).first;
+    }
+
+    template <typename... Args>
+    std::pair<iterator, bool> try_emplace(const key_type& key, Args&&... args) {
+        return try_emplace_impl(key, std::forward<Args>(args)...);
+    }
+
+    template <typename... Args>
+    std::pair<iterator, bool> try_emplace(key_type&& key, Args&&... args) {
+        return try_emplace_impl(std::move(key), std::forward<Args>(args)...);
+    }
+
+    template <typename... Args>
+    iterator try_emplace(const_iterator hint, const key_type& key, Args&&... args) {
+        (void)hint;
+        return try_emplace_impl(key, std::forward<Args>(args)...).first;
+    }
+
+    template <typename... Args>
+    iterator try_emplace(const_iterator hint, key_type&& key, Args&&... args) {
+        (void)hint;
+        return try_emplace_impl(std::move(key), std::forward<Args>(args)...).first;
+    }
+
+    template <typename Mapped>
+    std::pair<iterator, bool> insert_or_assign(const key_type& key, Mapped&& obj) {
+        return insertOrAssignImpl(key, std::forward<Mapped>(obj));
+    }
+
+    template <typename Mapped>
+    std::pair<iterator, bool> insert_or_assign(key_type&& key, Mapped&& obj) {
+        return insertOrAssignImpl(std::move(key), std::forward<Mapped>(obj));
+    }
+
+    template <typename Mapped>
+    iterator insert_or_assign(const_iterator hint, const key_type& key, Mapped&& obj) {
+        (void)hint;
+        return insertOrAssignImpl(key, std::forward<Mapped>(obj)).first;
+    }
+
+    template <typename Mapped>
+    iterator insert_or_assign(const_iterator hint, key_type&& key, Mapped&& obj) {
+        (void)hint;
+        return insertOrAssignImpl(std::move(key), std::forward<Mapped>(obj)).first;
+    }
+
+    std::pair<iterator, bool> insert(const value_type& keyval) {
+        ROBIN_HOOD_TRACE(this)
+        return emplace(keyval);
+    }
+
+    iterator insert(const_iterator hint, const value_type& keyval) {
+        (void)hint;
+        return emplace(keyval).first;
+    }
+
+    std::pair<iterator, bool> insert(value_type&& keyval) {
+        return emplace(std::move(keyval));
+    }
+
+    iterator insert(const_iterator hint, value_type&& keyval) {
+        (void)hint;
+        return emplace(std::move(keyval)).first;
+    }
+
+    // Returns 1 if key is found, 0 otherwise.
+    size_t count(const key_type& key) const { // NOLINT(modernize-use-nodiscard)
+        ROBIN_HOOD_TRACE(this)
+        auto kv = mKeyVals + findIdx(key);
+        if (kv != reinterpret_cast_no_cast_align_warning<Node*>(mInfo)) {
+            return 1;
+        }
+        return 0;
+    }
+
+    template <typename OtherKey, typename Self_ = Self>
+    // NOLINTNEXTLINE(modernize-use-nodiscard)
+    typename std::enable_if<Self_::is_transparent, size_t>::type count(const OtherKey& key) const {
+        ROBIN_HOOD_TRACE(this)
+        auto kv = mKeyVals + findIdx(key);
+        if (kv != reinterpret_cast_no_cast_align_warning<Node*>(mInfo)) {
+            return 1;
+        }
+        return 0;
+    }
+
+    bool contains(const key_type& key) const { // NOLINT(modernize-use-nodiscard)
+        return 1U == count(key);
+    }
+
+    template <typename OtherKey, typename Self_ = Self>
+    // NOLINTNEXTLINE(modernize-use-nodiscard)
+    typename std::enable_if<Self_::is_transparent, bool>::type contains(const OtherKey& key) const {
+        return 1U == count(key);
+    }
+
+    // Returns a reference to the value found for key.
+    // Throws std::out_of_range if element cannot be found
+    template <typename Q = mapped_type>
+    // NOLINTNEXTLINE(modernize-use-nodiscard)
+    typename std::enable_if<!std::is_void<Q>::value, Q&>::type at(key_type const& key) {
+        ROBIN_HOOD_TRACE(this)
+        auto kv = mKeyVals + findIdx(key);
+        if (kv == reinterpret_cast_no_cast_align_warning<Node*>(mInfo)) {
+            doThrow<std::out_of_range>("key not found");
+        }
+        return kv->getSecond();
+    }
+
+    // Returns a reference to the value found for key.
+    // Throws std::out_of_range if element cannot be found
+    template <typename Q = mapped_type>
+    // NOLINTNEXTLINE(modernize-use-nodiscard)
+    typename std::enable_if<!std::is_void<Q>::value, Q const&>::type at(key_type const& key) const {
+        ROBIN_HOOD_TRACE(this)
+        auto kv = mKeyVals + findIdx(key);
+        if (kv == reinterpret_cast_no_cast_align_warning<Node*>(mInfo)) {
+            doThrow<std::out_of_range>("key not found");
+        }
+        return kv->getSecond();
+    }
+
+    const_iterator find(const key_type& key) const { // NOLINT(modernize-use-nodiscard)
+        ROBIN_HOOD_TRACE(this)
+        const size_t idx = findIdx(key);
+        return const_iterator{mKeyVals + idx, mInfo + idx};
+    }
+
+    template <typename OtherKey>
+    const_iterator find(const OtherKey& key, is_transparent_tag /*unused*/) const {
+        ROBIN_HOOD_TRACE(this)
+        const size_t idx = findIdx(key);
+        return const_iterator{mKeyVals + idx, mInfo + idx};
+    }
+
+    template <typename OtherKey, typename Self_ = Self>
+    typename std::enable_if<Self_::is_transparent, // NOLINT(modernize-use-nodiscard)
+                            const_iterator>::type  // NOLINT(modernize-use-nodiscard)
+    find(const OtherKey& key) const {              // NOLINT(modernize-use-nodiscard)
+        ROBIN_HOOD_TRACE(this)
+        const size_t idx = findIdx(key);
+        return const_iterator{mKeyVals + idx, mInfo + idx};
+    }
+
+    iterator find(const key_type& key) {
+        ROBIN_HOOD_TRACE(this)
+        const size_t idx = findIdx(key);
+        return iterator{mKeyVals + idx, mInfo + idx};
+    }
+
+    template <typename OtherKey>
+    iterator find(const OtherKey& key, is_transparent_tag /*unused*/) {
+        ROBIN_HOOD_TRACE(this)
+        const size_t idx = findIdx(key);
+        return iterator{mKeyVals + idx, mInfo + idx};
+    }
+
+    template <typename OtherKey, typename Self_ = Self>
+    typename std::enable_if<Self_::is_transparent, iterator>::type find(const OtherKey& key) {
+        ROBIN_HOOD_TRACE(this)
+        const size_t idx = findIdx(key);
+        return iterator{mKeyVals + idx, mInfo + idx};
+    }
+
+    iterator begin() {
+        ROBIN_HOOD_TRACE(this)
+        if (empty()) {
+            return end();
+        }
+        return iterator(mKeyVals, mInfo, fast_forward_tag{});
+    }
+    const_iterator begin() const { // NOLINT(modernize-use-nodiscard)
+        ROBIN_HOOD_TRACE(this)
+        return cbegin();
+    }
+    const_iterator cbegin() const { // NOLINT(modernize-use-nodiscard)
+        ROBIN_HOOD_TRACE(this)
+        if (empty()) {
+            return cend();
+        }
+        return const_iterator(mKeyVals, mInfo, fast_forward_tag{});
+    }
+
+    iterator end() {
+        ROBIN_HOOD_TRACE(this)
+        // no need to supply valid info pointer: end() must not be dereferenced, and only node
+        // pointer is compared.
+        return iterator{reinterpret_cast_no_cast_align_warning<Node*>(mInfo), nullptr};
+    }
+    const_iterator end() const { // NOLINT(modernize-use-nodiscard)
+        ROBIN_HOOD_TRACE(this)
+        return cend();
+    }
+    const_iterator cend() const { // NOLINT(modernize-use-nodiscard)
+        ROBIN_HOOD_TRACE(this)
+        return const_iterator{reinterpret_cast_no_cast_align_warning<Node*>(mInfo), nullptr};
+    }
+
+    iterator erase(const_iterator pos) {
+        ROBIN_HOOD_TRACE(this)
+        // its safe to perform const cast here
+        // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)
+        return erase(iterator{const_cast<Node*>(pos.mKeyVals), const_cast<uint8_t*>(pos.mInfo)});
+    }
+
+    // Erases element at pos, returns iterator to the next element.
+    iterator erase(iterator pos) {
+        ROBIN_HOOD_TRACE(this)
+        // we assume that pos always points to a valid entry, and not end().
+        auto const idx = static_cast<size_t>(pos.mKeyVals - mKeyVals);
+
+        shiftDown(idx);
+        --mNumElements;
+
+        if (*pos.mInfo) {
+            // we've backward shifted, return this again
+            return pos;
+        }
+
+        // no backward shift, return next element
+        return ++pos;
+    }
+
+    size_t erase(const key_type& key) {
+        ROBIN_HOOD_TRACE(this)
+        size_t idx{};
+        InfoType info{};
+        keyToIdx(key, &idx, &info);
+
+        // check while info matches with the source idx
+        do {
+            if (info == mInfo[idx] && WKeyEqual::operator()(key, mKeyVals[idx].getFirst())) {
+                shiftDown(idx);
+                --mNumElements;
+                return 1;
+            }
+            next(&info, &idx);
+        } while (info <= mInfo[idx]);
+
+        // nothing found to delete
+        return 0;
+    }
+
+    // reserves space for the specified number of elements. Makes sure the old data fits.
+    // exactly the same as reserve(c).
+    void rehash(size_t c) {
+        // forces a reserve
+        reserve(c, true);
+    }
+
+    // reserves space for the specified number of elements. Makes sure the old data fits.
+    // Exactly the same as rehash(c). Use rehash(0) to shrink to fit.
+    void reserve(size_t c) {
+        // reserve, but don't force rehash
+        reserve(c, false);
+    }
+
+    // If possible reallocates the map to a smaller one. This frees the underlying table.
+    // Does not do anything if load_factor is too large for decreasing the table's size.
+    void compact() {
+        ROBIN_HOOD_TRACE(this)
+        auto newSize = InitialNumElements;
+        while (calcMaxNumElementsAllowed(newSize) < mNumElements && newSize != 0) {
+            newSize *= 2;
+        }
+        if (ROBIN_HOOD_UNLIKELY(newSize == 0)) {
+            throwOverflowError();
+        }
+
+        ROBIN_HOOD_LOG("newSize > mMask + 1: " << newSize << " > " << mMask << " + 1")
+
+        // only actually do anything when the new size is bigger than the old one. This prevents to
+        // continuously allocate for each reserve() call.
+        if (newSize < mMask + 1) {
+            rehashPowerOfTwo(newSize, true);
+        }
+    }
+
+    size_type size() const noexcept { // NOLINT(modernize-use-nodiscard)
+        ROBIN_HOOD_TRACE(this)
+        return mNumElements;
+    }
+
+    size_type max_size() const noexcept { // NOLINT(modernize-use-nodiscard)
+        ROBIN_HOOD_TRACE(this)
+        return static_cast<size_type>(-1);
+    }
+
+    ROBIN_HOOD(NODISCARD) bool empty() const noexcept {
+        ROBIN_HOOD_TRACE(this)
+        return 0 == mNumElements;
+    }
+
+    float max_load_factor() const noexcept { // NOLINT(modernize-use-nodiscard)
+        ROBIN_HOOD_TRACE(this)
+        return MaxLoadFactor100 / 100.0F;
+    }
+
+    // Average number of elements per bucket. Since we allow only 1 per bucket
+    float load_factor() const noexcept { // NOLINT(modernize-use-nodiscard)
+        ROBIN_HOOD_TRACE(this)
+        return static_cast<float>(size()) / static_cast<float>(mMask + 1);
+    }
+
+    ROBIN_HOOD(NODISCARD) size_t mask() const noexcept {
+        ROBIN_HOOD_TRACE(this)
+        return mMask;
+    }
+
+    ROBIN_HOOD(NODISCARD) size_t calcMaxNumElementsAllowed(size_t maxElements) const noexcept {
+        if (ROBIN_HOOD_LIKELY(maxElements <= (std::numeric_limits<size_t>::max)() / 100)) {
+            return maxElements * MaxLoadFactor100 / 100;
+        }
+
+        // we might be a bit inprecise, but since maxElements is quite large that doesn't matter
+        return (maxElements / 100) * MaxLoadFactor100;
+    }
+
+    ROBIN_HOOD(NODISCARD) size_t calcNumBytesInfo(size_t numElements) const noexcept {
+        // we add a uint64_t, which houses the sentinel (first byte) and padding so we can load
+        // 64bit types.
+        return numElements + sizeof(uint64_t);
+    }
+
+    ROBIN_HOOD(NODISCARD)
+    size_t calcNumElementsWithBuffer(size_t numElements) const noexcept {
+        auto maxNumElementsAllowed = calcMaxNumElementsAllowed(numElements);
+        return numElements + (std::min)(maxNumElementsAllowed, (static_cast<size_t>(0xFF)));
+    }
+
+    // calculation only allowed for 2^n values
+    ROBIN_HOOD(NODISCARD) size_t calcNumBytesTotal(size_t numElements) const {
+#if ROBIN_HOOD(BITNESS) == 64
+        return numElements * sizeof(Node) + calcNumBytesInfo(numElements);
+#else
+        // make sure we're doing 64bit operations, so we are at least safe against 32bit overflows.
+        auto const ne = static_cast<uint64_t>(numElements);
+        auto const s = static_cast<uint64_t>(sizeof(Node));
+        auto const infos = static_cast<uint64_t>(calcNumBytesInfo(numElements));
+
+        auto const total64 = ne * s + infos;
+        auto const total = static_cast<size_t>(total64);
+
+        if (ROBIN_HOOD_UNLIKELY(static_cast<uint64_t>(total) != total64)) {
+            throwOverflowError();
+        }
+        return total;
+#endif
+    }
+
+private:
+    template <typename Q = mapped_type>
+    ROBIN_HOOD(NODISCARD)
+    typename std::enable_if<!std::is_void<Q>::value, bool>::type has(const value_type& e) const {
+        ROBIN_HOOD_TRACE(this)
+        auto it = find(e.first);
+        return it != end() && it->second == e.second;
+    }
+
+    template <typename Q = mapped_type>
+    ROBIN_HOOD(NODISCARD)
+    typename std::enable_if<std::is_void<Q>::value, bool>::type has(const value_type& e) const {
+        ROBIN_HOOD_TRACE(this)
+        return find(e) != end();
+    }
+
+    void reserve(size_t c, bool forceRehash) {
+        ROBIN_HOOD_TRACE(this)
+        auto const minElementsAllowed = (std::max)(c, mNumElements);
+        auto newSize = InitialNumElements;
+        while (calcMaxNumElementsAllowed(newSize) < minElementsAllowed && newSize != 0) {
+            newSize *= 2;
+        }
+        if (ROBIN_HOOD_UNLIKELY(newSize == 0)) {
+            throwOverflowError();
+        }
+
+        ROBIN_HOOD_LOG("newSize > mMask + 1: " << newSize << " > " << mMask << " + 1")
+
+        // only actually do anything when the new size is bigger than the old one. This prevents to
+        // continuously allocate for each reserve() call.
+        if (forceRehash || newSize > mMask + 1) {
+            rehashPowerOfTwo(newSize, false);
+        }
+    }
+
+    // reserves space for at least the specified number of elements.
+    // only works if numBuckets if power of two
+    // True on success, false otherwise
+    void rehashPowerOfTwo(size_t numBuckets, bool forceFree) {
+        ROBIN_HOOD_TRACE(this)
+
+        Node* const oldKeyVals = mKeyVals;
+        uint8_t const* const oldInfo = mInfo;
+
+        const size_t oldMaxElementsWithBuffer = calcNumElementsWithBuffer(mMask + 1);
+
+        // resize operation: move stuff
+        initData(numBuckets);
+        if (oldMaxElementsWithBuffer > 1) {
+            for (size_t i = 0; i < oldMaxElementsWithBuffer; ++i) {
+                if (oldInfo[i] != 0) {
+                    // might throw an exception, which is really bad since we are in the middle of
+                    // moving stuff.
+                    insert_move(std::move(oldKeyVals[i]));
+                    // destroy the node but DON'T destroy the data.
+                    oldKeyVals[i].~Node();
+                }
+            }
+
+            // this check is not necessary as it's guarded by the previous if, but it helps
+            // silence g++'s overeager "attempt to free a non-heap object 'map'
+            // [-Werror=free-nonheap-object]" warning.
+            if (oldKeyVals != reinterpret_cast_no_cast_align_warning<Node*>(&mMask)) {
+                // don't destroy old data: put it into the pool instead
+                if (forceFree) {
+                    std::free(oldKeyVals);
+                } else {
+                    DataPool::addOrFree(oldKeyVals, calcNumBytesTotal(oldMaxElementsWithBuffer));
+                }
+            }
+        }
+    }
+
+    ROBIN_HOOD(NOINLINE) void throwOverflowError() const {
+#if ROBIN_HOOD(HAS_EXCEPTIONS)
+        throw std::overflow_error("robin_hood::map overflow");
+#else
+        abort();
+#endif
+    }
+
+    template <typename OtherKey, typename... Args>
+    std::pair<iterator, bool> try_emplace_impl(OtherKey&& key, Args&&... args) {
+        ROBIN_HOOD_TRACE(this)
+        auto idxAndState = insertKeyPrepareEmptySpot(key);
+        switch (idxAndState.second) {
+        case InsertionState::key_found:
+            break;
+
+        case InsertionState::new_node:
+            ::new (static_cast<void*>(&mKeyVals[idxAndState.first])) Node(
+                *this, std::piecewise_construct, std::forward_as_tuple(std::forward<OtherKey>(key)),
+                std::forward_as_tuple(std::forward<Args>(args)...));
+            break;
+
+        case InsertionState::overwrite_node:
+            mKeyVals[idxAndState.first] = Node(*this, std::piecewise_construct,
+                                               std::forward_as_tuple(std::forward<OtherKey>(key)),
+                                               std::forward_as_tuple(std::forward<Args>(args)...));
+            break;
+
+        case InsertionState::overflow_error:
+            throwOverflowError();
+            break;
+        }
+
+        return std::make_pair(iterator(mKeyVals + idxAndState.first, mInfo + idxAndState.first),
+                              InsertionState::key_found != idxAndState.second);
+    }
+
+    template <typename OtherKey, typename Mapped>
+    std::pair<iterator, bool> insertOrAssignImpl(OtherKey&& key, Mapped&& obj) {
+        ROBIN_HOOD_TRACE(this)
+        auto idxAndState = insertKeyPrepareEmptySpot(key);
+        switch (idxAndState.second) {
+        case InsertionState::key_found:
+            mKeyVals[idxAndState.first].getSecond() = std::forward<Mapped>(obj);
+            break;
+
+        case InsertionState::new_node:
+            ::new (static_cast<void*>(&mKeyVals[idxAndState.first])) Node(
+                *this, std::piecewise_construct, std::forward_as_tuple(std::forward<OtherKey>(key)),
+                std::forward_as_tuple(std::forward<Mapped>(obj)));
+            break;
+
+        case InsertionState::overwrite_node:
+            mKeyVals[idxAndState.first] = Node(*this, std::piecewise_construct,
+                                               std::forward_as_tuple(std::forward<OtherKey>(key)),
+                                               std::forward_as_tuple(std::forward<Mapped>(obj)));
+            break;
+
+        case InsertionState::overflow_error:
+            throwOverflowError();
+            break;
+        }
+
+        return std::make_pair(iterator(mKeyVals + idxAndState.first, mInfo + idxAndState.first),
+                              InsertionState::key_found != idxAndState.second);
+    }
+
+    void initData(size_t max_elements) {
+        mNumElements = 0;
+        mMask = max_elements - 1;
+        mMaxNumElementsAllowed = calcMaxNumElementsAllowed(max_elements);
+
+        auto const numElementsWithBuffer = calcNumElementsWithBuffer(max_elements);
+
+        // malloc & zero mInfo. Faster than calloc everything.
+        auto const numBytesTotal = calcNumBytesTotal(numElementsWithBuffer);
+        ROBIN_HOOD_LOG("std::calloc " << numBytesTotal << " = calcNumBytesTotal("
+                                      << numElementsWithBuffer << ")")
+        mKeyVals = reinterpret_cast<Node*>(
+            detail::assertNotNull<std::bad_alloc>(std::malloc(numBytesTotal)));
+        mInfo = reinterpret_cast<uint8_t*>(mKeyVals + numElementsWithBuffer);
+        std::memset(mInfo, 0, numBytesTotal - numElementsWithBuffer * sizeof(Node));
+
+        // set sentinel
+        mInfo[numElementsWithBuffer] = 1;
+
+        mInfoInc = InitialInfoInc;
+        mInfoHashShift = InitialInfoHashShift;
+    }
+
+    enum class InsertionState { overflow_error, key_found, new_node, overwrite_node };
+
+    // Finds key, and if not already present prepares a spot where to pot the key & value.
+    // This potentially shifts nodes out of the way, updates mInfo and number of inserted
+    // elements, so the only operation left to do is create/assign a new node at that spot.
+    template <typename OtherKey>
+    std::pair<size_t, InsertionState> insertKeyPrepareEmptySpot(OtherKey&& key) {
+        for (int i = 0; i < 256; ++i) {
+            size_t idx{};
+            InfoType info{};
+            keyToIdx(key, &idx, &info);
+            nextWhileLess(&info, &idx);
+
+            // while we potentially have a match
+            while (info == mInfo[idx]) {
+                if (WKeyEqual::operator()(key, mKeyVals[idx].getFirst())) {
+                    // key already exists, do NOT insert.
+                    // see http://en.cppreference.com/w/cpp/container/unordered_map/insert
+                    return std::make_pair(idx, InsertionState::key_found);
+                }
+                next(&info, &idx);
+            }
+
+            // unlikely that this evaluates to true
+            if (ROBIN_HOOD_UNLIKELY(mNumElements >= mMaxNumElementsAllowed)) {
+                if (!increase_size()) {
+                    return std::make_pair(size_t(0), InsertionState::overflow_error);
+                }
+                continue;
+            }
+
+            // key not found, so we are now exactly where we want to insert it.
+            auto const insertion_idx = idx;
+            auto const insertion_info = info;
+            if (ROBIN_HOOD_UNLIKELY(insertion_info + mInfoInc > 0xFF)) {
+                mMaxNumElementsAllowed = 0;
+            }
+
+            // find an empty spot
+            while (0 != mInfo[idx]) {
+                next(&info, &idx);
+            }
+
+            if (idx != insertion_idx) {
+                shiftUp(idx, insertion_idx);
+            }
+            // put at empty spot
+            mInfo[insertion_idx] = static_cast<uint8_t>(insertion_info);
+            ++mNumElements;
+            return std::make_pair(insertion_idx, idx == insertion_idx
+                                                     ? InsertionState::new_node
+                                                     : InsertionState::overwrite_node);
+        }
+
+        // enough attempts failed, so finally give up.
+        return std::make_pair(size_t(0), InsertionState::overflow_error);
+    }
+
+    bool try_increase_info() {
+        ROBIN_HOOD_LOG("mInfoInc=" << mInfoInc << ", numElements=" << mNumElements
+                                   << ", maxNumElementsAllowed="
+                                   << calcMaxNumElementsAllowed(mMask + 1))
+        if (mInfoInc <= 2) {
+            // need to be > 2 so that shift works (otherwise undefined behavior!)
+            return false;
+        }
+        // we got space left, try to make info smaller
+        mInfoInc = static_cast<uint8_t>(mInfoInc >> 1U);
+
+        // remove one bit of the hash, leaving more space for the distance info.
+        // This is extremely fast because we can operate on 8 bytes at once.
+        ++mInfoHashShift;
+        auto const numElementsWithBuffer = calcNumElementsWithBuffer(mMask + 1);
+
+        for (size_t i = 0; i < numElementsWithBuffer; i += 8) {
+            auto val = unaligned_load<uint64_t>(mInfo + i);
+            val = (val >> 1U) & UINT64_C(0x7f7f7f7f7f7f7f7f);
+            std::memcpy(mInfo + i, &val, sizeof(val));
+        }
+        // update sentinel, which might have been cleared out!
+        mInfo[numElementsWithBuffer] = 1;
+
+        mMaxNumElementsAllowed = calcMaxNumElementsAllowed(mMask + 1);
+        return true;
+    }
+
+    // True if resize was possible, false otherwise
+    bool increase_size() {
+        // nothing allocated yet? just allocate InitialNumElements
+        if (0 == mMask) {
+            initData(InitialNumElements);
+            return true;
+        }
+
+        auto const maxNumElementsAllowed = calcMaxNumElementsAllowed(mMask + 1);
+        if (mNumElements < maxNumElementsAllowed && try_increase_info()) {
+            return true;
+        }
+
+        ROBIN_HOOD_LOG("mNumElements=" << mNumElements << ", maxNumElementsAllowed="
+                                       << maxNumElementsAllowed << ", load="
+                                       << (static_cast<double>(mNumElements) * 100.0 /
+                                           (static_cast<double>(mMask) + 1)))
+
+        if (mNumElements * 2 < calcMaxNumElementsAllowed(mMask + 1)) {
+            // we have to resize, even though there would still be plenty of space left!
+            // Try to rehash instead. Delete freed memory so we don't steadyily increase mem in case
+            // we have to rehash a few times
+            nextHashMultiplier();
+            rehashPowerOfTwo(mMask + 1, true);
+        } else {
+            // we've reached the capacity of the map, so the hash seems to work nice. Keep using it.
+            rehashPowerOfTwo((mMask + 1) * 2, false);
+        }
+        return true;
+    }
+
+    void nextHashMultiplier() {
+        // adding an *even* number, so that the multiplier will always stay odd. This is necessary
+        // so that the hash stays a mixing function (and thus doesn't have any information loss).
+        mHashMultiplier += UINT64_C(0xc4ceb9fe1a85ec54);
+    }
+
+    void destroy() {
+        if (0 == mMask) {
+            // don't deallocate!
+            return;
+        }
+
+        Destroyer<Self, IsFlat && std::is_trivially_destructible<Node>::value>{}
+            .nodesDoNotDeallocate(*this);
+
+        // This protection against not deleting mMask shouldn't be needed as it's sufficiently
+        // protected with the 0==mMask check, but I have this anyways because g++ 7 otherwise
+        // reports a compile error: attempt to free a non-heap object 'fm'
+        // [-Werror=free-nonheap-object]
+        if (mKeyVals != reinterpret_cast_no_cast_align_warning<Node*>(&mMask)) {
+            ROBIN_HOOD_LOG("std::free")
+            std::free(mKeyVals);
+        }
+    }
+
+    void init() noexcept {
+        mKeyVals = reinterpret_cast_no_cast_align_warning<Node*>(&mMask);
+        mInfo = reinterpret_cast<uint8_t*>(&mMask);
+        mNumElements = 0;
+        mMask = 0;
+        mMaxNumElementsAllowed = 0;
+        mInfoInc = InitialInfoInc;
+        mInfoHashShift = InitialInfoHashShift;
+    }
+
+    // members are sorted so no padding occurs
+    uint64_t mHashMultiplier = UINT64_C(0xc4ceb9fe1a85ec53);                // 8 byte  8
+    Node* mKeyVals = reinterpret_cast_no_cast_align_warning<Node*>(&mMask); // 8 byte 16
+    uint8_t* mInfo = reinterpret_cast<uint8_t*>(&mMask);                    // 8 byte 24
+    size_t mNumElements = 0;                                                // 8 byte 32
+    size_t mMask = 0;                                                       // 8 byte 40
+    size_t mMaxNumElementsAllowed = 0;                                      // 8 byte 48
+    InfoType mInfoInc = InitialInfoInc;                                     // 4 byte 52
+    InfoType mInfoHashShift = InitialInfoHashShift;                         // 4 byte 56
+                                                    // 16 byte 56 if NodeAllocator
+};
+
+} // namespace detail
+
+// map
+
+template <typename Key, typename T, typename Hash = hash<Key>,
+          typename KeyEqual = std::equal_to<Key>, size_t MaxLoadFactor100 = 80>
+using unordered_flat_map = detail::Table<true, MaxLoadFactor100, Key, T, Hash, KeyEqual>;
+
+template <typename Key, typename T, typename Hash = hash<Key>,
+          typename KeyEqual = std::equal_to<Key>, size_t MaxLoadFactor100 = 80>
+using unordered_node_map = detail::Table<false, MaxLoadFactor100, Key, T, Hash, KeyEqual>;
+
+template <typename Key, typename T, typename Hash = hash<Key>,
+          typename KeyEqual = std::equal_to<Key>, size_t MaxLoadFactor100 = 80>
+using unordered_map =
+    detail::Table<sizeof(robin_hood::pair<Key, T>) <= sizeof(size_t) * 6 &&
+                      std::is_nothrow_move_constructible<robin_hood::pair<Key, T>>::value &&
+                      std::is_nothrow_move_assignable<robin_hood::pair<Key, T>>::value,
+                  MaxLoadFactor100, Key, T, Hash, KeyEqual>;
+
+// set
+
+template <typename Key, typename Hash = hash<Key>, typename KeyEqual = std::equal_to<Key>,
+          size_t MaxLoadFactor100 = 80>
+using unordered_flat_set = detail::Table<true, MaxLoadFactor100, Key, void, Hash, KeyEqual>;
+
+template <typename Key, typename Hash = hash<Key>, typename KeyEqual = std::equal_to<Key>,
+          size_t MaxLoadFactor100 = 80>
+using unordered_node_set = detail::Table<false, MaxLoadFactor100, Key, void, Hash, KeyEqual>;
+
+template <typename Key, typename Hash = hash<Key>, typename KeyEqual = std::equal_to<Key>,
+          size_t MaxLoadFactor100 = 80>
+using unordered_set = detail::Table<sizeof(Key) <= sizeof(size_t) * 6 &&
+                                        std::is_nothrow_move_constructible<Key>::value &&
+                                        std::is_nothrow_move_assignable<Key>::value,
+                                    MaxLoadFactor100, Key, void, Hash, KeyEqual>;
+
+} // namespace robin_hood
+
+#endif
diff --git a/intel_extension_for_pytorch/nn/modules/__init__.py b/intel_extension_for_pytorch/nn/modules/__init__.py
index fdce63d89..cc7f5c2dd 100644
--- a/intel_extension_for_pytorch/nn/modules/__init__.py
+++ b/intel_extension_for_pytorch/nn/modules/__init__.py
@@ -3,3 +3,6 @@ from . import _roi_align
 from .merged_embeddingbag import MergedEmbeddingBag
 from .merged_embeddingbag import MergedEmbeddingBagWithSGD
 from .linear_fuse_eltwise import IPEXLinearEltwise
+from .mlperf_merged_embeddingbag import MergedEmbeddingBagWithAdagrad
+from .mlperf_interaction import mlperf_interaction
+from .mlperf_mergedfreqembbag import MergFreqEmbeddingBag
diff --git a/intel_extension_for_pytorch/nn/modules/mlperf_interaction.py b/intel_extension_for_pytorch/nn/modules/mlperf_interaction.py
new file mode 100644
index 000000000..73fd9b738
--- /dev/null
+++ b/intel_extension_for_pytorch/nn/modules/mlperf_interaction.py
@@ -0,0 +1,30 @@
+import torch
+from torch import nn
+from torch.autograd import Function
+import time
+
+def mlperf_interaction(x0, xlw, xl):
+    if torch.is_grad_enabled():
+        res = MLPerfInteractionFunc.apply(x0, xlw, xl)
+        return res
+    return torch.ops.torch_ipex.mlperf_interaction_forward(x0, xlw, xl)
+
+class MLPerfInteractionFunc(Function):
+    @staticmethod
+    def unpack(*args):
+        return args
+
+    @staticmethod
+    def forward(ctx, x0, xlw, xl):
+        output = torch.ops.torch_ipex.mlperf_interaction_forward(x0, xlw, xl)
+        ctx.x0 = x0
+        ctx.xlw = xlw
+        ctx.xl = xl
+        return output
+
+    @staticmethod
+    def backward(ctx, grad_out):
+        x0, xlw, xl = ctx.x0, ctx.xlw, ctx.xl
+        x0_diff, xlw_diff = torch.ops.torch_ipex.mlperf_interaction_backward(
+            x0, xlw, xl, grad_out)
+        return x0_diff, xlw_diff, grad_out
diff --git a/intel_extension_for_pytorch/nn/modules/mlperf_merged_embeddingbag.py b/intel_extension_for_pytorch/nn/modules/mlperf_merged_embeddingbag.py
new file mode 100644
index 000000000..c70cef97d
--- /dev/null
+++ b/intel_extension_for_pytorch/nn/modules/mlperf_merged_embeddingbag.py
@@ -0,0 +1,74 @@
+import torch
+from torch import nn
+from torch.autograd import Function
+
+def merged_embeddingbag_adagrad(indices, offsets, weights, hessian, multihot_size, embedding_size, lr=None, eps=None):
+    if torch.is_grad_enabled():
+        res = MergedEmbeddingBagAdagradFunc.apply(indices, offsets, hessian, multihot_size, embedding_size, -lr, eps, *weights)
+        return res
+    return torch.ops.torch_ipex.mlperf_merged_embeddingbag_forward(
+            indices, offsets, weights, multihot_size)
+
+class MergedEmbeddingBagAdagradFunc(Function):
+    @staticmethod
+    def unpack(*args):
+        return args
+
+    @staticmethod
+    def forward(ctx, indices, offsets, hessian, multihot_size, embedding_size, lr, eps, *weights):
+
+        output = torch.ops.torch_ipex.mlperf_merged_embeddingbag_forward(
+            indices, offsets, weights, multihot_size)
+        ctx.indices = indices
+        ctx.offsets = offsets
+        ctx.weights = weights
+        ctx.hessian = hessian
+        ctx.multihot_size = multihot_size
+        ctx.embedding_size = embedding_size
+        ctx.lr = lr
+        ctx.eps = eps
+        return output
+
+    @staticmethod
+    def backward(ctx, grad_out):
+        indices = ctx.indices
+        offsets = ctx.offsets
+        weights = ctx.weights
+        hessian = ctx.hessian
+        multihot_size = ctx.multihot_size
+        embedding_size = ctx.embedding_size
+        lr = ctx.lr
+        eps = ctx.eps
+        assert(hessian is not None)
+        torch.ops.torch_ipex.mlperf_merged_embeddingbag_backward(
+            indices, offsets, weights, hessian, grad_out.contiguous(), multihot_size, embedding_size, lr, eps)
+        return MergedEmbeddingBagAdagradFunc.unpack(*[None]*(7 + len(weights)))
+
+class MergedEmbeddingBagWithAdagrad(nn.Module):
+    def __init__(self,
+                 weights,
+                 multihot_size=[3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1],
+                 embedding_size=[40000000,39060,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36]):
+        super(MergedEmbeddingBagWithAdagrad, self).__init__()
+        self._weights = torch.nn.ParameterList(weights)
+        for i in range(len(self._weights)):
+            self._weights[i].no_state_sum = True
+        self._hessian = [torch.zeros_like(w) for w in self._weights]
+        self._multihot_size = multihot_size
+        self._embedding_size = embedding_size
+        self._optimizer = None
+
+    def set_optimizer(self, optimizer):
+        self._optimizer = optimizer
+
+    def forward(self, indices, offsets):
+        if self._optimizer is not None:
+            param_groups = self._optimizer.param_groups[0]
+            lr = param_groups['lr']
+            eps = param_groups['eps']
+        else:
+            lr = None
+            eps = None
+        return merged_embeddingbag_adagrad(indices, offsets, self._weights, self._hessian,
+                                           self._multihot_size, self._embedding_size,
+                                           lr, eps)
diff --git a/intel_extension_for_pytorch/nn/modules/mlperf_mergedfreqembbag.py b/intel_extension_for_pytorch/nn/modules/mlperf_mergedfreqembbag.py
new file mode 100644
index 000000000..bbc929cb6
--- /dev/null
+++ b/intel_extension_for_pytorch/nn/modules/mlperf_mergedfreqembbag.py
@@ -0,0 +1,419 @@
+import torch
+from torch.autograd import Function
+import torch.distributed as dist
+import oneccl_bindings_for_pytorch
+from typing import Dict, List
+import time
+
+def classify_index_by_freq(emb_idx_hist: torch.Tensor,
+                           nden: int = None, nfrq: int = None,
+                           ratio: float = 0.5):
+    # emb_idx_hist, index in history
+    # nden, number of dense row
+    index, count = torch.unique(emb_idx_hist, sorted=True, return_counts=True)
+    if nden is not None:
+        assert nden < len(index), "nden must be smaller than unique index"
+        sort_idx = torch.argsort(count, descending=True)
+        dense_idx = index[sort_idx[:nden]]
+        return dense_idx
+    elif nfrq is not None:
+        cond = count >= nfrq
+        dense_idx = index[torch.where(cond)]
+        return dense_idx
+    else:
+        assert ratio >= 0.0 and ratio <= 1.0, "ratio must be between 0 and 1"
+        sort_idx = torch.argsort(count, descending=True)
+        nden = int(len(sort_idx) * ratio)
+        dense_idx = index[sort_idx[:nden]]
+        return dense_idx
+
+def classify_embwgt_by_freq(emb_idx_hist: torch.Tensor, wgt: torch.Tensor, ratio: float):
+    # emb_idx_hist, index in history
+    # sparse_idx: split_sparse_id -> row_id
+    # dense_idx: dense_id -> row_id
+    dense_idx = classify_index_by_freq(emb_idx_hist, ratio=ratio).long()
+    nrow_wgt = wgt.shape[0]
+    dense_lookup = torch.full(fill_value= -1, size=(nrow_wgt, )) # lookup dense row_idx by emb idx
+    # build up inverse lookup table
+    for i, j in enumerate(dense_idx):
+        dense_lookup[j] = i
+    wgt_dense = wgt[dense_idx, :]
+    return wgt_dense, dense_lookup
+
+def dense_forward(rank: int, lbatch: int,
+                  multi_hots: List[int],
+                  indexes: List[torch.Tensor],
+                  idx_offset: torch.Tensor,
+                  lookup: torch.Tensor,
+                  dense_wgt: torch.Tensor,
+                  res: torch.Tensor):
+    # idx input index
+    # if it's a dense row, copy to local result
+    dim = dense_wgt.shape[1]
+    for n, (idx, multi_hot, offset) in enumerate(zip(indexes, multi_hots, idx_offset)):
+        for l in range(lbatch):
+            for h in range(multi_hot):
+                emb_idx = idx[rank * lbatch * multi_hot + l * multi_hot + h] + offset
+                if lookup[emb_idx] >= 0:
+                    res[l, n, :] += dense_wgt[lookup[emb_idx], :]
+ZERO_SHAPE = (1, )
+
+def sparse_forward_local(rank: int, size: int,
+                         gbatch: int, lbatch: int, num_emb: int,
+                         multi_hots: List[int],
+                         indexes: List[torch.Tensor],
+                         idx_offset: torch.Tensor,
+                         lookup: torch.Tensor,
+                         sparse_wgt: torch.Tensor):
+    # if it's a sparse row, copy to local buffer for further exchange
+    # local buffer format, [rank requires this row]{rowi: vector} rowi is the index of this row in remote result
+    exch_buffer = [{} for i in range(size)]
+    for n, (idx, multi_hot, offset) in enumerate(zip(indexes, multi_hots, idx_offset)):
+        for b in range(gbatch):
+            for h in range(multi_hot):
+                emb_idx = idx[b * multi_hot + h] + offset
+                if (lookup[emb_idx] < 0) and (emb_idx % size == rank):
+                    # if sparse row and save in this rank
+                    dest = b // lbatch
+                    rowi = (b % lbatch * num_emb) + n
+                    loc_se_idx = emb_idx // size
+                    if rowi in exch_buffer[dest]:
+                        exch_buffer[dest][rowi] += sparse_wgt[loc_se_idx, :]
+                    else:
+                        exch_buffer[dest][rowi] = sparse_wgt[loc_se_idx, :].clone()
+    send_idx = [torch.tensor(list(i.keys()), dtype=torch.int64)
+                if len(i) > 0 else
+                torch.zeros(ZERO_SHAPE, dtype=torch.int64)
+                for i in exch_buffer]
+    send_buf = [torch.cat([v.unsqueeze(0) for v in r.values()], dim=0)
+                if len(r) > 0 else
+                torch.zeros(ZERO_SHAPE, dtype=torch.float32)
+                for r in exch_buffer]
+    input_splits = torch.tensor([len(b) for b in exch_buffer], dtype=torch.int64)
+    return input_splits, send_idx, send_buf
+
+def sync_print(*args, **kwargs):
+    rank = dist.get_rank()
+    size = dist.get_world_size()
+    for i in range(size):
+        if rank == i:
+            print(f'{rank}/{size}: ', *args)
+        dist.barrier()
+
+def sparse_all2all(rank: int, size: int,
+                   ofs_size: int, emb_dim: int,
+                   is_buffer: torch.Tensor,
+                   os_buffer: torch.Tensor,
+                   send_idx: List[torch.Tensor],
+                   send_buf: List[torch.Tensor],
+                   send_ofs: List[torch.Tensor]):
+    is_buffers = list(is_buffer.chunk(size))
+    os_buffers = list(os_buffer.chunk(size))
+    dist.all_to_all(os_buffers, is_buffers)
+    dist.barrier()
+
+    recv_idx = [torch.zeros(i, dtype=torch.int64)
+                if i > 0 else
+                torch.zeros(ZERO_SHAPE, dtype=torch.int64)
+                for i in os_buffer]
+    output_splits = list(os_buffer)
+    recv_buf = [torch.zeros((i, emb_dim), dtype=torch.bfloat16)
+                if i > 0 else
+                torch.zeros(ZERO_SHAPE, dtype=torch.bfloat16)
+                for i in output_splits]
+    recv_ofs = [torch.zeros((ofs_size, ), dtype=torch.int64)
+                for i in os_buffer]
+    dist.all_to_all(recv_idx, send_idx)
+    dist.all_to_all(recv_buf, send_buf)
+    dist.all_to_all(recv_ofs, send_ofs)
+    dist.barrier()
+    return recv_idx, recv_buf, recv_ofs
+
+def sparse_forward_merge(num_emb: int,
+                         os_buffer: torch.Tensor,
+                         local_res: torch.Tensor,
+                         recv_i: List[torch.Tensor],
+                         recv_v: List[torch.Tensor]):
+    for j, (i, v) in enumerate(zip(recv_i, recv_v)):
+        if os_buffer[j] == 0:
+            continue
+        for ii, vv in zip(i, v):
+            local_res[ii // num_emb, ii % num_emb, :] += vv
+    return
+
+# def sparse_backward_all2all(rank: int, size: int,
+#                             lbatch: int, emb_dim: int,
+#                             exch_buffer: Dict):
+#     input_splits = [len(i) for i in exch_buffer]
+#     is_buffer = [torch.tensor(i) for i in input_splits]
+#     os_buffer = [torch.tensor(0) for i in range(len(input_splits))]
+#     dist.all_to_all(os_buffer, is_buffer)
+#     dist.barrier()
+
+#     send_idx = [torch.tensor(list(i.keys()), dtype=torch.int64)
+#                 if len(i) > 0 else
+#                 torch.zeros(ZERO_SHAPE, dtype=torch.int64)
+#                 for i in exch_buffer]
+#     recv_idx = [torch.zeros(i, dtype=torch.int64)
+#                 if i > 0 else
+#                 torch.zeros(ZERO_SHAPE, dtype=torch.int64)
+#                 for i in os_buffer]
+#     output_splits = list(os_buffer)
+#     recv_buffer = [torch.zeros((i, emb_dim), dtype=torch.float32)
+#                    if i > 0 else
+#                    torch.zeros(ZERO_SHAPE, dtype=torch.float32)
+#                    for i in output_splits]
+#     send_buffer = [torch.cat([v.unsqueeze(0) for v in r.values()], dim=0)
+#                    if len(r) > 0 else
+#                    torch.zeros(ZERO_SHAPE, dtype=torch.float32)
+#                    for r in exch_buffer]
+#     dist.all_to_all(recv_idx, send_idx)
+#     dist.all_to_all(recv_buffer, send_buffer)
+#     dist.barrier()
+#     return recv_idx, recv_buffer
+
+def dense_backward_allreduce(dense_grad):
+    dist.all_reduce(dense_grad)
+    return
+
+def sparse_backward_local(rank: int, size: int,
+                          lbatch: int,
+                          is_buffer: torch.Tensor,
+                          multi_hots: torch.Tensor,
+                          indexes: List[torch.Tensor],
+                          lookup:torch.Tensor,
+                          idx_offset: torch.Tensor,
+                          grad: torch.Tensor):
+    exch_buffer = [{} for i in range(size)]
+    for n, (index, multi_hot, offset) in enumerate(zip(indexes, multi_hots, idx_offset)):
+        for i in range(lbatch):
+            for j in range(multi_hot):
+                gi = i * multi_hot + j + rank * lbatch * multi_hot
+                emb_idx = index[gi].item() + offset # make sure emb_idx is a number instead of a tensor
+                remote_rank = emb_idx % size
+                if lookup[emb_idx] < 0:
+                    # if sparse row, only send to emb_idx%size
+                    if emb_idx in exch_buffer[remote_rank]:
+                        exch_buffer[remote_rank][emb_idx] += grad[i, n, :]
+                    else:
+                        exch_buffer[remote_rank][emb_idx] = grad[i, n, :].clone()
+    send_idx = [torch.tensor(list(i.keys()), dtype=torch.int64)
+                if len(i) > 0 else
+                torch.zeros(ZERO_SHAPE, dtype=torch.int64)
+                for i in exch_buffer]
+    send_buf = [torch.cat([v.unsqueeze(0) for v in r.values()], dim=0)
+                if len(r) > 0 else
+                torch.zeros(ZERO_SHAPE, dtype=torch.float32)
+                for r in exch_buffer]
+    input_splits = torch.tensor([len(i) for i in exch_buffer], dtype=torch.int64)
+    return input_splits, send_idx, send_buf
+
+def dense_backward_local(rank: int, size: int,
+                         lbatch: int, multi_hots: List[int],
+                         indexes: List[torch.Tensor],
+                         lookup: torch.Tensor,
+                         idx_offset: torch.Tensor,
+                         grad: torch.Tensor,
+                         dense_grad: torch.Tensor):
+    for n, (index, multi_hot, offset) in enumerate(zip(indexes, multi_hots, idx_offset)):
+        for i in range(lbatch):
+            for j in range(multi_hot):
+                gi = i * multi_hot + j + rank * lbatch * multi_hot
+                emb_idx = index[gi] + offset
+                dense_idx = lookup[emb_idx]
+                if dense_idx >= 0:
+                    dense_grad[dense_idx] += grad[i, n, :]
+    return dense_grad
+
+def sparse_backward_merge(os_buffer: torch.Tensor,
+                          rank_idx: List[torch.Tensor],
+                          rank_buffer: List[torch.Tensor]):
+    local_grad = {}
+    for num, idx, buf in zip(os_buffer, rank_idx, rank_buffer):
+        if num == 0:
+            continue
+        for k, (i, r) in enumerate(zip(idx, buf)):
+            j = i.item()
+            if j in local_grad:
+                local_grad[j] += r
+            else:
+                local_grad[j] = r.clone()
+    return local_grad
+
+def adagrad_update_dense(lr: float,
+                         eps: float,
+                         dense_grad: torch.Tensor,
+                         dense_wgt: torch.Tensor,
+                         dense_hessian: torch.Tensor):
+    if dense_wgt.shape[0] == 0:
+        return
+    dense_hessian += dense_grad ** 2.0
+    dense_wgt += dense_grad * lr / (torch.sqrt(dense_hessian) + eps)
+    return
+
+def adagrad_update_sparse(size: int,
+                          lr: float,
+                          eps: float,
+                          sparse_grad: Dict,
+                          sparse_wgt: torch.Tensor,
+                          sparse_hessian: torch.Tensor):
+    for rid, lgrad in sparse_grad.items():
+        lrid = rid // size
+        sparse_hessian[lrid, :] += lgrad ** 2.0
+        sparse_wgt[lrid] += lgrad * lr / (torch.sqrt(sparse_hessian[lrid, :]) + eps)
+    return
+
+def sparse_backward_adagrad(size: int,
+                            lr: float,
+                            eps: float,
+                            os_buffer: torch.Tensor,
+                            recv_idx: List[torch.Tensor],
+                            recv_buf: List[torch.Tensor],
+                            sparse_weight: torch.Tensor,
+                            sparse_hessian: torch.Tensor):
+    sparse_grad = sparse_backward_merge(os_buffer, recv_idx, recv_buf)
+    adagrad_update_sparse(size, lr, eps, sparse_grad, sparse_weight, sparse_hessian)
+    return
+
+class MergFreqEmbeddingBagFunc(Function):
+    @staticmethod
+    def forward(ctx, rank: int, size: int,
+                gbatch: int, multihot: List[int],
+                lr: float, eps: float,
+                index: List[torch.Tensor], offset: List[torch.Tensor],
+                idx_offset: torch.Tensor, lookup: torch.Tensor,
+                dense_weight: torch.Tensor, sparse_weight: torch.Tensor,
+                dense_hessian: torch.Tensor, sparse_hessian: torch.Tensor):
+        time_stat = {}
+        t0 = time.time()
+        ctx.save_for_backward(lookup, idx_offset, multihot)
+        emb_dim = 128 #dense_weight.shape[1]
+        lbatch = gbatch // size
+        num_emb = len(index) # 26
+        ctx.cfg = (rank, size, lbatch, lr, eps, index, offset, dense_weight, sparse_weight, dense_hessian, sparse_hessian)
+        res = torch.zeros((lbatch, num_emb, emb_dim), dtype=torch.bfloat16)
+        is_buffer = torch.zeros(size, dtype=torch.int64)
+        os_buffer = torch.zeros(size, dtype=torch.int64)
+        time_stat['fwd_init'] = time.time() - t0
+        t0 = time.time()
+        send_idx, send_buf, send_ofs = torch.ops.torch_ipex.mlperf_freqembbag_spsfwd_local(
+            rank, size, gbatch,
+            is_buffer, multihot,
+            index, idx_offset, lookup,
+            sparse_weight)
+        time_stat['fwd_sparse_local'] = time.time() - t0
+        t0 = time.time()
+        recv_idx, recv_buf, recv_ofs = sparse_all2all(rank, size, num_emb + 1, emb_dim,
+                                                      is_buffer, os_buffer, send_idx, send_buf, send_ofs)
+        time_stat['fwd_all2all'] = time.time() - t0
+        t0 = time.time()
+        ## torch.ops.torch_ipex.mlperf_freqembbag_denfwd(rank, lbatch, multihot, index, idx_offset, lookup, dense_weight, res)
+        time_stat['fwd_dense'] = time.time() - t0
+        t0 = time.time()
+        torch.ops.torch_ipex.mlperf_freqembbag_spsfwd_merge(size, num_emb, os_buffer, recv_idx, recv_buf, recv_ofs, res);
+        time_stat['fwd_sparse_merge'] = time.time() - t0
+        # print(time_stat)
+        return res
+
+    @staticmethod
+    def backward(ctx, grad: torch.Tensor):
+        time_stat = {}
+        grad = grad.contiguous() # why need contiguous manually syk
+        t0 = time.time()
+        lookup, idx_offset, multihot = ctx.saved_tensors
+        rank, size, lbatch, lr, eps, index, offset, dense_weight, sparse_weight, dense_hessian, sparse_hessian = ctx.cfg
+        emb_dim = 128# dense_weight.shape[1]
+        is_buffer = torch.zeros(size, dtype=torch.int64)
+        os_buffer = torch.zeros(size, dtype=torch.int64)
+        time_stat['bwd_init'] = time.time() - t0
+        t0 = time.time()
+        send_idx, send_buf, send_ofs = torch.ops.torch_ipex.mlperf_freqembbag_spsbwd_local(
+            rank, size, lbatch,
+            is_buffer, multihot,
+            index, idx_offset, lookup, grad)
+        time_stat['bwd_local'] = time.time() - t0
+        t0 = time.time()
+        recv_idx, recv_buf, recv_ofs = sparse_all2all(
+            rank, size, torch.get_num_threads() + 1, emb_dim,
+            is_buffer, os_buffer,
+            send_idx, send_buf, send_ofs)
+        time_stat['bwd_all2all'] = time.time() - t0
+        ## dense_grad = torch.zeros_like(dense_weight)
+        t0 = time.time()
+        ## torch.ops.torch_ipex.mlperf_freqembbag_denbwd_local(rank, size, lbatch,
+        ##                                                     multihot,
+        ##                                                     index, lookup,
+        ##                                                     idx_offset,
+        ##                                                     grad, dense_grad)
+        time_stat['bwd_denlocal'] = time.time() - t0
+        t0 = time.time()
+        ## dense_backward_allreduce(dense_grad)
+        time_stat['bwd_reduce'] = time.time() - t0
+        t0 = time.time()
+        torch.ops.torch_ipex.mlperf_freqembbag_spsbwd_adagrad(size, lr, eps, os_buffer, recv_idx, recv_buf, recv_ofs,
+                                                              sparse_weight, sparse_hessian)
+        time_stat['bwd_adasps'] = time.time() - t0
+        t0 = time.time()
+        ## torch.ops.torch_ipex.mlperf_freqembbag_denbwd_adagrad(lr, eps, dense_grad, dense_weight, dense_hessian)
+        time_stat['bwd_adaden'] = time.time() - t0
+        # print(time_stat)
+        return None, None, None, None, \
+            None, None, None, None, \
+            None, None, None, None, \
+            None, None
+
+class MergFreqEmbeddingBag(torch.nn.Module):
+    """
+    Frequency based embeddingbag, fused with adagrad update weight
+    """
+    def __init__(self,
+                 ln_emb: List[int], multi_hot: List[int],
+                 index_histories: List[torch.Tensor],
+                 weight: List[torch.Tensor],
+                 dense_ratio: float =0.0):
+        super(MergFreqEmbeddingBag, self).__init__()
+        self._rank = dist.get_rank()
+        self._size = dist.get_world_size()
+        self._multi_hot = torch.tensor(multi_hot, dtype=torch.int64)
+        # create offset
+        index_offset = [0 for i in range(len(ln_emb) + 1)]
+        for i, l in enumerate(ln_emb):
+            index_offset[i + 1] = l + index_offset[i]
+        self._idx_offset = torch.tensor(index_offset)
+
+        # create index with offset
+        ## index_history_off = torch.cat([i + o for o, i in zip(index_offset, index_histories)])
+        # create allin1 weight
+        weight_allin1 = torch.cat(weight)
+        ## _dense_weight, self._lookup = classify_embwgt_by_freq(
+        ##     index_history_off, weight_allin1, dense_ratio)
+        _dense_weight = torch.zeros(size=(1, 128), dtype=torch.float32)
+        self._lookup = torch.full(fill_value= -1, size=(weight_allin1.shape[0], ))
+        self._dense_weight = torch.nn.parameter.Parameter(_dense_weight)
+        self._dense_weight.no_state_sum = True
+        self._dense_hessian = torch.zeros_like(self._dense_weight)
+        self._sparse_weight = torch.nn.parameter.Parameter(weight_allin1[self._rank::self._size, :].clone())
+        self._sparse_weight.no_state_sum = True
+        self._sparse_hessian = torch.zeros_like(self._sparse_weight, dtype=torch.bfloat16)
+        return
+
+    def set_optimizer(self, opt):
+        self._optimizer = opt
+        return
+
+    def forward(self, index: List[torch.Tensor],
+                offset: List[torch.Tensor]):
+        batch = index[0].shape[0] // self._multi_hot[0].item()
+        param_groups = self._optimizer.param_groups[0]
+        lr = -param_groups['lr']
+        eps = param_groups['eps']
+        out = MergFreqEmbeddingBagFunc.apply(self._rank, self._size,
+                                              batch, self._multi_hot,
+                                              lr, eps, index, offset,
+                                              self._idx_offset,
+                                              self._lookup,
+                                              self._dense_weight,
+                                              self._sparse_weight,
+                                              self._dense_hessian,
+                                              self._sparse_hessian)
+        return out
diff --git a/intel_extension_for_pytorch/nn/utils/_weight_cast.py b/intel_extension_for_pytorch/nn/utils/_weight_cast.py
index ce422bcc0..ffc2bd961 100644
--- a/intel_extension_for_pytorch/nn/utils/_weight_cast.py
+++ b/intel_extension_for_pytorch/nn/utils/_weight_cast.py
@@ -15,8 +15,8 @@ IPEX_WEIGHT_CAST_MODULE = {
     torch.nn.ConvTranspose2d,
     torch.nn.ConvTranspose3d,
     # ipex support
-    torch.nn.EmbeddingBag,
-    torch.nn.Embedding,
+    # torch.nn.EmbeddingBag,
+    # torch.nn.Embedding,
     _LSTM,
 }
 
diff --git a/intel_extension_for_pytorch/optim/__init__.py b/intel_extension_for_pytorch/optim/__init__.py
index 8b1378917..2970b8155 100644
--- a/intel_extension_for_pytorch/optim/__init__.py
+++ b/intel_extension_for_pytorch/optim/__init__.py
@@ -1 +1 @@
-
+from ._mlperf_adagrad import MLPerfAdagrad
diff --git a/intel_extension_for_pytorch/optim/_mlperf_adagrad.py b/intel_extension_for_pytorch/optim/_mlperf_adagrad.py
new file mode 100644
index 000000000..86890ec2f
--- /dev/null
+++ b/intel_extension_for_pytorch/optim/_mlperf_adagrad.py
@@ -0,0 +1,172 @@
+import torch
+from torch import Tensor
+
+from torch.optim import Optimizer
+from torch.optim.adagrad import adagrad
+from typing import List, Optional
+
+__all__ = ['MLPerfAdagrad']
+
+class MLPerfAdagrad(Optimizer):
+    r"""Implements Adagrad algorithm.
+
+    .. math::
+       \begin{aligned}
+            &\rule{110mm}{0.4pt}                                                                 \\
+            &\textbf{input}      : \gamma \text{ (lr)}, \: \theta_0 \text{ (params)}, \: f(\theta)
+                \text{ (objective)}, \: \lambda \text{ (weight decay)},                          \\
+            &\hspace{12mm}    \tau \text{ (initial accumulator value)}, \: \eta\text{ (lr decay)}\\
+            &\textbf{initialize} :  state\_sum_0 \leftarrow 0                             \\[-1.ex]
+            &\rule{110mm}{0.4pt}                                                                 \\
+            &\textbf{for} \: t=1 \: \textbf{to} \: \ldots \: \textbf{do}                         \\
+            &\hspace{5mm}g_t           \leftarrow   \nabla_{\theta} f_t (\theta_{t-1})           \\
+            &\hspace{5mm} \tilde{\gamma}    \leftarrow \gamma / (1 +(t-1) \eta)                  \\
+            &\hspace{5mm} \textbf{if} \: \lambda \neq 0                                          \\
+            &\hspace{10mm} g_t \leftarrow g_t + \lambda \theta_{t-1}                             \\
+            &\hspace{5mm}state\_sum_t  \leftarrow  state\_sum_{t-1} + g^2_t                      \\
+            &\hspace{5mm}\theta_t \leftarrow
+                \theta_{t-1}- \tilde{\gamma} \frac{g_t}{\sqrt{state\_sum_t}+\epsilon}            \\
+            &\rule{110mm}{0.4pt}                                                          \\[-1.ex]
+            &\bf{return} \:  \theta_t                                                     \\[-1.ex]
+            &\rule{110mm}{0.4pt}                                                          \\[-1.ex]
+       \end{aligned}
+
+    For further details regarding the algorithm we refer to `Adaptive Subgradient Methods for Online Learning
+    and Stochastic Optimization`_.
+
+    Args:
+        params (iterable): iterable of parameters to optimize or dicts defining
+            parameter groups
+        lr (float, optional): learning rate (default: 1e-2)
+        lr_decay (float, optional): learning rate decay (default: 0)
+        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
+        eps (float, optional): term added to the denominator to improve
+            numerical stability (default: 1e-10)
+        foreach (bool, optional): whether foreach implementation of optimizer is used (default: None)
+        maximize (bool, optional): maximize the params based on the objective, instead of
+            minimizing (default: False)
+
+    .. _Adaptive Subgradient Methods for Online Learning and Stochastic
+        Optimization: http://jmlr.org/papers/v12/duchi11a.html
+    """
+
+    def __init__(
+        self,
+        params,
+        lr=1e-2,
+        lr_decay=0,
+        weight_decay=0,
+        initial_accumulator_value=0,
+        eps=1e-10,
+        foreach: Optional[bool] = None,
+        *,
+        maximize: bool = False
+    ):
+        if not 0.0 <= lr:
+            raise ValueError("Invalid learning rate: {}".format(lr))
+        if not 0.0 <= lr_decay:
+            raise ValueError("Invalid lr_decay value: {}".format(lr_decay))
+        if not 0.0 <= weight_decay:
+            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
+        if not 0.0 <= initial_accumulator_value:
+            raise ValueError(
+                "Invalid initial_accumulator_value value: {}".format(
+                    initial_accumulator_value
+                )
+            )
+        if not 0.0 <= eps:
+            raise ValueError("Invalid epsilon value: {}".format(eps))
+
+        defaults = dict(
+            lr=lr,
+            lr_decay=lr_decay,
+            eps=eps,
+            weight_decay=weight_decay,
+            initial_accumulator_value=initial_accumulator_value,
+            foreach=foreach,
+            maximize=maximize,
+        )
+        super(MLPerfAdagrad, self).__init__(params, defaults)
+
+        for group in self.param_groups:
+            for p in group["params"]:
+                state = self.state[p]
+                state["step"] = torch.tensor(0.0)
+                init_value = (
+                    complex(initial_accumulator_value, initial_accumulator_value)
+                    if torch.is_complex(p)
+                    else initial_accumulator_value
+                )
+                if not hasattr(p, "no_state_sum"):
+                    state["sum"] = torch.full_like(
+                        p, init_value, memory_format=torch.preserve_format
+                    )
+                else:
+                    print('Skip State sum allocation')
+
+    def __setstate__(self, state):
+        super().__setstate__(state)
+        for group in self.param_groups:
+            group.setdefault("foreach", None)
+            group.setdefault("maximize", False)
+
+        state_values = list(self.state.values())
+        step_is_tensor = (len(state_values) != 0) and torch.is_tensor(
+            state_values[0]["step"]
+        )
+        if not step_is_tensor:
+            for s in state_values:
+                s["step"] = torch.tensor(float(s["step"]))
+
+    def share_memory(self):
+        for group in self.param_groups:
+            for p in group["params"]:
+                state = self.state[p]
+                state["sum"].share_memory_()
+
+    @torch.no_grad()
+    def step(self, closure=None):
+        """Performs a single optimization step.
+
+        Args:
+            closure (Callable, optional): A closure that reevaluates the model
+                and returns the loss.
+        """
+        loss = None
+
+        if closure is not None:
+            with torch.enable_grad():
+                loss = closure()
+
+        for group in self.param_groups:
+            params_with_grad = []
+            grads = []
+            state_sums = []
+            state_steps = []
+
+            has_sparse_grad = False
+            for p in group["params"]:
+                if p.grad is not None:
+                    if p.grad.is_sparse:
+                        has_sparse_grad = True
+                    params_with_grad.append(p)
+                    grads.append(p.grad)
+                    state = self.state[p]
+                    state_sums.append(state["sum"])
+                    state_steps.append(state["step"])
+
+            adagrad(
+                params_with_grad,
+                grads,
+                state_sums,
+                state_steps,
+                lr=group["lr"],
+                weight_decay=group["weight_decay"],
+                lr_decay=group["lr_decay"],
+                eps=group["eps"],
+                has_sparse_grad=has_sparse_grad,
+                foreach=group["foreach"],
+                maximize=group["maximize"],
+            )
+
+        return loss
diff --git a/intel_extension_for_pytorch/optim/_optimizer_utils.py b/intel_extension_for_pytorch/optim/_optimizer_utils.py
index 3f7f2b1c6..c8f20768b 100644
--- a/intel_extension_for_pytorch/optim/_optimizer_utils.py
+++ b/intel_extension_for_pytorch/optim/_optimizer_utils.py
@@ -6,6 +6,7 @@ from copy import deepcopy
 from itertools import chain
 from collections import defaultdict
 from ._functional import sgd_step, adagrad_step, lamb_step, adam_step, adamw_step
+from ._mlperf_adagrad import MLPerfAdagrad
 from ._lamb import Lamb
 from ..nn import utils
 
@@ -13,6 +14,7 @@ IPEX_FUSED_OPTIMIZER_LIST_CPU = [
     torch.optim.SGD,
     torch.optim.Adagrad,
     torch.optim.Adam,
+    MLPerfAdagrad,
     Lamb,
 ]
 
@@ -24,6 +26,7 @@ IPEX_FUSED_OPTIMIZER_LIST_XPU = [
 OPTIMIZER_FUSED_STEP_MAPPING_CPU = {
     torch.optim.SGD: sgd_step,
     torch.optim.Adagrad: adagrad_step,
+    MLPerfAdagrad: adagrad_step,
     torch.optim.Adam: adam_step,
     Lamb: lamb_step,
 }
@@ -146,10 +149,10 @@ def patch_load_state_dict(optimizer):
                         for state_key, state_value in state.items():
                             if isinstance(state_value, torch.Tensor) and state_value.size() == public_p.size():
                                 # We have an assumption here that any tensor's in parameter state, if they
-                                # have same shapes with the parameter, they should share same layout with 
+                                # have same shapes with the parameter, they should share same layout with
                                 # the parameter. Thus we need pack the state as we did to parameters.
                                 pack_state(state, state_key, state_value, attr)
-      
+
     def original_load_state_dict_without_state_cast(self, state_dict):
         r"""Loads the optimizer state.
 
@@ -163,7 +166,7 @@ def patch_load_state_dict(optimizer):
         bfloat16 which will loss accuracy
 
         The original code:
-    
+
             def cast(param, value, key=None):
                 if isinstance(value, torch.Tensor):
                     # Floating-point types are a bit special here. They are the only ones
@@ -236,7 +239,7 @@ def patch_load_state_dict(optimizer):
         param_groups = [
             update_group(g, ng) for g, ng in zip(groups, saved_groups)]
         self.__setstate__({'state': state, 'param_groups': param_groups})
-  
+
     def load_state_dict(self, state_dict):
         original_load_state_dict_without_state_cast(self, state_dict)
         repack(self)
@@ -291,7 +294,7 @@ def pack_optimizer_params_and_states(optimizer, param_pair, attrs, pack_dtype):
                             for state_key, state_value in state.items():
                                 if isinstance(state_value, torch.Tensor) and state_value.size() == p.size():
                                     # We have an assumption here that any tensor's in parameter state, if they
-                                    # have same shapes with the parameter, they should share same layout with 
+                                    # have same shapes with the parameter, they should share same layout with
                                     # the parameter. Thus we need pack the state as we did to parameters.
                                     pack_state(state, state_key, state_value, attr)
 
@@ -309,7 +312,7 @@ def patch_state_dict(optimizer):
                 for state_key, state_value in v2.items():
                     if isinstance(state_value, torch.Tensor) and state_value.shape == k1.shape:
                         # We have an assumption  here that any tensor's in parameter state, if they
-                        # have same shapes with the parameter, they should share same layout with 
+                        # have same shapes with the parameter, they should share same layout with
                         # the parameter. Thus we need unpack the state as we did to parameters.
                         if 'op' in params_attr:
                             # Secondly, unpack releated states
diff --git a/requirements.txt b/requirements.txt
index b86ac6e18..3c2e0b09d 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -5,4 +5,3 @@ wheel>=0.36
 setuptools>=50.0
 packaging
 psutil
-torchdynamo==1.13
diff --git a/scripts/compile_bundle.sh b/scripts/compile_bundle.sh
index 07b27667a..0aa4489b7 100644
--- a/scripts/compile_bundle.sh
+++ b/scripts/compile_bundle.sh
@@ -38,9 +38,9 @@ git checkout ${VER_LLVM}
 git submodule sync
 git submodule update --init --recursive
 cd ../intel-extension-for-pytorch
-git checkout ${VER_IPEX}
-git submodule sync
-git submodule update --init --recursive
+#git checkout ${VER_IPEX}
+#git submodule sync
+#git submodule update --init --recursive
 
 # Install dependencies
 python -m pip install cmake
diff --git a/scripts/test_embeddingbags_adagrad.py b/scripts/test_embeddingbags_adagrad.py
new file mode 100644
index 000000000..2f0f100bf
--- /dev/null
+++ b/scripts/test_embeddingbags_adagrad.py
@@ -0,0 +1,160 @@
+import torch
+from torch import nn
+import numpy as np
+import pickle
+from copy import deepcopy
+import intel_extension_for_pytorch as ipex
+from intel_extension_for_pytorch.nn.modules import MergedEmbeddingBagWithAdagrad
+from torch.autograd import Function
+import time
+
+def get_data(ln, multihot_size):
+    bs = 65536
+    index = []
+    offset = []
+    for i in range(26):
+        index.append(torch.randint(0, ln[i], (bs*multihot_size[i], ) , dtype=torch.int32))
+        offset.append(multihot_size[i] * torch.arange(bs+1, dtype=torch.int32))
+    return index, offset
+
+def get_refe_model(ln):
+    merged_emb_refe = MergedEmbeddingBag(128, ln)
+    for i in range(26):
+        merged_emb_refe.embedding_bags[f'cat{i}'].weight.data = merged_emb_refe.embedding_bags[f'cat{i}'].weight.data.to(torch.bfloat16).to(torch.float)
+    return merged_emb_refe
+
+class MergedEmbeddingBag(nn.Module):
+    def __init__(self,
+                 embedding_dim: int,
+                 num_embeddings_pool):
+        super().__init__()
+        self._embedding_dim = embedding_dim
+        self._num_embeddings = len(num_embeddings_pool)
+        self.embedding_bags: nn.ModuleDict = nn.ModuleDict()
+        for i, num_embeddings in enumerate(num_embeddings_pool):
+            embedding_name = f'cat{i}'
+            print(f"create embeddingBag{i}: {num_embeddings}")
+            W = np.random.uniform(
+                low=-np.sqrt(1 / num_embeddings),
+                high=np.sqrt(1 / num_embeddings),
+                size=(num_embeddings, embedding_dim)).astype(np.float32)
+            self.embedding_bags[embedding_name] = nn.EmbeddingBag(
+                num_embeddings=num_embeddings,
+                embedding_dim=embedding_dim,
+                _weight=torch.tensor(W, requires_grad=True),
+                include_last_offset=True,
+                mode='sum',
+                sparse=False)
+        return
+
+    def forward(self, values, offsets) -> torch.Tensor:
+        pooled_embeddings = []
+        for i, embedding_bag in enumerate(self.embedding_bags.values()):
+            res = embedding_bag(input=values[i].to(torch.long), offsets=offsets[i].to(torch.long))
+            pooled_embeddings.append(res)
+        data = torch.cat(pooled_embeddings, dim=1).reshape(-1, self._num_embeddings, self._embedding_dim)
+        return data
+
+def test_mlperf_merged_embeddingbag_forward(ln, multihot_size):
+    dist = lambda x, y: np.sum(np.isclose(x.detach().numpy(), y.detach().numpy(), 1e-3, 1e-3) != True)
+    merge_emb_ref = get_refe_model(ln)
+    merged_emb_test_weights = [merge_emb_ref.embedding_bags[f'cat{i}'].weight.data.clone().detach() for i in range(26)]
+    merged_emb_test = MergedEmbeddingBagWithAdagrad(merged_emb_test_weights, multihot_size, ln, lr=0.004, eps=None)
+    index, offset = get_data(ln, multihot_size)
+
+    with torch.no_grad():
+        # ipex_output = torch.ops.torch_ipex.mlperf_embedding_forward(index, offset, merge_emb_test_weights, multihot_size)
+        ipex_output = merged_emb_test(index, offset)
+        ref_output = merge_emb_ref(index, offset)
+
+    cnt_output_not_inclose = dist(ipex_output, ref_output)
+    assert(cnt_output_not_inclose == 0)
+    print("fwd1 pass")
+
+    ipex_output = merged_emb_test(index, offset)
+    ref_output = merge_emb_ref(index, offset)
+    cnt_output_not_inclose = dist(ipex_output, ref_output)
+    assert(cnt_output_not_inclose == 0)
+    print("fwd2 pass")
+
+def test_merged_embeddingbag_adagrad_func(ln, multihot_size, lr=0.004, eps=1e-8, iter=10):
+    dist = lambda x, y: np.sum(np.isclose(x.detach().numpy(), y.detach().numpy(), 1e-3, 1e-3) != True)
+    dist2 = lambda x, y: np.sum(np.isclose(x.detach().numpy(), y.detach().numpy(), 5e-2, 1e-2) != True)
+
+    merged_emb_refe = get_refe_model(ln)
+    merged_emb_test_weights = [merged_emb_refe.embedding_bags[f'cat{i}'].weight.data.clone().detach().requires_grad_() for i in range(26)]
+
+    # gen data
+    diffres_pool = [torch.randn([65536, 26, 128]).to(torch.bfloat16).to(torch.float) for i in range(iter)]
+    index_pool = []
+    offset_pool = []
+    test_outputs = []
+    refe_outputs = []
+    for i in range(iter):
+        index, offset = get_data(ln, multihot_size)
+        index_pool.append(index)
+        offset_pool.append(offset)
+
+    # refe [
+    opt_refe = torch.optim.Adagrad(merged_emb_refe.parameters(), lr=lr, eps=eps)
+    merged_emb_refe, opt_refe = ipex.optimize(merged_emb_refe, optimizer=opt_refe, dtype=torch.bfloat16, inplace=True)
+    for i in range(iter):
+        print(i)
+
+        index, offset, diffres = index_pool[i], offset_pool[i], diffres_pool[i]
+
+        if 0:
+            with torch.no_grad():
+                refe_output = merged_emb_refe(index, offset)
+                refe_outputs.append(refe_output.clone().detach())
+        else:
+            refe_output = merged_emb_refe(index, offset)
+            refe_outputs.append(refe_output.clone().detach())
+
+            opt_refe.zero_grad()
+            refe_output.backward(diffres.to(torch.bfloat16))
+            opt_refe.step()
+
+    # del opt_refe.state
+    # refe ]
+
+    # test [
+    merged_emb_test = MergedEmbeddingBagWithAdagrad(merged_emb_test_weights, multihot_size, ln, lr=lr, eps=eps)
+    opt_test = torch.optim.Adagrad(merged_emb_test.parameters(), lr=lr, eps=eps)
+    hessian = [torch.zeros(opt_test.state[merged_emb_test.weights[i]]['sum'].shape) for i in range(len(merged_emb_test.weights))]
+    merged_emb_test.set_hessian(hessian)
+
+    for i in range(iter):
+        print(i)
+
+        index, offset, diffres = index_pool[i], offset_pool[i], diffres_pool[i]
+
+        if 0:
+            with torch.no_grad():
+                test_output = merged_emb_test(index, offset)
+                test_outputs.append(test_output.clone().detach())
+        else:
+            test_output = merged_emb_test(index, offset)
+            test_outputs.append(test_output.clone().detach())
+
+            opt_test.zero_grad()
+            test_output.backward(diffres.to(torch.bfloat16))
+            opt_test.step()
+
+    # del opt_test.state
+    # test ]
+
+    merged_emb_refe_weights = [merged_emb_refe.embedding_bags[f'cat{i}'].weight for i in range(len(merged_emb_test.weights))]
+    hessian_refe = [opt_refe.state[merged_emb_refe.embedding_bags[f'cat{i}'].weight]['sum'] for i in range(len(merged_emb_test.weights))]
+    for i in range(26): print(f"hessian_{i} fail: ", dist(hessian[i], hessian_refe[i]), ' / ', hessian[i].numel())
+    for i in range(26): print(f"weight_{i} fail: ", dist(merged_emb_test_weights[i], merged_emb_refe_weights[i]), ' / ', merged_emb_test_weights[i].numel())
+    for i in range(iter): print(f"out_{i} fail: ", dist(test_outputs[i], refe_outputs[i]), ' / ', refe_outputs[i].numel())
+
+def main():
+    ln = [40000, 3906, 17295, 7424, 20265, 3, 7122, 1543, 63, 40000, 30679, 40528, 10, 2209, 11938, 155, 4, 976, 14, 40000, 40000, 40000, 5901, 12973, 108, 36]
+    multihot_size = [3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1]
+    test_mlperf_merged_embeddingbag_forward(ln, multihot_size)
+    test_merged_embeddingbag_adagrad_func(ln, multihot_size, iter=1)
+
+if __name__ == "__main__":
+    main()
\ No newline at end of file
diff --git a/scripts/test_embeddingbags_adagrad2.py b/scripts/test_embeddingbags_adagrad2.py
new file mode 100644
index 000000000..ffd9ae78d
--- /dev/null
+++ b/scripts/test_embeddingbags_adagrad2.py
@@ -0,0 +1,153 @@
+import torch
+from torch import nn
+import numpy as np
+import pickle
+from copy import deepcopy
+import intel_extension_for_pytorch as ipex
+from intel_extension_for_pytorch.nn.modules import MergedEmbeddingBagWithAdagrad
+from torch.autograd import Function
+from intel_extension_for_pytorch.optim import MLPerfAdagrad
+import time
+import numpy as np
+
+class MergedEmbeddingBag(nn.Module):
+    def __init__(self, wgts, emb_len, emb_dim):
+        super(MergedEmbeddingBag, self).__init__()
+        self._embbags = nn.Sequential(*[nn.EmbeddingBag(
+            num_embeddings=l,
+            embedding_dim=emb_dim,
+            _weight=w,
+            include_last_offset=True,
+            mode="sum",
+            sparse=False) for (w, l) in zip(wgts, emb_len)])
+        self.emb_dim = emb_dim
+        self.num_emb = len(wgts)
+
+    def forward(self, index, offset):
+        res = []
+        for i, (idx, off) in enumerate(zip(index, offset)):
+            res.append(self._embbags[i](idx.long(), off.long()))
+        b = offset[0].shape[0] - 1
+        res = torch.cat(res, dim=1).reshape(b, self.num_emb, self.emb_dim)
+        return res
+
+def nearclose(a, b, atol=1e-3, rtol=1e-3):
+    return np.sum(np.isclose(a, b, atol=atol, rtol=rtol)) / a.size > 0.999
+
+def get_relative_error(x, y, atol=1e-3, rtol=1e-3):
+    not_fin = ~np.isclose(x, y, atol=atol, rtol=rtol)
+    if np.sum(not_fin) == 0:
+        return None
+    res = (np.abs(x - y)[not_fin] - atol) / np.abs(y)[not_fin]
+    return res
+
+def test_forward(seed, batch_size, emb_len, emb_hot):
+    np.random.seed(seed)
+    emb_dim = 128
+    emb_wgt_np = [np.random.uniform(-1, 1, size=(n, emb_dim)).astype(np.float32)
+                  for n in emb_len]
+    emb_wgt_test = [torch.tensor(w.copy()).to(torch.bfloat16).to(torch.float) for w in emb_wgt_np]
+    emb_wgt_refe = [torch.tensor(w.copy()).to(torch.bfloat16).to(torch.float) for w in emb_wgt_np]
+    # print(f'rank {my_rank}', emb_wgt_np)
+    index_np = [np.minimum(
+        np.random.binomial(
+            n - 1, 0.3, size=(batch_size * h)).astype(np.int32),
+        n - 1) for n, h in zip(emb_len, emb_hot)]
+    # print(index_np)
+    offset_np = [
+        np.arange(0, (batch_size+1) * h, h).astype(np.int32)
+        for h in emb_hot]
+    index_test = [torch.tensor(i.copy()) for i in index_np]
+    offset_test = [torch.tensor(o.copy()) for o in offset_np]
+    index_refe = [torch.tensor(i.copy()) for i in index_np]
+    offset_refe = [torch.tensor(o.copy()) for o in offset_np]
+
+    test_model = MergedEmbeddingBagWithAdagrad(emb_wgt_test, emb_hot, emb_len)
+    test_model.eval()
+    refe_model = MergedEmbeddingBag(emb_wgt_refe, emb_len, emb_dim)
+    refe_model.eval()
+    refe_model = ipex.optimize(refe_model, dtype=torch.bfloat16)
+    with torch.no_grad():
+        test_y = test_model(index_test, offset_test)
+        refe_y = refe_model(index_refe, offset_refe).bfloat16()
+
+    test_y_np = test_y.float().detach().numpy()
+    refe_y_np = refe_y.float().detach().numpy()
+    assert np.array_equal(test_y_np, refe_y_np)
+
+def test_backward(seed, batch_size, emb_len, emb_hot, num_iter=8):
+    lr = float(0.004)
+    eps = float(1e-8)
+    np.random.seed(seed)
+    emb_dim = 128
+    emb_wgt_np = [np.random.uniform(-1, 1, size=(n, emb_dim)).astype(np.float32)
+                  for n in emb_len]
+    emb_wgt_test = [torch.tensor(w.copy()).to(torch.bfloat16).to(torch.float) for w in emb_wgt_np]
+    emb_wgt_refe = [torch.tensor(w.copy()).to(torch.bfloat16).to(torch.float) for w in emb_wgt_np]
+    # emb_weight_ori = [torch.tensor(w.copy()).to(torch.bfloat16).to(torch.float) for w in emb_wgt_np]
+    # print(f'rank {my_rank}', emb_wgt_np)
+
+    test_model = MergedEmbeddingBagWithAdagrad(emb_wgt_test, emb_hot, emb_len)
+    test_opt = MLPerfAdagrad(test_model.parameters(), lr=lr, eps=eps)
+    test_hessian = test_model._hessian
+    test_weights = test_model._weights
+    test_model.set_optimizer(test_opt)
+
+    refe_model = MergedEmbeddingBag(emb_wgt_refe, emb_len, emb_dim);
+    refe_opt = torch.optim.Adagrad(refe_model.parameters(), lr=lr, eps=eps)
+    refe_model, refe_opt = ipex.optimize(refe_model, optimizer=refe_opt, dtype=torch.bfloat16, inplace=True)
+    refe_hessian = [refe_opt.state[refe_model._embbags[i].weight]['sum'] for i in range(len(refe_model._embbags))]
+    refe_weights = [refe_model._embbags[i].weight for i in range(len(refe_model._embbags))]
+
+    for _ in range(num_iter):
+        index_np = [np.minimum(
+            np.random.binomial(
+                n - 1, 0.3, size=(batch_size * h)).astype(np.int32),
+            n - 1) for n, h in zip(emb_len, emb_hot)]
+        # print(index_np)
+        offset_np = [
+            np.arange(0, (batch_size+1) * h, h).astype(np.int32)
+            for h in emb_hot]
+        index_test = [torch.tensor(i.copy()) for i in index_np]
+        offset_test = [torch.tensor(o.copy()) for o in offset_np]
+        index_refe = [torch.tensor(i.copy()) for i in index_np]
+        offset_refe = [torch.tensor(o.copy()) for o in offset_np]
+
+        grad_np = np.random.uniform(-1, 1, size=(batch_size, len(emb_len), emb_dim)).astype(np.float32)
+        grad_test = torch.tensor(grad_np.copy()).to(torch.bfloat16)
+        grad_refe = torch.tensor(grad_np.copy()).to(torch.bfloat16).float()
+
+        test_y = test_model(index_test, offset_test)
+        refe_y = refe_model(index_refe, offset_refe).bfloat16()
+
+        test_y_np = test_y.float().detach().numpy()
+        refe_y_np = refe_y.float().detach().numpy()
+        assert np.array_equal(test_y_np, refe_y_np)
+        print("fwd pass")
+        test_opt.zero_grad()
+        test_y.backward(grad_test)
+        test_opt.step()
+
+        refe_opt.zero_grad()
+        refe_y.backward(grad_refe)
+        refe_opt.step()
+
+        for i in range(len(emb_len)):
+            test_hessian_np_i = test_hessian[i].numpy()
+            refe_hessian_np_i = refe_hessian[i].numpy()
+            assert np.array_equal(test_hessian_np_i, refe_hessian_np_i)
+        print(f"hessian pass")
+        for i in range(len(emb_len)):
+            test_weights_np_i = test_weights[i].detach().numpy()
+            refe_weights_np_i = refe_weights[i].detach().numpy()
+            assert nearclose(test_weights_np_i, refe_weights_np_i, rtol=5e-3)
+        print(f"weights pass")
+
+def main():
+    ln = [40000000,39060,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36]
+    multihot_size = [3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1]
+    test_forward(123, 65536, ln, multihot_size)
+    test_backward(123, 65536, ln, multihot_size)
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/test_freqperf.sh b/scripts/test_freqperf.sh
new file mode 100755
index 000000000..e49e38727
--- /dev/null
+++ b/scripts/test_freqperf.sh
@@ -0,0 +1,12 @@
+#!/bin/bash
+source ${CONDA_PREFIX}/lib/python3.9/site-packages/oneccl_bindings_for_pytorch/env/setvars.sh
+export LD_PRELOAD=${CONDA_PREFIX}/lib/libiomp5.so:${CONDA_PREFIX}/lib/libtcmalloc.so
+#export LD_PRELOAD=${CONDA_PREFIX}/lib/libtcmalloc.so
+#export LD_PRELOAD=${CONDA_PREFIX}/lib/libiomp5.so
+export KMP_AFFINITY=granularity='fine,compact,1,0'
+export KMP_BLOCKTIME=1
+export I_MPI_PIN_DOMAIN=[0xffffffffffffff,0xffffffffffffff00000000000000,]
+export CCL_WORKER_AFFINITY='112,113,114,115,168,169,170,171'
+export CCL_WORKER_COUNT=4
+
+OMP_NUM_THREADS=56 mpirun -n 2 -ppn 2 python -u test_mergfreqemb_perf.py
diff --git a/scripts/test_mergfreqemb.py b/scripts/test_mergfreqemb.py
new file mode 100644
index 000000000..da221d521
--- /dev/null
+++ b/scripts/test_mergfreqemb.py
@@ -0,0 +1,280 @@
+import os
+import torch
+from torch import nn
+import intel_extension_for_pytorch as ipex
+import torch.nn.parallel
+import torch.distributed as dist
+import oneccl_bindings_for_pytorch
+import numpy as np
+import pytest
+from typing import List
+from intel_extension_for_pytorch.nn.modules import MergFreqEmbeddingBag
+# from mlperf_mergedfreqembbag2 import MergFreqEmbeddingBag
+ADAGRAD_LR_DECAY = 0
+ADAGRAD_INIT_ACC = 0
+ADAGRAD_EPS = 1e-10
+WEIGHT_DECAY = 0
+LR_WARMUP_STEPS = 0
+LR_DECAY_START = 0
+LR_DECAY_STEPS = 0
+LEARNING_RATE = 15.0
+
+class MergedEmbeddingBag(nn.Module):
+    def __init__(self, wgts, emb_len, emb_dim):
+        super(MergedEmbeddingBag, self).__init__()
+        self._embbags = nn.Sequential(*[nn.EmbeddingBag(
+            num_embeddings=l,
+            embedding_dim=emb_dim,
+            _weight=torch.tensor(w.copy(), requires_grad=True),
+            mode="sum") for (w, l) in zip(wgts, emb_len)])
+        self.emb_dim = emb_dim
+        self.num_emb = len(wgts)
+
+    def forward(self, index, offset):
+        res = []
+        for i, (idx, off) in enumerate(zip(index, offset)):
+            res.append(self._embbags[i](idx, off))
+        b = offset[0].shape[0]
+        res = torch.cat(res, dim=1).reshape(b, self.num_emb, self.emb_dim)
+        return res
+
+def init_ccl():
+    os.environ['MASTER_ADDR'] = '127.0.0.1'
+    os.environ['MASTER_PORT'] = '29500'
+    os.environ['RANK'] = str(os.environ.get('PMI_RANK', 0))
+    os.environ['WORLD_SIZE'] = str(os.environ.get('PMI_SIZE', 1))
+    backend = 'ccl'
+    dist.init_process_group(backend)
+    my_rank = dist.get_rank()
+    my_size = dist.get_world_size()
+    print("my rank = %d  my size = %d" % (my_rank, my_size))
+    # a = [torch.randn(4 + my_rank, dtype=torch.float32) for i in range(my_size)]
+    # b = [torch.zeros(4 + i, dtype=torch.float32) for i in range(my_size)]
+    # dist.all_to_all(b, a)
+    return
+
+def nearlyclose(a, b, atol=1e-3, rtol=1e-3):
+    N = a.size
+    ratio = np.sum(np.isclose(a, b, atol, rtol)) / float(N)
+    print('close ratio', ratio)
+    return ratio > 0.99
+
+init_ccl()
+@pytest.mark.parametrize('seed',
+                          [
+                              123,
+                              123456,
+                              134567789
+                          ]
+                          )
+@pytest.mark.parametrize('batch_size,emb_len,emb_dim,emb_hot',
+                          [
+                              (2048, [4, 11], 128, [1, 1]),
+                              (8192, [91, 365], 128, [2, 3]),
+                              (16384, [123491, 40000], 128, [7, 9]),
+                              (16384, [40000, 123491], 128, [1, 27]),
+                              (16384, [123491, 400000], 128, [10, 100]),
+                          ]
+                          )
+@pytest.mark.parametrize('binomial_p',
+                          [
+                              0.0,
+                              0.3,
+                              0.5,
+                              0.7,
+                              1.0
+                          ])
+@pytest.mark.parametrize('dense_ratio',
+                          [
+                              0.0,
+                              0.1,
+                              0.3,
+                              0.5,
+                              0.7,
+                              1.0
+                          ])
+def test_freqemb_fwd(seed: int,
+                     batch_size: int, emb_len: List[int], emb_dim: int,
+                     emb_hot: List[int],
+                     binomial_p: float, dense_ratio: float):
+    my_rank = dist.get_rank()
+    my_size = dist.get_world_size()
+    np.random.seed(seed)
+    emb_wgt_np = [np.random.uniform(-1, 1, size=(n, emb_dim)).astype(np.float32)
+                   for n in emb_len]
+    # print(f'rank {my_rank}', emb_wgt_np)
+    index_np = [np.minimum(
+        np.random.binomial(
+            n - 1, binomial_p, size=(batch_size * h)).astype(np.int64),
+        n - 1) for n, h in zip(emb_len, emb_hot)]
+    # print(index_np)
+    offset_np = [
+        np.arange(0, batch_size * h, h).astype(np.int64)
+        for h in emb_hot]
+    index = [torch.tensor(i) for i in index_np]
+    offset = [torch.tensor(o) for o in offset_np]
+
+    refe_model = MergedEmbeddingBag(emb_wgt_np, emb_len, emb_dim);
+
+    test_model = MergFreqEmbeddingBag(emb_len, emb_hot, index,
+                                      [torch.tensor(w) for w in emb_wgt_np],
+                                      dense_ratio)
+    opt_test = torch.optim.Adagrad(test_model.parameters(),
+                                   lr=LEARNING_RATE,
+                                   lr_decay=ADAGRAD_LR_DECAY,
+                                   weight_decay=WEIGHT_DECAY,
+                                   initial_accumulator_value=ADAGRAD_INIT_ACC,
+                                   eps=ADAGRAD_EPS)
+    test_model.set_optimizer(opt_test)
+
+    y_refe = refe_model(index, offset)
+    y_test = test_model(index, offset)
+    # print(f'rank {my_rank}', y_test)
+    y_test_all = [torch.zeros_like(y_test, dtype=torch.float32) for i in range(my_size)]
+    dist.all_gather(y_test_all, y_test)
+
+    y_test = torch.cat(y_test_all)
+    y_refe_np = y_refe.cpu().detach().numpy()
+    y_test_np = y_test.cpu().detach().numpy()
+    assert np.allclose(y_refe_np, y_test_np, atol=1e-4, rtol=1e-4)
+    print("Done forward test")
+
+@pytest.mark.parametrize('seed',
+                          [
+                              123,
+                              123456,
+                              134567789
+                          ]
+                          )
+@pytest.mark.parametrize('batch_size,emb_len,emb_dim,emb_hot',
+                          [
+                              (2048, [4, 11], 128, [1, 1]),
+                              (8192, [91, 365], 128, [2, 3]),
+                              (16384, [123491, 40000], 128, [7, 9]),
+                              (16384, [40000, 123491], 128, [1, 27]),
+                              (16384, [123491, 400000], 128, [10, 100]),
+                          ]
+                          )
+@pytest.mark.parametrize('binomial_p',
+                          [
+                              0.0,
+                              0.3,
+                              0.5,
+                              0.7,
+                              1.0
+                          ])
+@pytest.mark.parametrize('dense_ratio',
+                          [
+                              0.0,
+                              0.1,
+                              0.3,
+                              0.5,
+                              0.7,
+                              1.0
+                          ])
+def test_freqemb_bwd(seed: int,
+                     batch_size: int, emb_len: List[int], emb_dim: int, emb_hot: List[int],
+                     binomial_p: float, dense_ratio: float):
+    my_rank = dist.get_rank()
+    my_size = dist.get_world_size()
+    np.random.seed(seed)
+    emb_wgt_np = [np.random.uniform(-1, 1, size=(n, emb_dim)).astype(np.float32)
+                  for n in emb_len]
+    num_emb = len(emb_len)
+    grad_np = np.random.uniform(-1, 1, size=(batch_size, num_emb, emb_dim)).astype(np.float32)
+    # print(f'rank {my_rank}', emb_wgt_np)
+    index_np = [np.minimum(
+        np.random.binomial(
+            n - 1, binomial_p, size=(batch_size * h)).astype(np.int64),
+        n - 1) for n, h in zip(emb_len, emb_hot)]
+    # print(index_np)
+    offset_np = [np.arange(0, batch_size * h, h).astype(np.int64)
+                 for h in emb_hot]
+
+    index_test = [torch.tensor(i.copy()) for i in index_np]
+    offset_test = [torch.tensor(o.copy()) for o in offset_np]
+    grad_test = torch.tensor(grad_np.copy())
+
+    index_refe = [torch.tensor(i.copy()) for i in index_np]
+    offset_refe = [torch.tensor(o.copy()) for o in offset_np]
+    grad_refe = torch.tensor(grad_np.copy())
+
+    refe_model = MergedEmbeddingBag(emb_wgt_np, emb_len, emb_dim)
+
+    test_model = MergFreqEmbeddingBag(emb_len, emb_hot, index_test,
+                                      [torch.tensor(w.copy()) for w in emb_wgt_np],
+                                      dense_ratio)
+    opt_test = torch.optim.Adagrad(test_model.parameters(),
+                                   lr=LEARNING_RATE,
+                                   lr_decay=ADAGRAD_LR_DECAY,
+                                   weight_decay=WEIGHT_DECAY,
+                                   initial_accumulator_value=ADAGRAD_INIT_ACC,
+                                   eps=ADAGRAD_EPS)
+    opt_refe = torch.optim.Adagrad(refe_model.parameters(),
+                                   lr=LEARNING_RATE,
+                                   lr_decay=ADAGRAD_LR_DECAY,
+                                   weight_decay=WEIGHT_DECAY,
+                                   initial_accumulator_value=ADAGRAD_INIT_ACC,
+                                   eps=ADAGRAD_EPS)
+    test_model.set_optimizer(opt_test)
+
+    lbatch = batch_size // my_size
+    for i in range(4):
+        index_np = [np.minimum(
+            np.random.binomial(
+                n - 1, binomial_p, size=(batch_size * h)).astype(np.int64),
+            n - 1) for n, h in zip(emb_len, emb_hot)]
+        offset_np = [np.arange(0, batch_size * h, h).astype(np.int64)
+                     for h in emb_hot]
+        grad_np = np.random.uniform(-1, 1, size=(batch_size, num_emb, emb_dim)).astype(np.float32)
+
+        index_test = [torch.tensor(i.copy()) for i in index_np]
+        offset_test = [torch.tensor(o.copy()) for o in offset_np]
+        grad_test = torch.tensor(grad_np.copy())
+
+        index_refe = [torch.tensor(i.copy()) for i in index_np]
+        offset_refe = [torch.tensor(o.copy()) for o in offset_np]
+        grad_refe = torch.tensor(grad_np.copy())
+
+        y_refe = refe_model(index_refe, offset_refe)
+        y_test = test_model(index_test, offset_test)
+
+        opt_refe.zero_grad()
+        opt_test.zero_grad()
+
+        y_refe.backward(grad_refe)
+        y_test.backward(grad_test[my_rank * lbatch: (my_rank + 1) * lbatch].clone())
+        opt_refe.step()
+        opt_test.step()
+        all_emb_len = np.sum(emb_len)
+        test_weight = torch.zeros((all_emb_len, emb_dim), dtype=torch.float32)
+        sparse_ln = [(all_emb_len // my_size + 1) if i < all_emb_len % my_size else (all_emb_len // my_size) for i in range(my_size)]
+        wgts = [torch.zeros((r, emb_dim), dtype=torch.float32)
+                for r in sparse_ln]
+        dist.all_gather(wgts, test_model._sparse_weight.data)
+        for j in range(my_size):
+            test_weight[j::my_size, :] = wgts[j]
+        for i, j in enumerate(test_model._lookup):
+            if j >= 0:
+                test_weight[i] = test_model._dense_weight[j]
+        test_weight_np = test_weight.detach().cpu().numpy()
+        refe_weight = torch.cat([e.weight.data for e in refe_model._embbags])
+        refe_weight_np = refe_weight
+        assert nearlyclose(test_weight_np, refe_weight_np, atol=1e-4, rtol=1e-4)
+    return
+
+if __name__ == '__main__':
+    test_freqemb_fwd(123, 16, [5, 11], 128, [2, 3], 0.3, 0.0)
+    test_freqemb_fwd(123, 16, [5, 11], 128, [1, 2], 0.3, 0.1)
+    test_freqemb_fwd(123, 16, [100, 300], 128, [6, 11], 0.3, 0.3)
+    test_freqemb_fwd(123, 16, [100, 300], 128, [1, 3], 0.3, 0.5)
+    test_freqemb_fwd(123, 16, [1003, 4993], 128, [7, 6], 0.3, 0.7)
+    test_freqemb_fwd(123, 16, [1033, 40993, 333], 128, [1, 2, 4], 0.3, 0.9)
+    test_freqemb_fwd(123, 16, [13, 9340, 233, 39282], 128, [3, 2, 1, 3], 0.3, 1.0)
+    test_freqemb_bwd(123, 16, [5, 11], 128, [2, 3], 0.3, 0.0)
+    test_freqemb_bwd(123, 16, [5, 11], 128, [1, 2], 0.3, 0.1)
+    test_freqemb_bwd(123, 16, [100, 300], 128, [6, 11], 0.3, 0.3)
+    test_freqemb_bwd(123, 16, [100, 300], 128, [1, 3], 0.3, 0.5)
+    test_freqemb_bwd(123, 16, [1003, 4993], 128, [7, 6], 0.3, 0.7)
+    test_freqemb_bwd(123, 16, [1033, 40993, 333], 128, [1, 2, 4], 0.3, 0.9)
+    test_freqemb_bwd(123, 16, [13, 9340, 233, 39282], 128, [3, 2, 1, 3], 0.3, 1.0)
diff --git a/scripts/test_mergfreqemb2.py b/scripts/test_mergfreqemb2.py
new file mode 100644
index 000000000..4e8776659
--- /dev/null
+++ b/scripts/test_mergfreqemb2.py
@@ -0,0 +1,280 @@
+import os
+import torch
+from torch import nn
+import intel_extension_for_pytorch as ipex
+import torch.nn.parallel
+import torch.distributed as dist
+import oneccl_bindings_for_pytorch
+import numpy as np
+import pytest
+from typing import List
+from intel_extension_for_pytorch.nn.modules import MergFreqEmbeddingBag
+# from mlperf_mergedfreqembbag2 import MergFreqEmbeddingBag
+ADAGRAD_LR_DECAY = 0
+ADAGRAD_INIT_ACC = 0
+ADAGRAD_EPS = 1e-10
+WEIGHT_DECAY = 0
+LR_WARMUP_STEPS = 0
+LR_DECAY_START = 0
+LR_DECAY_STEPS = 0
+LEARNING_RATE = 15.0
+
+class MergedEmbeddingBag(nn.Module):
+    def __init__(self, wgts, emb_len, emb_dim):
+        super(MergedEmbeddingBag, self).__init__()
+        self._embbags = nn.Sequential(*[nn.EmbeddingBag(
+            num_embeddings=l,
+            embedding_dim=emb_dim,
+            _weight=torch.tensor(w.copy(), requires_grad=True),
+            mode="sum") for (w, l) in zip(wgts, emb_len)])
+        self.emb_dim = emb_dim
+        self.num_emb = len(wgts)
+
+    def forward(self, index, offset):
+        res = []
+        for i, (idx, off) in enumerate(zip(index, offset)):
+            res.append(self._embbags[i](idx, off))
+        b = offset[0].shape[0]
+        res = torch.cat(res, dim=1).reshape(b, self.num_emb, self.emb_dim)
+        return res
+
+def init_ccl():
+    os.environ['MASTER_ADDR'] = '127.0.0.1'
+    os.environ['MASTER_PORT'] = '29530'
+    os.environ['RANK'] = str(os.environ.get('PMI_RANK', 0))
+    os.environ['WORLD_SIZE'] = str(os.environ.get('PMI_SIZE', 1))
+    backend = 'ccl'
+    dist.init_process_group(backend)
+    my_rank = dist.get_rank()
+    my_size = dist.get_world_size()
+    print("my rank = %d  my size = %d" % (my_rank, my_size))
+    # a = [torch.randn(4 + my_rank, dtype=torch.float32) for i in range(my_size)]
+    # b = [torch.zeros(4 + i, dtype=torch.float32) for i in range(my_size)]
+    # dist.all_to_all(b, a)
+    return
+
+def nearlyclose(a, b, atol=1e-3, rtol=1e-3):
+    N = a.size
+    ratio = np.sum(np.isclose(a, b, atol, rtol)) / float(N)
+    print('close ratio', ratio)
+    return ratio > 0.99
+
+init_ccl()
+@pytest.mark.parametrize('seed',
+                          [
+                              123,
+                              123456,
+                              134567789
+                          ]
+                          )
+@pytest.mark.parametrize('batch_size,emb_len,emb_dim,emb_hot',
+                          [
+                              (2048, [4, 11], 128, [1, 1]),
+                              (8192, [91, 365], 128, [2, 3]),
+                              (16384, [123491, 40000], 128, [7, 9]),
+                              (16384, [40000, 123491], 128, [1, 27]),
+                              (16384, [123491, 400000], 128, [10, 100]),
+                          ]
+                          )
+@pytest.mark.parametrize('binomial_p',
+                          [
+                              0.0,
+                              0.3,
+                              0.5,
+                              0.7,
+                              1.0
+                          ])
+@pytest.mark.parametrize('dense_ratio',
+                          [
+                              0.0,
+                              0.1,
+                              0.3,
+                              0.5,
+                              0.7,
+                              1.0
+                          ])
+def test_freqemb_fwd(seed: int,
+                     batch_size: int, emb_len: List[int], emb_dim: int,
+                     emb_hot: List[int],
+                     binomial_p: float, dense_ratio: float):
+    my_rank = dist.get_rank()
+    my_size = dist.get_world_size()
+    np.random.seed(seed)
+    emb_wgt_np = [np.random.uniform(-1, 1, size=(n, emb_dim)).astype(np.float32)
+                   for n in emb_len]
+    # print(f'rank {my_rank}', emb_wgt_np)
+    index_np = [np.minimum(
+        np.random.binomial(
+            n - 1, binomial_p, size=(batch_size * h)).astype(np.int64),
+        n - 1) for n, h in zip(emb_len, emb_hot)]
+    # print(index_np)
+    offset_np = [
+        np.arange(0, batch_size * h, h).astype(np.int64)
+        for h in emb_hot]
+    index = [torch.tensor(i) for i in index_np]
+    offset = [torch.tensor(o) for o in offset_np]
+
+    refe_model = MergedEmbeddingBag(emb_wgt_np, emb_len, emb_dim);
+
+    test_model = MergFreqEmbeddingBag(emb_len, emb_hot, index,
+                                      [torch.tensor(w) for w in emb_wgt_np],
+                                      dense_ratio)
+    opt_test = torch.optim.Adagrad(test_model.parameters(),
+                                   lr=LEARNING_RATE,
+                                   lr_decay=ADAGRAD_LR_DECAY,
+                                   weight_decay=WEIGHT_DECAY,
+                                   initial_accumulator_value=ADAGRAD_INIT_ACC,
+                                   eps=ADAGRAD_EPS)
+    test_model.set_optimizer(opt_test)
+
+    y_refe = refe_model(index, offset)
+    y_test = test_model(index, offset)
+    # print(f'rank {my_rank}', y_test)
+    y_test_all = [torch.zeros_like(y_test, dtype=torch.float32) for i in range(my_size)]
+    dist.all_gather(y_test_all, y_test)
+
+    y_test = torch.cat(y_test_all)
+    y_refe_np = y_refe.cpu().detach().numpy()
+    y_test_np = y_test.cpu().detach().numpy()
+    assert np.allclose(y_refe_np, y_test_np, atol=1e-4, rtol=1e-4)
+    print("Done forward test")
+
+@pytest.mark.parametrize('seed',
+                          [
+                              123,
+                              123456,
+                              134567789
+                          ]
+                          )
+@pytest.mark.parametrize('batch_size,emb_len,emb_dim,emb_hot',
+                          [
+                              (2048, [4, 11], 128, [1, 1]),
+                              (8192, [91, 365], 128, [2, 3]),
+                              (16384, [123491, 40000], 128, [7, 9]),
+                              (16384, [40000, 123491], 128, [1, 27]),
+                              (16384, [123491, 400000], 128, [10, 100]),
+                          ]
+                          )
+@pytest.mark.parametrize('binomial_p',
+                          [
+                              0.0,
+                              0.3,
+                              0.5,
+                              0.7,
+                              1.0
+                          ])
+@pytest.mark.parametrize('dense_ratio',
+                          [
+                              0.0,
+                              0.1,
+                              0.3,
+                              0.5,
+                              0.7,
+                              1.0
+                          ])
+def test_freqemb_bwd(seed: int,
+                     batch_size: int, emb_len: List[int], emb_dim: int, emb_hot: List[int],
+                     binomial_p: float, dense_ratio: float):
+    my_rank = dist.get_rank()
+    my_size = dist.get_world_size()
+    np.random.seed(seed)
+    emb_wgt_np = [np.random.uniform(-1, 1, size=(n, emb_dim)).astype(np.float32)
+                  for n in emb_len]
+    num_emb = len(emb_len)
+    grad_np = np.random.uniform(-1, 1, size=(batch_size, num_emb, emb_dim)).astype(np.float32)
+    # print(f'rank {my_rank}', emb_wgt_np)
+    index_np = [np.minimum(
+        np.random.binomial(
+            n - 1, binomial_p, size=(batch_size * h)).astype(np.int64),
+        n - 1) for n, h in zip(emb_len, emb_hot)]
+    # print(index_np)
+    offset_np = [np.arange(0, batch_size * h, h).astype(np.int64)
+                 for h in emb_hot]
+
+    index_test = [torch.tensor(i.copy()) for i in index_np]
+    offset_test = [torch.tensor(o.copy()) for o in offset_np]
+    grad_test = torch.tensor(grad_np.copy())
+
+    index_refe = [torch.tensor(i.copy()) for i in index_np]
+    offset_refe = [torch.tensor(o.copy()) for o in offset_np]
+    grad_refe = torch.tensor(grad_np.copy())
+
+    refe_model = MergedEmbeddingBag(emb_wgt_np, emb_len, emb_dim)
+
+    test_model = MergFreqEmbeddingBag(emb_len, emb_hot, index_test,
+                                      [torch.tensor(w.copy()) for w in emb_wgt_np],
+                                      dense_ratio)
+    opt_test = torch.optim.Adagrad(test_model.parameters(),
+                                   lr=LEARNING_RATE,
+                                   lr_decay=ADAGRAD_LR_DECAY,
+                                   weight_decay=WEIGHT_DECAY,
+                                   initial_accumulator_value=ADAGRAD_INIT_ACC,
+                                   eps=ADAGRAD_EPS)
+    opt_refe = torch.optim.Adagrad(refe_model.parameters(),
+                                   lr=LEARNING_RATE,
+                                   lr_decay=ADAGRAD_LR_DECAY,
+                                   weight_decay=WEIGHT_DECAY,
+                                   initial_accumulator_value=ADAGRAD_INIT_ACC,
+                                   eps=ADAGRAD_EPS)
+    test_model.set_optimizer(opt_test)
+
+    lbatch = batch_size // my_size
+    for i in range(4):
+        index_np = [np.minimum(
+            np.random.binomial(
+                n - 1, binomial_p, size=(batch_size * h)).astype(np.int64),
+            n - 1) for n, h in zip(emb_len, emb_hot)]
+        offset_np = [np.arange(0, batch_size * h, h).astype(np.int64)
+                     for h in emb_hot]
+        grad_np = np.random.uniform(-1, 1, size=(batch_size, num_emb, emb_dim)).astype(np.float32)
+
+        index_test = [torch.tensor(i.copy()) for i in index_np]
+        offset_test = [torch.tensor(o.copy()) for o in offset_np]
+        grad_test = torch.tensor(grad_np.copy())
+
+        index_refe = [torch.tensor(i.copy()) for i in index_np]
+        offset_refe = [torch.tensor(o.copy()) for o in offset_np]
+        grad_refe = torch.tensor(grad_np.copy())
+
+        y_refe = refe_model(index_refe, offset_refe)
+        y_test = test_model(index_test, offset_test)
+
+        opt_refe.zero_grad()
+        opt_test.zero_grad()
+
+        y_refe.backward(grad_refe)
+        y_test.backward(grad_test[my_rank * lbatch: (my_rank + 1) * lbatch].clone())
+        opt_refe.step()
+        opt_test.step()
+        all_emb_len = np.sum(emb_len)
+        test_weight = torch.zeros((all_emb_len, emb_dim), dtype=torch.float32)
+        sparse_ln = [(all_emb_len // my_size + 1) if i < all_emb_len % my_size else (all_emb_len // my_size) for i in range(my_size)]
+        wgts = [torch.zeros((r, emb_dim), dtype=torch.float32)
+                for r in sparse_ln]
+        dist.all_gather(wgts, test_model._sparse_weight.data)
+        for j in range(my_size):
+            test_weight[j::my_size, :] = wgts[j]
+        for i, j in enumerate(test_model._lookup):
+            if j >= 0:
+                test_weight[i] = test_model._dense_weight[j]
+        test_weight_np = test_weight.detach().cpu().numpy()
+        refe_weight = torch.cat([e.weight.data for e in refe_model._embbags])
+        refe_weight_np = refe_weight
+        assert nearlyclose(test_weight_np, refe_weight_np, atol=1e-4, rtol=1e-4)
+    return
+
+if __name__ == '__main__':
+    test_freqemb_fwd(123, 16, [5, 11], 128, [2, 3], 0.3, 0.0)
+    test_freqemb_fwd(123, 16, [5, 11], 128, [1, 2], 0.3, 0.1)
+    test_freqemb_fwd(123, 16, [100, 300], 128, [6, 11], 0.3, 0.3)
+    test_freqemb_fwd(123, 16, [100, 300], 128, [1, 3], 0.3, 0.5)
+    test_freqemb_fwd(123, 16, [1003, 4993], 128, [7, 6], 0.3, 0.7)
+    test_freqemb_fwd(123, 16, [1033, 40993, 333], 128, [1, 2, 4], 0.3, 0.9)
+    test_freqemb_fwd(123, 16, [13, 9340, 233, 39282], 128, [3, 2, 1, 3], 0.3, 1.0)
+    test_freqemb_bwd(123, 16, [5, 11], 128, [2, 3], 0.3, 0.0)
+    test_freqemb_bwd(123, 16, [5, 11], 128, [1, 2], 0.3, 0.1)
+    test_freqemb_bwd(123, 16, [100, 300], 128, [6, 11], 0.3, 0.3)
+    test_freqemb_bwd(123, 16, [100, 300], 128, [1, 3], 0.3, 0.5)
+    test_freqemb_bwd(123, 16, [1003, 4993], 128, [7, 6], 0.3, 0.7)
+    test_freqemb_bwd(123, 16, [1033, 40993, 333], 128, [1, 2, 4], 0.3, 0.9)
+    test_freqemb_bwd(123, 16, [13, 9340, 233, 39282], 128, [3, 2, 1, 3], 0.3, 1.0)
diff --git a/scripts/test_mergfreqemb_perf.py b/scripts/test_mergfreqemb_perf.py
new file mode 100644
index 000000000..e3eb379b0
--- /dev/null
+++ b/scripts/test_mergfreqemb_perf.py
@@ -0,0 +1,286 @@
+import os
+import torch
+from torch import nn
+import intel_extension_for_pytorch as ipex
+import torch.nn.parallel
+import torch.distributed as dist
+import oneccl_bindings_for_pytorch
+import numpy as np
+import pytest
+import time
+from typing import List
+from intel_extension_for_pytorch.nn.modules import MergFreqEmbeddingBag
+# from mlperf_mergedfreqembbag2 import MergFreqEmbeddingBag
+ADAGRAD_LR_DECAY = 0
+ADAGRAD_INIT_ACC = 0
+ADAGRAD_EPS = 1e-8
+WEIGHT_DECAY = 0
+LR_WARMUP_STEPS = 0
+LR_DECAY_START = 0
+LR_DECAY_STEPS = 0
+LEARNING_RATE = 15.0
+
+class MergedEmbeddingBag(nn.Module):
+    def __init__(self, wgts, emb_len, emb_dim):
+        super(MergedEmbeddingBag, self).__init__()
+        self._embbags = nn.Sequential(*[nn.EmbeddingBag(
+            num_embeddings=l,
+            embedding_dim=emb_dim,
+            _weight=torch.tensor(w.copy(), requires_grad=True),
+            mode="sum") for (w, l) in zip(wgts, emb_len)])
+        self.emb_dim = emb_dim
+        self.num_emb = len(wgts)
+
+    def forward(self, index, offset):
+        res = []
+        for i, (idx, off) in enumerate(zip(index, offset)):
+            res.append(self._embbags[i](idx, off))
+        b = offset[0].shape[0]
+        res = torch.cat(res, dim=1).reshape(b, self.num_emb, self.emb_dim)
+        return res
+
+def init_ccl():
+    os.environ['MASTER_ADDR'] = '127.0.0.1'
+    os.environ['MASTER_PORT'] = '29570'
+    os.environ['RANK'] = str(os.environ.get('PMI_RANK', 0))
+    os.environ['WORLD_SIZE'] = str(os.environ.get('PMI_SIZE', 1))
+    backend = 'ccl'
+    dist.init_process_group(backend)
+    my_rank = dist.get_rank()
+    my_size = dist.get_world_size()
+    print("my rank = %d  my size = %d" % (my_rank, my_size))
+    # a = [torch.randn(4 + my_rank, dtype=torch.float32) for i in range(my_size)]
+    # b = [torch.zeros(4 + i, dtype=torch.float32) for i in range(my_size)]
+    # dist.all_to_all(b, a)
+    return
+
+def nearlyclose(a, b, atol=1e-3, rtol=1e-3):
+    N = a.size
+    ratio = np.sum(np.isclose(a, b, atol, rtol)) / float(N)
+    return ratio > 0.99
+
+init_ccl()
+@pytest.mark.parametrize('seed',
+                          [
+                              123,
+                              123456,
+                              134567789
+                          ]
+                          )
+@pytest.mark.parametrize('batch_size,emb_len,emb_dim,emb_hot',
+                          [
+                              (2048, [4, 11], 128, [1, 1]),
+                              (8192, [91, 365], 128, [2, 3]),
+                              (16384, [123491, 40000], 128, [7, 9]),
+                              (16384, [40000, 123491], 128, [1, 27]),
+                              (16384, [123491, 400000], 128, [10, 100]),
+                          ]
+                          )
+@pytest.mark.parametrize('binomial_p',
+                          [
+                              0.0,
+                              0.3,
+                              0.5,
+                              0.7,
+                              1.0
+                          ])
+@pytest.mark.parametrize('dense_ratio',
+                          [
+                              0.0,
+                              0.1,
+                              0.3,
+                              0.5,
+                              0.7,
+                              1.0
+                          ])
+def test_freqemb_fwd(seed: int,
+                     batch_size: int, emb_len: List[int], emb_dim: int,
+                     emb_hot: List[int],
+                     binomial_p: float, dense_ratio: float):
+    my_rank = dist.get_rank()
+    my_size = dist.get_world_size()
+    np.random.seed(seed)
+    emb_wgt_np = [np.random.uniform(-1, 1, size=(n, emb_dim)).astype(np.float32)
+                   for n in emb_len]
+    # print(f'rank {my_rank}', emb_wgt_np)
+    index_np = [np.minimum(
+        np.random.binomial(
+            n - 1, binomial_p, size=(batch_size * h)).astype(np.int64),
+        n - 1) for n, h in zip(emb_len, emb_hot)]
+    # print(index_np)
+    offset_np = [
+        np.arange(0, batch_size * h, h).astype(np.int64)
+        for h in emb_hot]
+    index = [torch.tensor(i) for i in index_np]
+    offset = [torch.tensor(o) for o in offset_np]
+
+    refe_model = MergedEmbeddingBag(emb_wgt_np, emb_len, emb_dim);
+
+    test_model = MergFreqEmbeddingBag(emb_len, emb_hot, index,
+                                      [torch.tensor(w) for w in emb_wgt_np],
+                                      dense_ratio)
+    opt_test = torch.optim.Adagrad(test_model.parameters(),
+                                   lr=LEARNING_RATE,
+                                   lr_decay=ADAGRAD_LR_DECAY,
+                                   weight_decay=WEIGHT_DECAY,
+                                   initial_accumulator_value=ADAGRAD_INIT_ACC,
+                                   eps=ADAGRAD_EPS)
+    test_model.set_optimizer(opt_test)
+
+    y_refe = refe_model(index, offset)
+    y_test = test_model(index, offset)
+    # print(f'rank {my_rank}', y_test)
+    y_test_all = [torch.zeros_like(y_test, dtype=torch.float32) for i in range(my_size)]
+    dist.all_gather(y_test_all, y_test)
+
+    y_test = torch.cat(y_test_all)
+    y_refe_np = y_refe.cpu().detach().numpy()
+    y_test_np = y_test.cpu().detach().numpy()
+    assert np.allclose(y_refe_np, y_test_np, atol=1e-4, rtol=1e-4)
+    print("Done forward test")
+
+@pytest.mark.parametrize('seed',
+                          [
+                              123,
+                              123456,
+                              134567789
+                          ]
+                          )
+@pytest.mark.parametrize('batch_size,emb_len,emb_dim,emb_hot',
+                          [
+                              (2048, [4, 11], 128, [1, 1]),
+                              (8192, [91, 365], 128, [2, 3]),
+                              (16384, [123491, 40000], 128, [7, 9]),
+                              (16384, [40000, 123491], 128, [1, 27]),
+                              (16384, [123491, 400000], 128, [10, 100]),
+                          ]
+                          )
+@pytest.mark.parametrize('binomial_p',
+                          [
+                              0.0,
+                              0.3,
+                              0.5,
+                              0.7,
+                              1.0
+                          ])
+@pytest.mark.parametrize('dense_ratio',
+                          [
+                              0.0,
+                              0.1,
+                              0.3,
+                              0.5,
+                              0.7,
+                              1.0
+                          ])
+def test_freqemb_bwd(seed: int,
+                     batch_size: int, emb_len: List[int], emb_dim: int, emb_hot: List[int],
+                     binomial_p: float, dense_ratio: float):
+    my_rank = dist.get_rank()
+    my_size = dist.get_world_size()
+    np.random.seed(seed)
+    emb_wgt_np = [np.random.uniform(-1, 1, size=(n, emb_dim)).astype(np.float32)
+                  for n in emb_len]
+    num_emb = len(emb_len)
+    grad_np = np.random.uniform(-1, 1, size=(batch_size, num_emb, emb_dim)).astype(np.float32)
+    # print(f'rank {my_rank}', emb_wgt_np)
+    index_np = [np.minimum(
+        np.random.binomial(
+            n - 1, binomial_p, size=(batch_size * h)).astype(np.int64),
+        n - 1) for n, h in zip(emb_len, emb_hot)]
+    # print(index_np)
+    offset_np = [np.arange(0, batch_size * h, h).astype(np.int64)
+                 for h in emb_hot]
+
+    index_test = [torch.tensor(i.copy()) for i in index_np]
+    offset_test = [torch.tensor(o.copy()) for o in offset_np]
+    grad_test = torch.tensor(grad_np.copy())
+
+    index_refe = [torch.tensor(i.copy()) for i in index_np]
+    offset_refe = [torch.tensor(o.copy()) for o in offset_np]
+    grad_refe = torch.tensor(grad_np.copy())
+
+    refe_model = MergedEmbeddingBag(emb_wgt_np, emb_len, emb_dim)
+
+    test_model = MergFreqEmbeddingBag(emb_len, emb_hot, index_test,
+                                      [torch.tensor(w.copy()) for w in emb_wgt_np],
+                                      dense_ratio)
+    opt_test = torch.optim.Adagrad(test_model.parameters(),
+                                   lr=LEARNING_RATE,
+                                   lr_decay=ADAGRAD_LR_DECAY,
+                                   weight_decay=WEIGHT_DECAY,
+                                   initial_accumulator_value=ADAGRAD_INIT_ACC,
+                                   eps=ADAGRAD_EPS)
+    opt_refe = torch.optim.Adagrad(refe_model.parameters(),
+                                   lr=LEARNING_RATE,
+                                   lr_decay=ADAGRAD_LR_DECAY,
+                                   weight_decay=WEIGHT_DECAY,
+                                   initial_accumulator_value=ADAGRAD_INIT_ACC,
+                                   eps=ADAGRAD_EPS)
+    test_model.set_optimizer(opt_test)
+
+    lbatch = batch_size // my_size
+    time_stats = {'fwd': 0.0, 'bwd': 0.0}
+    iter_no = 10
+    for i in range(iter_no + 1):
+        index_np = [np.minimum(
+            np.random.binomial(
+                n - 1, binomial_p, size=(batch_size * h)).astype(np.int64),
+            n - 1) for n, h in zip(emb_len, emb_hot)]
+        offset_np = [np.arange(0, batch_size * h, h).astype(np.int64)
+                     for h in emb_hot]
+        grad_np = np.random.uniform(-1, 1, size=(batch_size, num_emb, emb_dim)).astype(np.float32)
+
+        index_test = [torch.tensor(i.copy()) for i in index_np]
+        offset_test = [torch.tensor(o.copy()) for o in offset_np]
+        grad_test = torch.tensor(grad_np.copy())
+
+        index_refe = [torch.tensor(i.copy()) for i in index_np]
+        offset_refe = [torch.tensor(o.copy()) for o in offset_np]
+        grad_refe = torch.tensor(grad_np.copy())
+
+        y_refe = refe_model(index_refe, offset_refe)
+        ts = time.time()
+        y_test = test_model(index_test, offset_test)
+        te = time.time()
+        if i > 0:
+            time_stats['fwd'] += te - ts
+        opt_refe.zero_grad()
+        opt_test.zero_grad()
+
+        y_refe.backward(grad_refe)
+        ts = time.time()
+        y_test.backward(grad_test[my_rank * lbatch: (my_rank + 1) * lbatch].clone())
+        te = time.time()
+        if i > 0:
+            time_stats['bwd'] += te - ts
+        opt_refe.step()
+        opt_test.step()
+        all_emb_len = np.sum(emb_len)
+        test_weight = torch.zeros((all_emb_len, emb_dim), dtype=torch.float32)
+        sparse_ln = [(all_emb_len // my_size + 1) if i < all_emb_len % my_size else (all_emb_len // my_size) for i in range(my_size)]
+        test_weight_np = test_model._sparse_weight.data.detach().float().cpu().numpy()
+        refe_weight = torch.cat([e.weight.data for e in refe_model._embbags])
+        refe_weight_np = refe_weight.detach().float().cpu().numpy()
+        for i in range(test_weight_np.shape[0]):
+            j = i * my_size + my_rank
+            if not nearlyclose(test_weight_np[i], refe_weight_np[j], atol=1e-3, rtol=1e-3):
+                print(f"{j}-th not match")
+                print('test', test_weight_np[i])
+                print('refe', refe_weight_np[j])
+                print('diff', test_weight_np[i] - refe_weight_np[j])
+                idx = np.argmax(np.abs(test_weight_np[i] - refe_weight_np[j]))
+                print('val', test_weight_np[i][idx], refe_weight_np[j][idx])
+                sys.exit(0)
+    fwd_t = time_stats['fwd']
+    bwd_t = time_stats['bwd']
+    print(f"fwd {fwd_t/iter_no*1000} ms bwd {bwd_t/iter_no*1000} ms")
+    return
+
+if __name__ == '__main__':
+    len_emb = [40000000,39060,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36]
+    len_emb = [40000,39060,17295,7424,20265,3,7122,1543,63,40000,30679,4052,10,2209,11938,155,4,976,14,40000,40000,40000,5901,12973,108,36]
+    # len_emb = [20000000,39060,17295,7424,20265,3,7122,1543,63,20000000,3067956,405282,10,2209,11938,155,4,976,14,20000000,20000000,20000000,590152,12973,108,36]
+    multi_hot = [3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1]
+    batch_size = 65536
+    emb_dim = 128
+    test_freqemb_bwd(123, batch_size, len_emb, emb_dim, multi_hot, 0.3, 0.5)
